#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2025/5/8 16:59
# @Author  : caijian
# @File    : get_images.py
# @Software: PyCharm
import requests
import threading
import os
import re
import pandas as pd
from urllib.parse import urlparse
import time

def download_image(url, save_dir, max_retries=3, retry_delay=5, timeout=30):
    """ä¸‹è½½å•ä¸ªå›¾ç‰‡ï¼Œæ–‡ä»¶åå–è‡ª URL çš„æœ€åä¸€éƒ¨åˆ†ï¼Œå¸¦æœ‰é‡è¯•æœºåˆ¶å’Œæ›´é•¿çš„è¶…æ—¶"""
    for i in range(max_retries):
        try:
            response = requests.get(url, stream=True, timeout=timeout)
            response.raise_for_status()  # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ

            parsed_url = urlparse(url)
            filename = os.path.basename(parsed_url.path)
            filepath = os.path.join(save_dir, filename)

            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            print(f"æˆåŠŸä¸‹è½½: {url} -> {filepath} (å°è¯•æ¬¡æ•°: {i+1})")
            return  # ä¸‹è½½æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
        except requests.exceptions.RequestException as e:
            print(f"ä¸‹è½½å¤±è´¥ (å°è¯• {i+1}/{max_retries}): {url} - {e}")
            if i < max_retries - 1:
                time.sleep(retry_delay)
        print(f"ä¸‹è½½å¤šæ¬¡å¤±è´¥: {url}")

def download_images_multithreaded(urls, save_dir, num_threads=5):
    """å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡"""
    os.makedirs(save_dir, exist_ok=True)
    threads = []
    for url in urls:
        thread = threading.Thread(target=download_image, args=(url, save_dir))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    print("æ‰€æœ‰å›¾ç‰‡ä¸‹è½½å®Œæˆï¼")

if __name__ == "__main__":
    excel_file = "test.csv"  # å°†ä½ çš„ CSV æ–‡ä»¶åæ›¿æ¢ä¸ºå®é™…çš„æ–‡ä»¶å
    save_directory = "downloaded_images_last_part"  # æŒ‡å®šä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹
    url_column = "url"  # æŒ‡å®šåŒ…å«å›¾ç‰‡ URL çš„åˆ—å
    num_threads = 10  # å¯ä»¥è°ƒæ•´çº¿ç¨‹æ•°é‡
    max_retries = 3   # ä¸‹è½½å¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°
    retry_delay = 5   # é‡è¯•ä¹‹é—´çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    timeout = 30      # ä¸‹è½½è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    try:
        df = pd.read_csv(excel_file)
        if url_column not in df.columns:
            print(f"é”™è¯¯ï¼šCSV æ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åä¸º '{url_column}' çš„åˆ—ã€‚")
        else:
            image_urls = df[url_column].tolist()
            download_images_multithreaded(image_urls, save_directory, num_threads)
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{excel_file}'ã€‚")
    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")

æŸ¥çœ‹imagesæ–‡ä»¶å¤¹æœ‰å¤šå°‘å¼ å›¾ç‰‡çš„è„šæœ¬
import os
import sys

# é»˜è®¤è¦æŸ¥çœ‹çš„æ–‡ä»¶å¤¹è·¯å¾„
# ä½ ä¹Ÿå¯ä»¥åœ¨è¿è¡Œè„šæœ¬æ—¶é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šè·¯å¾„
DEFAULT_FOLDER_PATH = './images' # æ ¹æ®ä½ çš„å®é™…æ–‡ä»¶å¤¹åç§°ä¿®æ”¹è¿™é‡Œ

def count_files_in_folder(folder_path):
    """
    è®¡ç®—æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹æ•°é‡ã€‚
    """
    if not os.path.isdir(folder_path):
        print(f"é”™è¯¯ï¼šæ–‡ä»¶å¤¹ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„: {folder_path}")
        return None

    try:
        # è·å–æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹çš„åˆ—è¡¨
        entries = os.listdir(folder_path)
        # è®¡ç®—åˆ—è¡¨çš„é•¿åº¦ï¼Œå³æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹çš„æ•°é‡
        count = len(entries)
        return count
    except Exception as e:
        print(f"é”™è¯¯ï¼šæ— æ³•è®¿é—®æ–‡ä»¶å¤¹ {folder_path} çš„å†…å®¹ï¼š{e}")
        return None

if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„
    if len(sys.argv) > 1:
        folder_to_check = sys.argv[1]
    else:
        folder_to_check = DEFAULT_FOLDER_PATH
        print(f"æœªæŒ‡å®šæ–‡ä»¶å¤¹è·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„: {folder_to_check}")

    file_count = count_files_in_folder(folder_to_check)

    if file_count is not None:
        print(f"æ–‡ä»¶å¤¹ '{folder_to_check}' ä¸­å…±æœ‰ {file_count} ä¸ªæ¡ç›® (åŒ…æ‹¬æ–‡ä»¶å’Œå­æ–‡ä»¶å¤¹)ã€‚")

# åˆ é™¤å·²ç»ä¸‹è½½çš„æ–‡ä»¶å¤¹é‡Œé¢çš„æŸåçš„æ–‡ä»¶ 
import os
from PIL import Image
import pandas as pd

def check_and_remove_corrupted_images(folder_path):
    corrupted_images = []

    for root, _, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            try:
                with Image.open(file_path) as img:
                    img.verify()  # éªŒè¯å›¾ç‰‡æ˜¯å¦æŸå
            except Exception as e:
                print(f"æŸåå›¾ç‰‡: {file_path}ï¼Œé”™è¯¯: {e}")
                corrupted_images.append(file_path)
                try:
                    os.remove(file_path)
                    print(f"å·²åˆ é™¤æŸåå›¾ç‰‡: {file_path}")
                except Exception as del_e:
                    print(f"åˆ é™¤å›¾ç‰‡å¤±è´¥: {file_path}ï¼Œé”™è¯¯: {del_e}")

    # ä¿å­˜æŸåå›¾ç‰‡è·¯å¾„åˆ°csv
    if corrupted_images:
        df = pd.DataFrame(corrupted_images, columns=['corrupted_image_path'])
        df.to_csv('corrupted_images.csv', index=False, encoding='utf-8-sig')
        print(f"å…±å‘ç° {len(corrupted_images)} å¼ æŸåå›¾ç‰‡ï¼Œå·²ä¿å­˜åˆ° corrupted_images.csv")
    else:
        print("æœªå‘ç°æŸåå›¾ç‰‡")

if __name__ == "__main__":
    folder = "/root/autodl-tmp/train_images"  # ä¿®æ”¹ä¸ºä½ çš„å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
    check_and_remove_corrupted_images(folder)

##åŸºäºä¸‹è½½æ–‡ä»¶å¤¹å‘ƒå‘ƒè„šæœ¬ä»¥åŠæŸ¥çœ‹å¹¶åˆ é™¤æŸåçš„å›¾ç‰‡ä»¥ååˆ¶ä½œæ•°æ®é›†è„šæœ¬

ä¸‹é¢çš„æ˜¯æ›´æœ‰æ•ˆçš„åˆ é™¤ç ´æŸå›¾ç‰‡çš„ä»£ç  
import os
from PIL import Image, ImageOps # å¯¼å…¥ Pillow åº“
# from io import BytesIO # BytesIO åœ¨è¿™ä¸ªè„šæœ¬ä¸­æ²¡æœ‰ç›´æ¥ç”¨åˆ°ï¼Œä½†ä¿ç•™äº†ç”¨æˆ·åŸæœ‰çš„å¯¼å…¥

# â€”â€” é…ç½®åŒº â€”â€”
# è¯·å°†è¿™é‡Œçš„è·¯å¾„æ›¿æ¢ä¸ºä½ å®é™…å­˜æ”¾å›¾ç‰‡çš„æ–‡ä»¶å¤¹è·¯å¾„
image_directory_path = r'D:\project\myfirstgo\downloaded_images_to_clean'
# â€”â€” é…ç½®ç»“æŸ â€”â€”

def main():
    """
    æ‰«ææŒ‡å®šç›®å½•ï¼Œæ£€æŸ¥å›¾ç‰‡æ–‡ä»¶æœ‰æ•ˆæ€§ï¼Œå¹¶åˆ é™¤æ— æ•ˆæ–‡ä»¶ã€‚
    """
    print(f"--- å¼€å§‹æ£€æŸ¥å’Œæ¸…ç†å›¾ç‰‡ç›®å½•: {image_directory_path} ---")

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”æ˜¯ä¸€ä¸ªç›®å½•
    if not os.path.isdir(image_directory_path):
        print(f"é”™è¯¯ï¼šæŒ‡å®šçš„å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€ä¸ªç›®å½•ï¼š{image_directory_path}")
        return

    try:
        # è·å–ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨
        items_in_directory = os.listdir(image_directory_path)
    except Exception as e:
        print(f"æ‰«æç›®å½•æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return

    total_items = len(items_in_directory)
    removed_count = 0
    processed_files_count = 0

    print(f"åœ¨ç›®å½• '{image_directory_path}' ä¸­æ‰¾åˆ° {total_items} ä¸ªæ¡ç›® (æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹)ã€‚")

    # éå†ç›®å½•ä¸­çš„æ¯ä¸ªæ¡ç›®
    for i, item_name in enumerate(items_in_directory):
        full_path = os.path.join(image_directory_path, item_name) # æ„å»ºå®Œæ•´è·¯å¾„

        # è·³è¿‡ä¸æ˜¯æ–‡ä»¶çš„æ¡ç›®ï¼ˆä¾‹å¦‚ï¼Œå­æ–‡ä»¶å¤¹ï¼‰
        if not os.path.isfile(full_path):
            print(f"Skipping (not a file): {item_name}")
            continue

        processed_files_count += 1 # åªå¯¹æ–‡ä»¶è¿›è¡Œè®¡æ•°å’Œå¤„ç†

        # æ‰“å°å½“å‰å¤„ç†è¿›åº¦ï¼ˆæ–‡ä»¶è®¡æ•°ï¼‰ï¼Œä¸æ¢è¡Œ
        print(f"Processing file {processed_files_count}/{total_items} ({i+1}/{total_items} total items): {item_name}... ", end='')

        try:
            # å°è¯•ä½¿ç”¨ Pillow æ‰“å¼€å›¾ç‰‡æ–‡ä»¶
            # ä½¿ç”¨ 'with' è¯­å¥ç¡®ä¿æ–‡ä»¶èµ„æºè¢«æ­£ç¡®ç®¡ç†å’Œå…³é—­
            with Image.open(full_path) as img:
                # å°è¯•åŠ è½½å›¾ç‰‡æ•°æ®ã€‚è¿™é€šå¸¸ä¼šæ£€æµ‹æ–‡ä»¶æ˜¯å¦æŸåæˆ–æ ¼å¼æ˜¯å¦æ— æ•ˆã€‚
                # å¦‚æœæ–‡ä»¶æ˜¯æ— æ•ˆçš„ï¼Œimg.load() ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚
                img.load()
                # å¦‚æœä½ ç‰¹åˆ«éœ€è¦æ£€æŸ¥å¹¶å¤„ç† EXIF æ–¹å‘ä¿¡æ¯ï¼Œå¯ä»¥ä¿ç•™ä¸‹é¢è¿™è¡Œï¼š
                # ImageOps.exif_transpose(img)

            # å¦‚æœä¸Šé¢ä»£ç æ²¡æœ‰æŠ›å‡ºå¼‚å¸¸ï¼Œè¯´æ˜å›¾ç‰‡æ˜¯æœ‰æ•ˆçš„
            print("OK")

        except Exception as e:
            # å¦‚æœåœ¨æ‰“å¼€æˆ–åŠ è½½å›¾ç‰‡æ—¶å‘ç”Ÿä»»ä½•å¼‚å¸¸ï¼Œè¯´æ˜æ–‡ä»¶æœ‰é—®é¢˜
            print(f"Error: {type(e).__name__} - {e}") # æ‰“å°å¼‚å¸¸ç±»å‹å’Œä¿¡æ¯
            print(f"Removing invalid file: {item_name}") # æç¤ºå°†è¦åˆ é™¤æ–‡ä»¶

            try:
                # åˆ é™¤æ— æ•ˆçš„æ–‡ä»¶
                os.remove(full_path)
                removed_count += 1
                print("File removed successfully.")
            except Exception as remove_e:
                # å¦‚æœåœ¨åˆ é™¤æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼ˆä¾‹å¦‚æƒé™é—®é¢˜ï¼‰
                print(f"Error removing file {item_name}: {remove_e}")

    print(f"\n--- æ£€æŸ¥ç»“æŸ ---")
    print(f"æ€»å…±æ£€æŸ¥æ–‡ä»¶æ•°ï¼š {processed_files_count}")
    print(f"ç§»é™¤æ— æ•ˆæ–‡ä»¶æ•°ï¼š {removed_count}")
    # æ³¨æ„ï¼šè¿™é‡Œæ— æ³•å‡†ç¡®è®¡ç®—å‰©ä½™æ–‡ä»¶ï¼Œå› ä¸º os.listdir() æ˜¯åœ¨å¼€å§‹æ—¶è·å–çš„åˆ—è¡¨ï¼Œ
    # ä¸”å¯èƒ½åŒ…å«å…¶ä»–ç±»å‹çš„æ–‡ä»¶æˆ–åˆ é™¤å¤±è´¥çš„æ–‡ä»¶ã€‚processed_files_count æ˜¯å®é™…æ£€æŸ¥è¿‡çš„æ–‡ä»¶æ•°ã€‚
    print(f"è¯·é‡æ–°æŸ¥çœ‹ç›®å½• '{image_directory_path}' ç¡®è®¤å‰©ä½™æ–‡ä»¶ã€‚")


if __name__ == "__main__":
    # è¿è¡Œä¸»å‡½æ•°
    main()

ç°åœ¨éœ€è¦æ ¹æ®å›¾ç‰‡ç”Ÿæˆä¸€ä¸ªcsvæ–‡ä»¶ åˆ°äº†è¿™ä¸ªæ­¥éª¤ å¹¶ä¸”æ¯ä¸ªå›¾ç‰‡åªä¿å­˜ä¸€è¡Œæ•°æ®

import os
import pandas as pd

# â€”â€” é…ç½®åŒº â€”â€”
image_directory = 'train_images' # åŒ…å«å›¾ç‰‡çš„ç›®å½•è·¯å¾„
csv_file_path ='train_reduced.csv' # ä½ çš„è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„
output_csv_path= 'filtered_data.csv' # ä½ å¸Œæœ›ä¿å­˜ç»“æœçš„æ–° CSV æ–‡ä»¶è·¯å¾„
# â€”â€” é…ç½®ç»“æŸ â€”â€”
print(f"--- å¼€å§‹å¤„ç† ---")
# 1. è·å–å›¾ç‰‡ç›®å½•ä¸­çš„æ–‡ä»¶ååˆ—è¡¨ï¼Œå¹¶è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿å¿«é€ŸæŸ¥æ‰¾

# è·å–ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶å’Œæ–‡ä»¶å¤¹åˆ—è¡¨
# é‡è¦çš„ä¿®æ­£ï¼šalist_set åº”è¯¥æ˜¯ä¸€ä¸ªé›†åˆ (set)
alist_set = set(os.listdir(image_directory))
print(f"åœ¨ç›®å½• '{image_directory}' ä¸­æ‰¾åˆ° {len(alist_set)} ä¸ªå”¯ä¸€æ–‡ä»¶/æ–‡ä»¶å¤¹åã€‚") # æ‰“å°é›†åˆå¤§å°

# 2. è¯»å– CSV æ–‡ä»¶åˆ° DataFrame
df = pd.read_csv(csv_file_path)
print(f"æˆåŠŸè¯»å– CSV æ–‡ä»¶: {csv_file_path}ï¼Œå…± {len(df)} è¡Œæ•°æ®ã€‚")

# 3. ä» 'url' åˆ—ä¸­æå–æ–‡ä»¶å
print("æ­£åœ¨ä» URL ä¸­æå–æ–‡ä»¶å...")
# ä½¿ç”¨ pandas çš„ apply æˆ– str æ–¹æ³•æå–æ–‡ä»¶å
df['extracted_filename'] = df['url'].apply(lambda x: x.split('/')[-1] if isinstance(x, str) else None)
# æ›´æ¨èä½¿ç”¨ str æ–¹æ³•ï¼Œé€šå¸¸æ›´é«˜æ•ˆ
#df['extracted_filename'] = df['url'].str.rsplit('/', 1).str[-1]


# 4. æ ¹æ®æå–çš„æ–‡ä»¶åæ˜¯å¦åœ¨ `alist_set` ä¸­æ¥ç­›é€‰ DataFrame (åˆç­›)
print("æ­£åœ¨æ ¹æ®å›¾ç‰‡ç›®å½•ä¸­çš„æ–‡ä»¶åˆ—è¡¨è¿›è¡Œåˆæ­¥ç­›é€‰...")
is_in_alist = df['extracted_filename'].isin(alist_set)
# ç­›é€‰å‡ºåŒ¹é…çš„è¡Œ
filtered_df = df[is_in_alist].copy() # ä½¿ç”¨ .copy()
print(f"åˆæ­¥ç­›é€‰å®Œæˆã€‚å…±æ‰¾åˆ° {len(filtered_df)} è¡Œæ•°æ®å¯¹åº”çš„æ–‡ä»¶ååœ¨å›¾ç‰‡ç›®å½•åˆ—è¡¨ä¸­ã€‚")
# 5. å»é™¤åŸºäºæ–‡ä»¶åçš„é‡å¤è¡Œï¼Œåªä¿ç•™æ¯å¼ å›¾ç‰‡å¯¹åº”çš„ç¬¬ä¸€è¡Œæ•°æ®
if not filtered_df.empty:
    print("æ­£åœ¨å»é™¤åŸºäºæ–‡ä»¶åçš„é‡å¤è¡Œ...")
    # drop_duplicates æ ¹æ® 'extracted_filename' åˆ—å»é™¤é‡å¤è¡Œï¼Œé»˜è®¤ä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è¡Œ
    deduplicated_df = filtered_df.drop_duplicates(subset=['extracted_filename']).copy()
    print(f"å»é™¤é‡å¤è¡Œå®Œæˆã€‚æœ€ç»ˆä¿ç•™ {len(deduplicated_df)} è¡Œæ•°æ®ã€‚")
    # 6. å°†å»é™¤é‡å¤åçš„ DataFrame ä¿å­˜åˆ°æ–°çš„ CSV æ–‡ä»¶
    try:
        deduplicated_df.to_csv(output_csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ‰ æˆåŠŸå°†å»é‡åçš„æ•°æ®ä¿å­˜åˆ°ï¼š\n    {output_csv_path}")
    except Exception as e:
        print(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
else:
     print("\nåˆæ­¥ç­›é€‰åæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è¡Œï¼Œæœªè¿›è¡Œå»é‡å’Œç”Ÿæˆè¾“å‡º CSV æ–‡ä»¶ã€‚")
print("--- å¤„ç†ç»“æŸ ---")


# images2csv.py æ–‡ä»¶ æŠŠimagesæ–‡ä»¶è½¬ä¸º csv

import pandas as pd
import os

# è¯»å–åŸCSV
df = pd.read_csv('total_all.csv')

# ç”Ÿæˆcaptionåˆ—
df['caption'] = (
    'è¯¥å›¾ç‰‡ä¸ºæ±½è½¦çš„:' + df['task_type_name'] + 
    'å›¾ç‰‡ ç‚¹ä½ç±»å‹:' + df['point_type_name'] + 
    ' ç‚¹ä½åç§°:' + df['point_name']
)

# æå–å›¾ç‰‡æ–‡ä»¶å
df['image_name'] = df['url'].apply(lambda x: os.path.basename(x))

# æ‹¼æ¥å›¾ç‰‡å®Œæ•´è·¯å¾„
df['image_path'] = '/root/autodl-tmp/train_images/' + df['image_name']

# é€‰æ‹©éœ€è¦çš„åˆ—è¾“å‡º
output_df = df[['caption', 'image_path']]

# ä¿å­˜ä¸ºæ–°çš„CSV
output_df.to_csv('output.csv', index=False, encoding='utf-8-sig')

print("ç”Ÿæˆoutput.csvå®Œæˆ")


csv2json.pyæ–‡ä»¶ 
import pandas as pd
import json
# è½½å…¥CSVæ–‡ä»¶
df = pd.read_csv('./output.csv')
conversations = []

# æ·»åŠ å¯¹è¯æ•°æ®
for i in range(len(df)):
    conversations.append({
        "id": f"identity_{i+1}",
        "conversations": [
            {
                "from": "user",
                "value": f"è¯·ä½ åˆ†æä¸€ä¸‹æ­¤å›¾ç‰‡ä¸ºæ±½è½¦çš„å†…é¥°å›¾ç‰‡è¿˜æ˜¯å¤–é¥°å›¾ç‰‡,æ˜¯ä»€ä¹ˆç‚¹ä½ç±»å‹ã€æ˜¯ä»€ä¹ˆç‚¹ä½åç§°: <|vision_start|>{df.iloc[i]['image_path']}<|vision_end|>"
            },
            {
                "from": "assistant", 
                "value": df.iloc[i]['caption']
            }
        ]
    })

# ä¿å­˜ä¸ºJson
with open('data_vl.json', 'w', encoding='utf-8') as f:
    json.dump(conversations, f, ensure_ascii=False, indent=2)



åˆ æ‰é‚£äº›æŸåçš„å›¾ç‰‡ filter_clean.py
import os
import pandas as pd

# 1. ä» corrupted_images.csv é‡Œè¯»å‡ºæ‰€æœ‰æŸåå›¾ç‰‡çš„â€œstemâ€ï¼ˆä¸å«åç¼€ï¼‰
df_corrupt = pd.read_csv('corrupted_images.csv')
corrupted_stems = {
    os.path.splitext(os.path.basename(p))[0]
    for p in df_corrupt['corrupted_image_path']
}
print(f'å…± {len(corrupted_stems)} å¼ æŸåå›¾çš„ stem è¢«æ”¶é›†ã€‚')

# 2. è¯»å…¥ downloaded_urls.csvï¼Œæå–æ¯æ¡ url å¯¹åº”çš„ stemï¼Œè¿‡æ»¤æ‰æŸåçš„ï¼Œä¿å­˜ä¸º total_all.csv
df_all = pd.read_csv('downloaded_urls.csv')

# æ–°åˆ—ï¼šurl å¯¹åº”çš„æ–‡ä»¶åå»æ‰åç¼€ä¹‹åçš„ stem
df_all['stem'] = df_all['url'].map(
    lambda u: os.path.splitext(os.path.basename(u))[0]
)

# ä¿ç•™ä¸åœ¨ corrupted_stems é‡Œçš„é‚£äº›è¡Œ
df_filtered = df_all[~df_all['stem'].isin(corrupted_stems)].copy()
df_filtered.drop(columns=['stem'], inplace=True)

# è¾“å‡º
df_filtered.to_csv('total_all.csv', index=False, encoding='utf-8-sig')
print(f'è¿‡æ»¤å {len(df_filtered)} æ¡è®°å½•ï¼Œå·²ä¿å­˜åˆ° total_all.csv.')

# 3. éå† train_images ç›®å½•ï¼Œåªä¿ç•™å‡ºç°åœ¨ total_all.csv é‡Œçš„å›¾ç‰‡ï¼ˆåŒæ ·æ¯”å¯¹ stemï¼‰
valid_stems = {
    os.path.splitext(os.path.basename(u))[0]
    for u in df_filtered['url']
}

train_dir = '/root/autodl-tmp/train_images'  # è¯·æ ¹æ®å®é™…ä¿®æ”¹
deleted = 0
for fn in os.listdir(train_dir):
    full = os.path.join(train_dir, fn)
    if not os.path.isfile(full):
        continue

    stem = os.path.splitext(fn)[0]
    # å¦‚æœè¿™ä¸ªæ–‡ä»¶çš„ stem ä¸åœ¨æˆ‘ä»¬ä¿ç•™ä¸‹æ¥çš„åˆ—è¡¨é‡Œï¼Œå°±åˆ æ‰
    if stem not in valid_stems:
        os.remove(full)
        deleted += 1

print(f'åœ¨ "{train_dir}" ä¸­å…±åˆ é™¤äº† {deleted} å¼ ä¸åœ¨ total_all.csv åˆ—è¡¨é‡Œçš„å›¾ç‰‡ã€‚')


# new_csv2json.py åˆ¶ä½œLLaMA-Factoryæ•°æ®é›†
import pandas as pd
import json
import os

# è½½å…¥CSVæ–‡ä»¶
csv_file_path = './output.csv'
try:
    df = pd.read_csv(csv_file_path)
except FileNotFoundError:
    print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ° {csv_file_path} æ–‡ä»¶ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
    exit()

# ç”¨äºå­˜å‚¨æœ€ç»ˆçš„ JSON æ•°æ®åˆ—è¡¨
json_data_list = []

# å®šä¹‰å›ºå®šçš„äººç±»æŒ‡ä»¤æ–‡æœ¬
# æ ¹æ®ä½ çš„ä»»åŠ¡è°ƒæ•´è¿™é‡Œçš„æ–‡æœ¬ã€‚å¦‚æœ CSV ä¸­æœ‰ä¸“é—¨çš„æŒ‡ä»¤åˆ—ï¼Œåº”ä» CSV è¯»å–ã€‚
# è¿™é‡Œçš„ç¤ºä¾‹ä½¿ç”¨äº†ä½ ä¹‹å‰æåˆ°è¿‡çš„æ±½è½¦åˆ†ææŒ‡ä»¤
FIXED_HUMAN_INSTRUCTION = "è¯·ä½ åˆ†æä¸€ä¸‹æ­¤å›¾ç‰‡ä¸ºæ±½è½¦çš„å†…é¥°å›¾ç‰‡è¿˜æ˜¯å¤–é¥°å›¾ç‰‡,æ˜¯ä»€ä¹ˆç‚¹ä½ç±»å‹ã€æ˜¯ä»€ä¹ˆç‚¹ä½åç§°:"

# å¯é€‰ï¼šå®šä¹‰å›ºå®šçš„ System æ¶ˆæ¯æ–‡æœ¬
# å¦‚æœä½ çš„æ•°æ®é›†éœ€è¦ System æ¶ˆæ¯ï¼Œå¯ä»¥åœ¨è¿™é‡Œå®šä¹‰ã€‚å¦‚æœä¸éœ€è¦ï¼Œå¯ä»¥æ³¨é‡Šæ‰æˆ–è®¾ä¸º Noneã€‚
FIXED_SYSTEM_MESSAGE = None # ä½ çš„æ ·ä¾‹ä¸­æœ‰ï¼Œä½†å¦‚æœä½ çš„ä»»åŠ¡ä¸éœ€è¦æˆ– CSV ä¸­æ²¡æœ‰ï¼Œå¯ä»¥è®¾ä¸º None
FIXED_SYSTEM_MESSAGE ="ä½ æ˜¯ä¸€ä¸ªæ“…é•¿è¯†åˆ«æ±½è½¦å›¾ç‰‡çš„åˆ†æå¸ˆã€‚" 

# éå† DataFrame çš„æ¯ä¸€è¡Œ
for index, row in df.iterrows():
    # è·å–å›¾ç‰‡è·¯å¾„å’Œ caption
    image_path = row.get('image_path') # ä½¿ç”¨ .get() é˜²æ­¢åˆ—ä¸å­˜åœ¨æŠ¥é”™
    caption = row.get('caption')     # ä½¿ç”¨ .get() é˜²æ­¢åˆ—ä¸å­˜åœ¨æŠ¥é”™

    # æ£€æŸ¥å…³é”®æ•°æ®æ˜¯å¦å­˜åœ¨
    if not image_path:
        print(f"Warning: Skipping row {index} due to missing image_path.")
        continue
    # caption å¯ä»¥æ˜¯ç©ºçš„ï¼Œä½†å¦‚æœ caption æ˜¯ AI å›å¤çš„æ ¸å¿ƒï¼Œéœ€è¦æ£€æŸ¥
    # if pd.isna(caption):
    #     print(f"Warning: Skipping row {index} due to missing caption.")
    #     continue


    # æ„å»º messages åˆ—è¡¨
    messages_entry = []

    # æ·»åŠ  System æ¶ˆæ¯ (å¦‚æœå®šä¹‰äº†)
    if FIXED_SYSTEM_MESSAGE is not None:
        messages_entry.append({
            "content": FIXED_SYSTEM_MESSAGE,
            "role": "system"
        })

    # æ·»åŠ  User æ¶ˆæ¯ï¼ŒåŒ…å« <image> æ ‡è®°å’ŒæŒ‡ä»¤
    # æ³¨æ„ï¼š<image> æ ‡è®°æ˜¯ç‰¹æ®Šçš„ï¼Œæ¨¡å‹å¤„ç†æ—¶ä¼šå°†å…¶ä¸ images åˆ—è¡¨ä¸­çš„å›¾ç‰‡å…³è”
    user_content = f"<image>{FIXED_HUMAN_INSTRUCTION}"
    # å¦‚æœ CSV ä¸­æœ‰ä¸“é—¨çš„æŒ‡ä»¤åˆ—ï¼Œä¾‹å¦‚ 'instruction_text'
    # instruction_text = row.get('instruction_text')
    # if instruction_text:
    #     user_content = f"<image>{instruction_text}"
    # else:
    #      user_content = f"<image>{FIXED_HUMAN_INSTRUCTION}" # ä½¿ç”¨å›ºå®šæŒ‡ä»¤ä½œä¸ºå›é€€

    messages_entry.append({
        "content": user_content,
        "role": "user"
    })

    # æ·»åŠ  Assistant æ¶ˆæ¯
    # æ³¨æ„ï¼šæ ·ä¾‹ä¸­çš„ assistant content æ˜¯ä¸€ä¸ª JSON å­—ç¬¦ä¸²ï¼Œä½ çš„ caption åˆ—éœ€è¦æ˜¯è¿™æ ·çš„æ ¼å¼
    assistant_content = str(caption) if pd.notna(caption) else "" # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²

    messages_entry.append({
        "content": assistant_content,
        "role": "assistant"
    })

    # æ„å»ºå•ä¸ªæ•°æ®æ ·æœ¬çš„å­—å…¸
    data_sample = {
        "messages": messages_entry, # ä½¿ç”¨ messages å­—æ®µ
        "images": [image_path]     # ä½¿ç”¨ images å­—æ®µï¼Œå€¼æ˜¯åŒ…å«å›¾ç‰‡è·¯å¾„çš„åˆ—è¡¨
        # æ ¹æ®ç›®æ ‡æ ¼å¼ï¼Œæ²¡æœ‰é¡¶å±‚çš„ "id" å­—æ®µ
    }

    # å°†æ„å»ºå¥½çš„æ ·æœ¬å­—å…¸æ·»åŠ åˆ°åˆ—è¡¨ä¸­
    json_data_list.append(data_sample)

# ä¿å­˜ä¸ºJsonæ–‡ä»¶
output_json_path = 'LLama_Factory_data.json'
try:
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data_list, f, ensure_ascii=False, indent=2) # ä½¿ç”¨ ensure_ascii=False ä¿ç•™ä¸­æ–‡
except IOError as e:
    print(f"é”™è¯¯ï¼šå†™å…¥æ–‡ä»¶ {output_json_path} å¤±è´¥ï¼š{e}")
    exit()


print(f"æˆåŠŸå°† {len(json_data_list)} æ¡æ•°æ®è½¬æ¢ä¸º {output_json_path}")



