# Docker å°è£…anacondaç¯å¢ƒï¼Œç”Ÿæˆé•œåƒå¹¶æ‰“åŒ…

### 1.åˆ—å‡ºæœ¬æœºçš„æ‰€æœ‰ image æ–‡ä»¶ï¼š
docker images

### 2.#åˆ—å‡ºæœ¬æœºçš„æ‰€æœ‰ dockerå®¹å™¨ï¼š

docker ps -a

### 3.åˆ—å‡ºæœ¬æœºçš„è¿è¡Œä¸­çš„ dockerå®¹å™¨ï¼š
docker ps

### 4.æ‹‰å–é•œåƒ

continuumio/anaconda3 é•œåƒæ˜¯dockeré•œåƒåº“ä¸­staræ•°æœ€é«˜çš„é•œåƒ

docker pull continuumio/anaconda

ç”¨ docker images æŒ‡ä»¤ æŸ¥çœ‹æ˜¯å¦æ‹‰å–æˆåŠŸã€‚

### 5.ç”¨continuumio/anaconda3é•œåƒåˆ›å»ºä¸€ä¸ªåä¸ºtestçš„å®¹å™¨

docker run --name test -idt continuumio/anaconda

### 6.è¿›å…¥testå®¹å™¨ï¼ŒæŸ¥çœ‹[conda](https://so.csdn.net/so/search?q=conda&spm=1001.2101.3001.7020)ä½ç½®

docker exec -it test /bin/bash

â—åœ¨testå®¹å™¨å†…æŸ¥çœ‹å®¹å™¨å†…çš„[anaconda](https://so.csdn.net/so/search?q=anaconda&spm=1001.2101.3001.7020)çš„ä½ç½®

whereis anaconda
#æˆ–è€…ä½¿ç”¨
conda info --envs

æŸ¥çœ‹å®Œåæ¥ç€é€€å‡ºå®¹å™¨ï¼š#å¿«æ·é”® Ctrl+d

å›¾ä¸­å¯ä»¥çœ‹å‡ºåœ¨å®¹å™¨å†…éƒ¨ï¼Œcondaçš„è·¯å¾„æ˜¯/opt/conda ã€‚

### 7.åœ¨æœ¬åœ°ç¯å¢ƒä¸­å°†æœ¬åœ°ç¯å¢ƒå¤åˆ¶åˆ°dockerä¸­

â—é€€å‡ºå®¹å™¨åï¼Œåœ¨æœ¬åœ°ç¯å¢ƒä¸­å°†éœ€è¦æ‰“åŒ…çš„æœ¬åœ°å¤åˆ¶åˆ°dockerä¸­

docker cp /home/b/miniconda3/envs/yolo1.7 test:/opt/conda/envs

å…¶ä¸­ ï¼š /home/b/miniconda3/envs/yolo1.7 æ˜¯è‡ªå·±éœ€è¦æ‰“åŒ…çš„æœ¬åœ°ç¯å¢ƒï¼Œ testæ˜¯å®¹å™¨åï¼Œ/opt/condaæ˜¯åœ¨å®¹å™¨å†…ä½¿ç”¨ conda info --envs æŸ¥çœ‹åˆ°çš„æ ¹ç›®å½•ã€‚

æ¥ç€å†è¿›å…¥å®¹å™¨ æŸ¥çœ‹ç¯å¢ƒæ˜¯å¦å¤åˆ¶æˆåŠŸï¼ˆç„¶åå†é€€å‡ºå®¹å™¨ï¼‰

docker exec -it test /bin/bash

conda info --envs

### 8.åœ¨æœ¬åœ°ç¯å¢ƒä¸­å°†æœ¬åœ°ä»£ç å¤åˆ¶åˆ°dockerä¸­

é€€å‡ºå®¹å™¨åï¼Œåœ¨æœ¬åœ°ç¯å¢ƒä¸­å°†éœ€è¦æ‰“åŒ…çš„ä»£ç å¤åˆ¶åˆ°dockerä¸­

 docker cp /home/b/.../tools test:/root/

å®¹å™¨å†…æŸ¥çœ‹å¤åˆ¶ç»“æœå¹¶é€€å‡ºå®¹å™¨

### 9.å°†å®¹å™¨ä¿å­˜ä¸ºé•œåƒ

â—é€€å‡ºå®¹å™¨åï¼Œæ‰§è¡Œï¼š

 docker commit -a 'author' -m 'instruction' test image_test 

è¯¥å‘½ä»¤å„å­—æ®µï¼š test ï¼šå®¹å™¨åå­—   image_testï¼šä¿å­˜çš„é•œåƒçš„åå­—ã€‚

â—æŸ¥çœ‹é•œåƒ ï¼Œå¯ä»¥çœ‹åˆ°å·²ç»ç”Ÿæˆäº†åä¸ºimage_testçš„æ–°é•œåƒ

### 10.å°†é•œåƒå­˜ä¸ºå‹ç¼©åŒ…

â—cdåˆ°ä¸€ä¸ªæŒ‡å®šç›®å½•ï¼Œä»¥ä¾¿äºæŸ¥æ‰¾ä¿å­˜çš„å‹ç¼©åŒ…ã€‚
â—å‹ç¼©:

 docker save -o test_tar.tar image_test

åˆ°æ­¤ä¸ºæ­¢ï¼Œæœ¬åœ°çš„condaç¯å¢ƒä¾¿æ‰“åŒ…å®Œæˆ

## ä¸‰ã€è¯»å–é•œåƒ

åœ¨å®¿ä¸»æœºä¸Šæ‰§è¡Œï¼š
â—å°†æ‰“åŒ…å¥½çš„é•œåƒå‹ç¼©åŒ…æ‹·è´åˆ°å®¿ä¸»æœºä¸Šã€‚
â—cd åˆ°å‹ç¼©åŒ…ç›®å½•ï¼Œæ‰§è¡Œï¼š

docker load -i pipeline_1211.tar

æ˜¾ç¤º Loaded image: pipeline_1211_image:latest å¯å‘ç°å·²ç»ç”Ÿæˆäº†ä¸€ä¸ªåä¸ºpipeline_1211_image çš„æ–°é•œåƒ

â—ç”¨image_testé•œåƒåˆ›å»ºä¸€ä¸ªåä¸ºcreate_testçš„å®¹å™¨ï¼š

docker run --name creat_pipeline -idt pipeline_1211_image

PS:è‹¥åˆ›å»ºå®¹å™¨æ—¶éœ€è¦å¯¹å®¹å™¨å†…æ–‡ä»¶ä¸å®¹å™¨å¤–æ–‡ä»¶åšæ˜ å°„ï¼Œåˆ™éœ€è¦æ‰§è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š

docker run --name creat_test -v /home/b/hxb:/root/hxb -idt image_test



docker exec -it test /bin/bash

ç”¨é•œåƒpipeline_1211_imageåˆ›å»ºå®¹å™¨ docker run --name creat_pipeline -idt pipeline_1211_image

å¯åŠ¨å®¹å™¨ docker start 7aa432abb1650b0cf4dac9040e9c96de125c6dcf5083a5217f572a91297e7b37

è¿›å…¥å®¹å™¨ docker exec -it creat_pipeline /bin/bash

http://ftp.nluug.nl/pub/vim/unix/vim-8.2.tar.bz2

./configure --prefix=/usr/local/vim

## Minio çš„è´¦å·å¯†ç  

```
docker run -it -d --name minio -p 9000:9000 -p 9001:9001 -v /tmp/minio/data:/data -e MINIO_ROOT_USER="minio" -e MINIO_ROOT_PASSWORD="caijminio" bitnami/minio:latest

Access Key  nMAFLTK9Od8rDv4lzYxg 

Secret Key  hDHrPZ8U87OYjivOECBpiRdpDoyAmP5e47VH9F3U
```





docker run -it -p 4000:4000 --name modelscope registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.13.1 /bin/bash

 ls
'=0.21.0'  '=2.25.0'   boot                         dev   home   lib32   libx32   mnt          NGC-DL-CONTAINER-LICENSE   proc   run    srv   tmp   var
'=0.6.0'    bin        cuda-keyring_1.0-1_all.deb   etc   lib    lib64   media    modelscope   opt                        root   sbin   sys   usr

è¿›å…¥äº†å®¹å™¨ 

```java
1ã€ä¸‹è½½è§£å‹
tar -zxvf ffmpeg-3.1.tar.gz 
2ã€ è¿›å…¥è§£å‹åç›®å½•,è¾“å…¥å¦‚ä¸‹å‘½ä»¤/usr/local/ffmpegä¸ºè‡ªå·±æŒ‡å®šçš„å®‰è£…ç›®å½•
cd ffmpeg-3.1
./configure 
make && make install
æŸ¥çœ‹æ˜¯å¦ç”Ÿæ•ˆ
source /etc/profile  è®¾ç½®ç”Ÿæ•ˆ
3ã€æŸ¥çœ‹ç‰ˆæœ¬

ffmpeg -version    æŸ¥çœ‹ç‰ˆæœ¬
```

***ä½†æ˜¯åœ¨./configureå°±æŠ¥é”™ï¼ŒåŸå› ç¼ºå°‘gccç¯å¢ƒ æ— æ³•ç¼–è¯‘***

**å®‰è£…gcc**
é“¾æ¥ï¼šhttps://pan.baidu.com/s/1sL_o1rV5rultKanFYHZ7aA
æå–ç ï¼š8y7d
ä¸‹è½½é‡Œé¢çš„åŒ… å¦‚ä½•è¿›å…¥å®¹å™¨å†…éƒ¨

```java
sh install_gcc.sh #æ‰§è¡Œè„šæœ¬
```

è¿™ä¸ªæ—¶å€™ç³»ç»Ÿå†…éƒ¨å°±æœ‰ä»¥ä¸Šgccç¯å¢ƒäº† ä½†æ˜¯åœ¨å®‰è£…ffmpegçš„æ—¶å€™ åˆè¯´ç¼ºå°‘makeå‘½ä»¤
è¿™ä¸ªæ—¶å€™éœ€è¦å®‰è£…makeå‘½ä»¤äº†
é“¾æ¥ï¼šhttps://pan.baidu.com/s/1Q9JMA7WcyEyhH4vFerGQSQ
æå–ç ï¼šw8w4

```java
ä¸Šä¼ åŒ…ä¹‹åæ·»åŠ åˆ°è§£å‹
rpm -ivh make-3.82-24.el7.x86_64.rpm
```

åœ¨æ‰§è¡Œmakeå‘½ä»¤å°±å¯ä»¥äº†

```python
docker run -it -p 5000:5000 --name caijian \ -v /root/swift:/home/project \ -v /root/checkpoint:/root/checkpoint \ -v /root/bge-small-zh-v1.5:/root/bge-small-zh-v1.5 \ --gpus all \ registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.13.1 /bin/bash
```



### åˆ›å»ºå¹¶å¯åŠ¨å®¹å™¨

docker run -it -p 2000:2000 --name caijian -v /root/swift:/home/project -v /root/checkpoint:/root/checkpoint -v /root/bge-small-zh-v1.5:/root/bge-small-zh-v1.5  registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.13.1 /bin/bash



å®¹å™¨å†…æŠ¥é”™

ile "/root/.cache/huggingface/modules/transformers_modules/checkpoint/modeling_baichuan.py", line 658, in from_pretrained
    return super(BaichuanForCausalLM, cls).from_pretrained(pretrained_model_name_or_path, *model_args, 
  File "/opt/conda/envs/swift/lib/python3.10/site-packages/modelscope/utils/hf_util.py", line 76, in from_pretrained
    return ori_from_pretrained(cls, model_dir, *model_args, **kwargs)
  File "/opt/conda/envs/swift/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3032, in from_pretrained
    raise RuntimeError("No GPU found. A GPU is needed for quantization.")
RuntimeError: No GPU found. A GPU is needed for quantization.



å¯èƒ½éœ€è¦åŠ ä¸ŠGPUä¿¡æ¯

docker run -it -p 5000:5000 --name caijian \ -v /root/swift:/home/project \ -v /root/checkpoint:/root/checkpoint \ -v /root/bge-small-zh-v1.5:/root/bge-small-zh-v1.5 \ --gpus all \ registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.1.0-py310-torch2.1.2-tf2.14.0-1.13.1 /bin/bash

```

```



```
LoRAå¾®è°ƒå:

CUDA_VISIBLE_DEVICES=0 swift deploy --ckpt_dir xxx/checkpoint-xxx
# ä½¿ç”¨VLLMåŠ é€Ÿ
CUDA_VISIBLE_DEVICES=0 swift deploy \
    --ckpt_dir output/baichuan2-7b/v11-20240511-210615/checkpoint-1200 --merge_lora false \
    --infer_backend vllm --max_model_len 2048
```

è§£å†³æŠ¥é”™çš„åŠæ³• 

 File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 235, in <module>
    from torch._C import *  # noqa: F403
ImportError: libnccl.so.2: cannot open shared object file: No such file or directory
(moe) root@iZwz9g714w5ed2y7c4elkvZ:~/swift/examples/pytorch/llm# find / -name libnccl.so.2
/var/lib/docker/overlay2/060e073ae8d6c6aeded9fc5dec7335b687096bf7aae9ac9ddf6d067ae897b362/diff/usr/lib/x86_64-linux-gnu/libnccl.so.2
/var/lib/docker/overlay2/e6daa29a989f460443b5b78aeece2b3be915458f4d6a43ce86e5d84adc1f9f0e/diff/usr/lib/x86_64-linux-gnu/libnccl.so.2
/var/lib/docker/overlay2/c4caefdcfa47deaefbe289478168761a3a38ab7baf02f840d0cb2328591dd73e/merged/usr/lib/x86_64-linux-gnu/libnccl.so.2
/root/miniconda3/lib/python3.12/site-packages/nvidia/nccl/lib/libnccl.so.2
/root/miniconda3/envs/moe/lib/python3.10/site-packages/nvidia/nccl/lib/libnccl.so.2
/root/miniconda3/envs/sdwebui/lib/python3.10/site-packages/nvidia/nccl/lib/libnccl.so.2

è§£å†³åŠæ³•ï¼š

export LD_LIBRARY_PATH=/root/miniconda3/envs/moe/lib/python3.10/site-packages/nvidia/nccl/lib:$LD_LIBRARY_PATH





File "/root/swift/examples/pytorch/llm/my_sft.py", line 4, in <module>
    import torch
  File "/usr/local/lib/python3.10/dist-packages/torch/__init__.py", line 235, in <module>
    from torch._C import *  # noqa: F403
ImportError: libnccl.so.2: cannot open shared object file: No such file or directory

è§£å†³åŠæ³•ï¼š

export LD_LIBRARY_PATH=/root/miniconda3/envs/moe/lib/python3.10/site-packages/nvidia/nccl/lib:$LD_LIBRARY_PATH



æ¨ç†å‘é€è¯·æ±‚ è¯•è¯•ï¼š

```
curl http://localhost:8000/v1/completions \
-H "Content-Type: application/json" \
-d '{
"model": "baichuan2-7b",
"prompt": "ä½ å¥½ï¼Œæˆ‘æœ‰ä¸€æ®µæ–‡æœ¬éœ€è¦å¤„ç†,è¯·ä½ å°†æ–‡æœ¬ä»¥å¥å­ä¸ºå•ä½è¿›è¡Œåˆ‡åˆ†,éœ€è¦ç‰¹åˆ«æ³¨æ„ä¸­æ–‡çš„å¥å­è¾¹ç•Œå¯èƒ½ä¸ä»…ä»…ä¾èµ–äºæ ‡ç‚¹ç¬¦å·ï¼Œè¿˜éœ€è¦è€ƒè™‘è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹,å„è‡ªåˆ†å‰²ä¼šçªå‡ºå„ä¸ªå¥å­çš„ç‹¬ç«‹æ„ä¹‰ã€‚ä½ èƒ½å¸®æˆ‘åˆ†å‰²å®ƒå—ï¼Ÿåœ¨æ¯ä¸ªå¥å­ç»“æŸåï¼Œä½¿ç”¨æ¢è¡Œç¬¦æ¥åŒºåˆ†ã€‚è°¢è°¢ï¼ä»¥ä¸‹æ˜¯æˆ‘ç»™å‡ºçš„ç¤ºä¾‹:\nè¾“å…¥:ä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿæœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nå¥å­åˆ†å‰²:\nä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿ\næœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nè¾“å…¥:ä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢ä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nå¥å­åˆ†å‰²:\nä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢\nä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nè¾“å…¥:æˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nå¥å­åˆ†å‰²:\næˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nè¾“å…¥:å‘å•¥å¿«é€’\nå¥å­åˆ†å‰²:\nå‘å•¥å¿«é€’\nè¯·ä½ å‚è€ƒæˆ‘ç»™å‡ºçš„ç¤ºä¾‹å¹¶ç»“åˆæ ‡ç‚¹ç¬¦å·,è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹è¿›è¡Œç»™å®šæ–‡æœ¬çš„å¥å­åˆ†å‰²ã€‚ç°ç»™å‡ºçš„æ–‡æœ¬æ˜¯[ä»€ä¹ˆæ—¶å€™å‘è´§å‘¢ï¼Ÿä½ å¥½ï¼Œè¯·é—®å¯ä»¥ç”¨å¾®ä¿¡æ”¯ä»˜å—]",
"max_tokens": 1024,
"temperature": 0.1,
"seed": 42
}'
```



```
curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{
"model": "baichuan2-7b",
"prompt": "æµ™æ±Ÿ -> æ­å·\nå®‰å¾½ -> åˆè‚¥\nå››å· ->",
"max_tokens": 256,
"temperature": 0
}'
```



{"query": "ä½ å¥½ï¼Œæˆ‘æœ‰ä¸€æ®µæ–‡æœ¬éœ€è¦å¤„ç†,è¯·ä½ å°†æ–‡æœ¬ä»¥å¥å­ä¸ºå•ä½è¿›è¡Œåˆ‡åˆ†,éœ€è¦ç‰¹åˆ«æ³¨æ„ä¸­æ–‡çš„å¥å­è¾¹ç•Œå¯èƒ½ä¸ä»…ä»…ä¾èµ–äºæ ‡ç‚¹ç¬¦å·ï¼Œè¿˜éœ€è¦è€ƒè™‘è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹,å„è‡ªåˆ†å‰²ä¼šçªå‡ºå„ä¸ªå¥å­çš„ç‹¬ç«‹æ„ä¹‰ã€‚ä½ èƒ½å¸®æˆ‘åˆ†å‰²å®ƒå—ï¼Ÿåœ¨æ¯ä¸ªå¥å­ç»“æŸåï¼Œä½¿ç”¨æ¢è¡Œç¬¦æ¥åŒºåˆ†ã€‚è°¢è°¢ï¼ä»¥ä¸‹æ˜¯æˆ‘ç»™å‡ºçš„ç¤ºä¾‹:\nè¾“å…¥:ä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿæœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nå¥å­åˆ†å‰²:\nä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿ\næœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nè¾“å…¥:ä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢ä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nå¥å­åˆ†å‰²:\nä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢\nä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nè¾“å…¥:æˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nå¥å­åˆ†å‰²:\næˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nè¾“å…¥:å‘å•¥å¿«é€’\nå¥å­åˆ†å‰²:\nå‘å•¥å¿«é€’\nè¯·ä½ å‚è€ƒæˆ‘ç»™å‡ºçš„ç¤ºä¾‹å¹¶ç»“åˆæ ‡ç‚¹ç¬¦å·,è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹è¿›è¡Œç»™å®šæ–‡æœ¬çš„å¥å­åˆ†å‰²ã€‚ç°ç»™å‡ºçš„æ–‡æœ¬æ˜¯[ä»€ä¹ˆæ—¶å€™å‘è´§å‘¢ï¼Ÿä½ å¥½ï¼Œè¯·é—®å¯ä»¥ç”¨å¾®ä¿¡æ”¯ä»˜å—]", "response": "ä»€ä¹ˆæ—¶å€™å‘è´§å‘¢ï¼Ÿ\nä½ å¥½ï¼Œè¯·é—®å¯ä»¥ç”¨å¾®ä¿¡æ”¯ä»˜å—"}



{"query": "ä½ æ˜¯å……æ»¡æ™ºæ…§çš„è¯­ä¹‰ç†è§£æœºå™¨äººï¼Œä½ èƒ½æ ¹æ®å¯¹è¯å†…å®¹é€‰æ‹©æœ€é€‚åˆçš„è¯­ä¹‰ç±»å‹\n    é—®é¢˜ï¼šè¯·ä¸ºä»¥ä¸‹å¯¹è¯è¿›è¡Œåˆ†ç±»ï¼š\n    å¯¹è¯å†…å®¹ï¼š[è¾“å…¥å¯¹è¯å†…å®¹]\n    é€‰é¡¹ï¼š*ä½¿ç”¨åœºæ‰€\n*é€€æ¬¾æˆåŠŸ\n*æ¶ˆè´¹è€…ç»™å¥½è¯„\n*æœ‰æ— å®ä½“åº—é“º\n*ä½ä»·åŸå› \n*æ˜¯å¦é™éŸ³\n*èƒ½å¦å¤šé€\n*æ˜¯å¦æœ‰èµ å“\n*æ¶ˆè´¹è€…å‘èµ·æŠ•è¯‰\n*æ²¡æœ‰æ”¶åˆ°èµ å“\n*èƒ½å¦æ‰¹å‘\n*æŒ‡å®šå¿«é€’\n*å’¨è¯¢å®¢æœä¸Šç­æ—¶é—´\n*æ¶ˆè´¹è€…è¦æ±‚å»¶è¿Ÿå‘è´§\n*æ¶ˆè´¹è€…å·²æäº¤èµ„æ–™ç­‰\n*å’¨è¯¢äº§å“æè´¨\n*å¼€ç¥¨è¿›åº¦æŸ¥è¯¢\n*åœè¿å‘è´§æ—¶é—´\n*æè¿°ä¸ç¬¦\n*å®šé‡‘æ— æ³•æ”¯ä»˜\n*æ¶ˆè´¹è€…ç”³è¯·é€€è´§\n*å¦‚ä½•æ‹’æ”¶\n*æ¶ˆè´¹è€…å’¨è¯¢äº§å“é‡é‡\n*èƒ½å¦ç›´æ’­ä»·è´­ä¹°\n*ä¼˜æƒ åˆ¸é¢†å–æ¬¡æ•°\n*è½¬äººå·¥\n*å‘ä»€ä¹ˆå¿«é€’\n*æœ‰æ— å‘è´§æ¸…å•\n*å‘è´§åœ°ç‚¹\n*æ˜¯å¦æ­£å“\n*äº§å“å·²ä¸‹æ¶\n*å‘è´§æ—¶é—´\n*æ¶ˆè´¹è€…è¯·æ±‚ä¿®æ”¹åœ°å€\n*å’¨è¯¢æ˜¯å¦åŒ…é‚®\n*ä½¿ç”¨äººç¾¤\n*æœªæ”¶åˆ°ä½†æ˜¾ç¤ºå·²ç­¾æ”¶\n*è¿è´¹é™©ç†èµ”\n*å–æ¶ˆè®¢å•\n*æ¶ˆè´¹è€…è¦æ±‚é€€è¿è´¹\n*æ¶ˆè´¹è€…å–æ¶ˆé€€æ¬¾é€€è´§\n*æ¶ˆè´¹è€…ç”³è¯·é€€æ¬¾é€€è´§\n*æ— æ³•æ”¯ä»˜å°¾æ¬¾\n*æ¶ˆè´¹è€…è¦æ±‚å»¶é•¿æ”¶è´§æ—¶é—´\n*ä»˜æ¬¾å¤±è´¥\n*æ˜¯å¦é¢„å”®æ¬¾\n*å…¶ä»–\n*å·²æ·»åŠ å¥½å‹\n*å’¨è¯¢äº§å“è´¨é‡\n*å’¨è¯¢ä¿æ¸©æ•ˆæœå¦‚ä½•\n*å¯ä»¥æ‰“å°å½©è‰²å›¾ç‰‡å—\n*æ— æ³•ç”³è¯·é€€æ¬¾\n*æ¶ˆè´¹è€…åé¦ˆè´¨é‡é—®é¢˜\n*æ— æ³•é¢†å–ä¼˜æƒ åˆ¸\n*å’¨è¯¢é…æ–™è¡¨\n*å’¨è¯¢æ¢è´§æµç¨‹\n*æ¶ˆè´¹è€…è¯¢é—®åŠ å·¥åˆ¶ä½œè´¹\n*æ˜¯å¦ä¸Šé—¨å®‰è£…\n*æ¶ˆè´¹è€…ç”³è¯·é€€æ¬¾\n*å¾®ä¿¡æ”¯ä»˜\n*è¯·æ±‚èµ é€ä¼˜æƒ åˆ¸\n*å’¨è¯¢åˆ°è´§æ—¶é—´\n*èƒ½å¦å®šåˆ¶\n*è´§åˆ°ä»˜æ¬¾\n*æ¶ˆè´¹è€…å·²ç»æ‹’æ”¶å¿«é€’\n*è®¢å•å·²ä¸‹å•/å·²æ”¯ä»˜\n*æ‰“æ‹›å‘¼\n*ä¿¡ç”¨å¡æ”¯ä»˜\n*æ¶ˆè´¹è€…æ²¡æœ‰æ”¶åˆ°äº§å“\n*å¦‚ä½•æŸ¥è¯¢æˆ–ä¸‹è½½å‘ç¥¨\n*æœ‰æ— å‘ç¥¨\n*æ¶ˆè´¹è€…ç»™å·®è¯„\n*ä¿è´¨æœŸ\n*ä½¿ç”¨åŠæ•™å­¦è§†é¢‘\n*å…¨å›½è”ä¿\n*å¦‚ä½•æ”¯ä»˜å®šé‡‘\n*ç‰©æµæ— æ›´æ–°\n*ç‰©æµæ½ä»¶ä¸­\n*è¦ä¸‹è½½ä»€ä¹ˆè½¯ä»¶\n*æ¶ˆè´¹è€…ç»“æŸå¯¹è¯\n*å’¨è¯¢äº§å“çš„ä½¿ç”¨å‘¨æœŸ\n*æœ‰æ— è¿è´¹é™©\n*å®‰è£…å¼‚å¸¸\n*æ˜¯å¦å¥½è¯„è¿”ç°\n*æ¶ˆè´¹è€…åé¦ˆåŒ…è£…å·®\n*é—®å¥½\n*æ¶ˆè´¹è€…å¸Œæœ›ä¿®æ”¹ä»·æ ¼\n*å®‰è£…ç½‘ç‚¹\n*å®‰è£…æ•™ç¨‹\n*å’¨è¯¢æœ‰æ²¡æœ‰å¼‚å‘³\n*æ˜¯å¦é™è´­\n*å’¨è¯¢ç”Ÿäº§æ—¥æœŸ\n*å’¨è¯¢å¯ä»¥å‘è´§çš„åœ°åŒº\n*è´§æºäº§åœ°\n*æ¶ˆè´¹è€…è¡¨ç¤ºç–‘é—®\n*æ¶ˆè´¹è€…å·²åšå¤‡æ³¨\n*åº“å­˜æ˜¯å¦å……è¶³\n*æ¶ˆè´¹è€…ç´¢è¦è”ç³»æ–¹å¼\n*å‘ç¥¨ç±»å‹\n*åŠ æ€¥å‘è´§\n*ç”¨æˆ·è¡¨ç¤ºå¦å®š\n*æ¶ˆè´¹è€…è¯·æ±‚å¤‡æ³¨\n    è¯·æ ¹æ®å¯¹è¯å†…å®¹é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»ï¼ˆè¾“å…¥é€‰é¡¹ç¼–å·ï¼‰ï¼š\n    å¯¹è¯å†…å®¹ï¼š[è´¨é‡ä¸å¥½]", "response": "æ¶ˆè´¹è€…åé¦ˆè´¨é‡é—®é¢˜"}



å¾®è°ƒæ ·ä¾‹

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch

from swift.llm import (
    DatasetName, InferArguments, ModelType, SftArguments,
    infer_main, sft_main, app_ui_main
)

model_type = ModelType.qwen_7b_chat
sft_args = SftArguments(
    model_type=model_type,
    train_dataset_sample=2000,
    dataset=[DatasetName.blossom_math_zh],
    output_dir='output')
result = sft_main(sft_args)
best_model_checkpoint = result['best_model_checkpoint']
print(f'best_model_checkpoint: {best_model_checkpoint}')
torch.cuda.empty_cache()

infer_args = InferArguments(
    ckpt_dir=best_model_checkpoint,
    load_dataset_config=True,
    val_dataset_sample=10)
# merge_lora(infer_args, device_map='cpu')
result = infer_main(infer_args)
torch.cuda.empty_cache()
```



### å¾®è°ƒåæ¨¡å‹



**å•æ ·æœ¬æ¨ç†**:

ä½¿ç”¨LoRA**å¢é‡**æƒé‡è¿›è¡Œæ¨ç†:

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type
)
from swift.tuners import Swift

ckpt_dir = 'vx-xxx/checkpoint-100'
model_type = ModelType.qwen_7b_chat
template_type = get_default_template_type(model_type)

model, tokenizer = get_model_tokenizer(model_type, model_kwargs={'device_map': 'auto'})

model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)
template = get_template(template_type, tokenizer)
query = 'xxxxxx'
response, history = inference(model, template, query)
print(f'response: {response}')
print(f'history: {history}')
```



ä½¿ç”¨LoRA **merged**çš„æƒé‡è¿›è¡Œæ¨ç†:

```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type
)

ckpt_dir = 'vx-xxx/checkpoint-100-merged'
model_type = ModelType.qwen_7b_chat
template_type = get_default_template_type(model_type)

model, tokenizer = get_model_tokenizer(model_type, model_kwargs={'device_map': 'auto'},
                                       model_id_or_path=ckpt_dir)

template = get_template(template_type, tokenizer)
query = 'xxxxxx'
response, history = inference(model, template, query)
print(f'response: {response}')
print(f'history: {history}')
```

æ¨ç† æ ·ä¾‹

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    get_model_tokenizer, get_template, inference, ModelType, get_default_template_type
)
from swift.tuners import Swift

ckpt_dir = 'vx-xxx/checkpoint-100'
model_type = ModelType.qwen_7b_chat
template_type = get_default_template_type(model_type)

model, tokenizer = get_model_tokenizer(model_type, model_kwargs={'device_map': 'auto'})

model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)
template = get_template(template_type, tokenizer)
query = 'xxxxxx'
response, history = inference(model, template, query)
print(f'response: {response}')
print(f'history: {history}')
```



model = Swift.from_pretrained(model, ckpt_dir, inference_mode=True)
template = get_template(template_type, tokenizer)
query = "ä½ å¥½ï¼Œæˆ‘æœ‰ä¸€æ®µæ–‡æœ¬éœ€è¦å¤„ç†,è¯·ä½ å°†æ–‡æœ¬ä»¥å¥å­ä¸ºå•ä½è¿›è¡Œåˆ‡åˆ†,éœ€è¦ç‰¹åˆ«æ³¨æ„ä¸­æ–‡çš„å¥å­è¾¹ç•Œå¯èƒ½ä¸ä»…ä»…ä¾èµ–äºæ ‡ç‚¹ç¬¦å·ï¼Œè¿˜éœ€è¦è€ƒè™‘è¯­å¢ƒå’Œè¯­ä¹‰å†…>å®¹,å„è‡ªåˆ†å‰²ä¼šçªå‡ºå„ä¸ªå¥å­çš„ç‹¬ç«‹æ„ä¹‰ã€‚ä½ èƒ½å¸®æˆ‘åˆ†å‰²å®ƒå—ï¼Ÿåœ¨æ¯ä¸ªå¥å­ç»“æŸåï¼Œä½¿ç”¨æ¢è¡Œç¬¦æ¥åŒºåˆ†ã€‚è°¢è°¢ï¼ä»¥ä¸‹æ˜¯æˆ‘ç»™å‡ºçš„ç¤ºä¾‹:\nè¾“å…¥:ä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿæœ€>æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nå¥å­åˆ†å‰²:\nä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿ\næœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nè¾“å…¥:ä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢ä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nå¥å­åˆ†å‰²:\nä¸ºä»€ä¹ˆä¼šæ¯”å…¶>ä»–åšä¸»ä¾¿å®œå‘¢\nä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nè¾“å…¥:æˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nå¥å­åˆ†å‰²:\næˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nè¾“å…¥:å‘å•¥å¿«é€’\nå¥å­åˆ†å‰²:\nå‘å•¥å¿«é€’\nè¯·ä½ å‚
è€ƒæˆ‘ç»™å‡ºçš„ç¤ºä¾‹å¹¶ç»“åˆæ ‡ç‚¹ç¬¦å·,è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹è¿›è¡Œç»™å®šæ–‡æœ¬çš„å¥å­åˆ†å‰²ã€‚ç°ç»™å‡ºçš„æ–‡æœ¬æ˜¯[ä»€ä¹ˆæ—¶å€™å‘è´§å‘¢ï¼Ÿä½ å¥½ï¼Œè¯·é—®å¯ä»¥ç”¨å¾®ä¿¡æ”¯ä»˜å—]"
response, history = inference(model, template, query)
print(f'response: {response}')
print(f'history: {history}')



å®šä¹‰å­—æ®µ CustomerType

ç”µå•† (E-commerce)     001

é¤é¥® (Food and Beverage)    002

æ±½è½¦ (Automotive)    003

åŒ»ç¾ (Medical Aesthetics)    004

ä¿¡æ¯æŠ€æœ¯ä¸æœåŠ¡ (Information Technology and Services)  005

é‡‘èæœåŠ¡ (Financial Services)   006

æ•™è‚² (Education)   007

åŒ»ç–—ä¿å¥ (Healthcare)   008

åˆ¶é€ ä¸š (Manufacturing)  009

å†œä¸š (Agriculture)  010

èƒ½æº (Energy)   011

åª’ä½“ä¸å¨±ä¹ (Media and Entertainment)  012

å»ºç­‘ä¸æˆ¿åœ°äº§ (Construction and Real Estate)  013

è¿è¾“ä¸ç‰©æµ (Transportation and Logistics)   014

é›¶å”® (Retail)   015

ç¯å¢ƒç§‘å­¦ä¸ç®¡ç† (Environmental Science and Management)  016

æ—…æ¸¸ä¸š (Tourism)  017

å…¬å…±äº‹ä¸š (Utilities)  018





output/baichuan2-7b/v16-20240523-145743/checkpoint-1724-merged

é‡åŒ–æ¨ç† éƒ¨ç½²å‘½ä»¤

CUDA_VISIBLE_DEVICES=0 swift deploy --ckpt_dir 'output/baichuan2-7b/v16-20240523-145743/checkpoint-1724-awq-int4' --dtype fp16





'query': 'ç”¨ä»€ä¹ˆå¿«é€’', 'answer': 'å‘æå…”ä¸­é€šå¿«é€’çš„äº²äº²~ ä¸æ”¯æŒæŒ‡å®šå¿«é€’'

'human_reply': 'äº²äº²ï¼Œæˆ‘ä»¬ç”¨çš„æ˜¯è·³è·ƒçš„å°å…”æå…”ä¸­é€šå¿«é€’å“¦~, å¿«é€’å°å“¥ä¼šé£å¿«åœ°æŠŠå®è´é€åˆ°ä½ æ‰‹ä¸Šï½ç”Ÿè‚–å…”å¹´ï¼Œå¿«é€’ä¹Ÿè¹¦è¹¦è·³è·³çš„å‘¢ğŸ°ğŸ’¨'

'query': 'è¯·æä¾›äº§å“åŒ…è£…å›¾ç‰‡', 'answer': 'æ²¡æœ‰'

 'human_reply': 'äº²äº²ï¼Œå¾ˆæŠ±æ­‰ï¼Œäº§å“åŒ…è£…å›¾ç‰‡æš‚æ—¶æ— æ³•æä¾›ï¼Œä½†æˆ‘å¯ä»¥æè¿°ä¸€ä¸‹æˆ‘ä»¬çš„åŒ…è£…ï¼Œä¿è¯ç²¾ç¾åˆå®‰å…¨ï¼Œå°±åƒç²¾å¿ƒæ‰“æ‰®çš„å°ç¤¼ç‰©ä¸€æ ·æ‹†å¼€ä¼šæœ‰æƒŠå–œå“¦âœ¨ğŸ'

'query': 'æœªå‘è´§è®¢å•æ˜¯å¦å¯ä»¥ç›´æ¥ä¿®æ”¹äº§å“å±æ€§', 'answer': 'å¯ä»¥',

'human_reply': 'äº²äº²ï¼Œæœªå‘è´§çš„è®¢å•å½“ç„¶å¯ä»¥çµæ´»è°ƒæ•´å•¦ï¼Œå°±åƒç©æ‹¼å›¾ä¸€æ ·ï¼Œæ‰¾åˆ°æœ€é€‚åˆä½ çš„é‚£ä¸€å—ï½ğŸ¨ğŸ‘'

'query': 'æµ·è‹”ç”Ÿäº§æ—¥æœŸ', 'answer': 'äº²ï¼Œ3å…‹æµ·è‹”å’Œ4.5å…‹æµ·è‹”ç”Ÿäº§æ—¥æœŸæ˜¯8æœˆä»½çš„ï¼Œä¿è´¨æœŸ12ä¸ªæœˆï¼Œæ—¥æœŸæ–°é²œçš„'

'human_reply': äº²äº²ï¼Œæµ·è‹”å®å®ä»¬éƒ½æ˜¯8æœˆæ–°å‡ºç‚‰çš„å“¦ï¼Œ3å…‹å’Œ4.5å…‹çš„å°å¯çˆ±ä»¬éƒ½è¿˜å«©ç€å‘¢ï¼Œä¿è´¨æœŸè¶³è¶³æœ‰12ä¸ªæœˆå‘¢ğŸ˜å¯†å°å¥½æ”¾å¹²ç‡¥å¤„ï¼Œç¾å‘³éšæ—¶ç­‰ä½ æ¥å°

'query': 'æœªå‘è´§è®¢å•æ˜¯å¦å¯ä»¥ç›´æ¥ä¿®æ”¹äº§å“å±æ€§', 'answer': 'å¯ä»¥'

{'query': 'æœªå‘è´§è®¢å•æ˜¯å¦å¯ä»¥ç›´æ¥ä¿®æ”¹äº§å“å±æ€§', 'human_reply': 'äº²äº²ï¼Œæœªå‘è´§çš„è®¢å•å½“ç„¶å¯ä»¥çµæ´»è°ƒæ•´ï¼Œå°±åƒç©æ‹¼å›¾ä¸€æ ·ï¼Œè¿˜æ²¡å›ºå®šå‰éƒ½å¯ä»¥éšå¿ƒå˜åŠ¨å‘¢ğŸ§©ğŸ˜‰'





```python
CUDA_VISIBLE_DEVICES=0 swift infer \
    --model_type baichuan2-7b \
    --model_id_or_path "output/baichuan2-7b/v16-20240523-145743/checkpoint-1724-awq-int4"
llama3_8b_instruct

`CUD`A_VISIBLE_DEVICES=0 \`
`swift sft \`
    `--model_id_or_path LLM-Research/Meta-Llama-3-8B-Instruct \`
    `--model_revision master \`
    `--sft_type lora \`
    `--tuner_backend peft \`
    `--template_type AUTO \`
    `--dtype AUTO \`
    `--output_dir output \`
    `--dataset great/7b \`
    `--train_dataset_sample -1 \`
    `--num_train_epochs 1 \`
    `--max_length 2048 \`
    `--check_dataset_strategy warning \`
    `--lora_rank 8 \`
    `--lora_alpha 32 \`
    `--lora_dropout_p 0.05 \`
    `--lora_target_modules ALL \`
    `--gradient_checkpointing true \`
    `--batch_size 1 \`
    `--weight_decay 0.1 \`
    `--learning_rate 1e-4 \`
    `--gradient_accumulation_steps 16 \`
    `--max_grad_norm 0.5 \`
    `--warmup_ratio 0.03 \`
    `--eval_steps 100 \`
    `--save_steps 100 \`
    `--save_total_limit 2 \`
    --logging_steps 10 \`

```

nohup bash qianwen.sh > sft.log 2>&1 &



nohup bash sst.sh 1>sss.log 2>&1



```
CUDA_VISIBLE_DEVICES=0 swift export \
    --ckpt_dir 'output/baichuan2-7b/v16-20240523-145743/checkpoint-1450' \
    --merge_lora true --quant_bits 4 \
    --dataset alpaca-zh  --quant_method awq --no_save_safetensors
```



## æ–­ç‚¹ç»­è®­ å‘½ä»¤

```bash
`PYTHONPATH=../../.. \`
`CUDA_VISIBLE_DEVICES=0 \`
`python llm_sft.py \`
    `--model_id_or_path qwen/Qwen-7B-Chat-Int4 \`
    `--model_revision master \`
    `--resume_from_checkpoint "output/qwen-7b-chat-int4/v1-20240527-141259/checkpoint-431" \`
    `--sft_type lora \`
    `--tuner_backend peft \`
    `--template_type AUTO \`
    `--dtype fp16 \`
    `--output_dir output \`
    `--dataset greatheart/7b \`
    `--train_dataset_sample -1 \`
    `--num_train_epochs 3 \`
    `--max_length 1024 \`
    `--check_dataset_strategy warning \`
    `--lora_rank 8 \`
    `--lora_alpha 32 \`
    `--lora_dropout_p 0.05 \`
    `--lora_target_modules ALL \`
    `--gradient_checkpointing true \`
    `--batch_size 1 \`
    `--weight_decay 0.1 \`
    `--learning_rate 1e-4 \`
    `--gradient_accumulation_steps 16 \`
    `--max_grad_norm 0.5 \`
    `--warmup_ratio 0.03 \`
    `--eval_steps 100 \`
    `--save_steps 100 \`
    `--save_total_limit 2 \`
    `--logging_steps 10 \`
    `--use_flash_attn false \`
    `--push_to_hub false \`
    `--check_model_is_latest \`
```





ç”¨æˆ·è¾“å…¥çš„é—®é¢˜ï¼šquery = str(data['query'])

åŒ¹é…çš„é—®é¢˜ï¼š       problem = str(data['problem'])   

åŒ¹é…çš„ç­”æ¡ˆï¼š         answer =data['answer']

é‡‡ç”¨çš„æ¨¡å‹ï¼š			type  

æ‰€å±è¡Œä¸šï¼š				CustomerType

ç”¨æˆ·å						userId



{'query': 'æœ‰æ²¡æœ‰ä¼˜æƒ ',  'problem':"æœ‰æ²¡æœ‰ä¼˜æƒ æ”¿ç­–","answer":"æ‚¨å¥½ï¼Œç›®å‰å°é¹æ±½è½¦",'type': 'qianwen', 'userId': 26, 'CustomerTypeCode': '001', 'CustomerTypeName': 'ç”µå•†'}





# Centos å®‰è£… Python 3.10 å…¨æµç¨‹

### 1.å®‰è£…Python

### ä¸‹è½½pythonæºç 

https://www.python.org/downloads/release/python-31012/

ç½‘å€ https://www.python.org/ftp/python/3.10.8/

### å®‰è£…ä¾èµ–åŒ…

```bash
yum install zlib-devel bzip2-devel opssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc libffi-devel
```

### æ›´æ–°SSLç‰ˆæœ¬

wget --no-check-certificate https://www.openssl.org/source/openssl-1.1.1l.tar.gz
tar -xvf ./openssl-1.1.1l.tar.gz
cd openssl-1.1.1l/
./config shared zlib --prefix=/usr/local/openssl
make && make install

### å®‰è£…Pythonå¤‡ä»½ç°æœ‰çš„openssl
mv /usr/bin/openssl /usr/bin/openssl.bak
mv /usr/include/openssl /usr/include/openssl.bak

### åˆ›å»ºè½¯é“¾
ln -s /usr/local/openssl/bin/openssl /usr/bin/openssl

echo "/usr/local/openssl/lib" >> /etc/ld.so.conf
ldconfig

### æŸ¥çœ‹ç‰ˆæœ¬
openssl version

### å®‰è£…Pythonå…¨æµç¨‹

tar -xvf ./Python-3.10.12.tgz
cd ./Python-3.10.12
./configure --prefix=/usr/local/python --with-openssl=/usr/local/openssl/
make & make install

fi
Looking in links: /tmp/tmp88zr6_85
Processing /tmp/tmp88zr6_85/setuptools-63.2.0-py3-none-any.whl
Processing /tmp/tmp88zr6_85/pip-22.2.2-py3-none-any.whl
Installing collected packages: setuptools, pip
  WARNING: The scripts pip3 and pip3.10 are installed in '/usr/local/python/bin' which is not on PATH.
  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.
Successfully installed pip-22.2.2 setuptools-63.2.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
[1]+  Exit 2                  make
[root@iZwz9h2a6fhpg5rzx8wj2tZ Python-3.10.8]# pythonæŒ‰ç…§ç»™å‡ºçš„å‘½ä»¤ç¼–è¯‘å¹¶å®‰è£…äº†Python 3.10.12åˆ°æŒ‡å®šçš„ç›®å½•ï¼ˆ`/usr/local/python`ï¼‰ï¼Œé‚£ä¹ˆå¯æ‰§è¡Œæ–‡ä»¶é€šå¸¸ä¼šä½äºè¯¥ç›®å½•ä¸‹çš„`bin/`å­ç›®å½•ä¸­ã€‚

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤æ¥æŸ¥æ‰¾å·²å®‰è£…çš„Python 3.10çš„è·¯å¾„ï¼š

```
bashå¤åˆ¶ä»£ç /usr/local/python/bin/python3 --version
```

è¿™åº”è¯¥ä¼šæ˜¾ç¤ºPythonçš„ç‰ˆæœ¬ï¼Œç±»ä¼¼äºè¿™æ ·ï¼š

```
bashå¤åˆ¶ä»£ç Python 3.10.12
```

/usr/local/python/bin/python3 --version
Python 3.10.8

å¦‚æœä½ æƒ³ä½¿ç”¨è¿™ä¸ªæ–°å®‰è£…çš„Pythonç‰ˆæœ¬ä½œä¸ºé»˜è®¤çš„`python3`å‘½ä»°ï¼Œä½ å¯ä»¥å°†å…¶æ·»åŠ åˆ°ç³»ç»Ÿçš„`PATH`ç¯å¢ƒå˜é‡ä¸­ã€‚å¯ä»¥ä¸´æ—¶ä¿®æ”¹`PATH`æˆ–è€…ä¿®æ”¹é…ç½®æ–‡ä»¶ï¼ˆå¦‚`.bashrc`æˆ–`.zshrc`ï¼‰ï¼Œä»¥ä¾¿æ°¸ä¹…æ›´æ”¹ã€‚

ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ°æ‚¨çš„ç”¨æˆ·ä¸»ç›®å½•ä¸‹çš„`.bashrc`æ–‡ä»¶ä¸­ï¼š

```
bashå¤åˆ¶ä»£ç # Set Python-3.10.12 as the default python3 version
export PATH="/usr/local/python/bin:$PATH"
```

åœ¨ä¿®æ”¹`.bashrc`åï¼Œæ‚¨éœ€è¦æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä½¿æ›´æ”¹ç”Ÿæ•ˆï¼š

```
bashå¤åˆ¶ä»£ç source ~/.bashrc
```

æˆ–è€…ï¼Œæ‚¨å¯ä»¥é‡æ–°ç™»å½•ä¼šè¯ã€‚

ç°åœ¨ï¼Œå½“æ‚¨é”®å…¥`python3`æ—¶ï¼Œåº”è¯¥å°±ä¼šé»˜è®¤ä½¿ç”¨æ–°å®‰è£…çš„Python 3.10.12ã€‚æ‚¨å¯ä»¥é€šè¿‡é”®å…¥ä»¥ä¸‹å‘½ä»¤æ¥ç¡®è®¤å®ƒï¼š

```
bashå¤åˆ¶ä»£ç   python3 --version
```

ç¡®è®¤`/usr/local/python/bin/python3`ç¡®å®æ˜¯Python 3.10.12çš„è·¯å¾„ï¼š /usr/local/python/bin/python3 --version

rm -i /usr/bin/python   rm: remove symbolic link '/usr/bin/python'? 

æ ¹æ®æ‚¨æä¾›çš„ç»ˆç«¯è¾“å‡ºï¼Œæ‚¨å·²ç»ä½œä¸º root ç”¨æˆ·æ­£åœ¨æ“ä½œç³»ç»Ÿã€‚

1. æ‚¨é¦–å…ˆå°è¯•ä½¿ç”¨ `rm -i /usr/bin/python` å‘½ä»¤åˆ é™¤ `/usr/bin/python` ç¬¦å·é“¾æ¥ï¼Œä½†ç³»ç»Ÿè¦æ±‚æ‚¨ç¡®è®¤åˆ é™¤ã€‚çœ‹èµ·æ¥æ‚¨æ²¡æœ‰åœ¨è¿™é‡Œç»™å‡ºç¡®è®¤ï¼ˆé€šå¸¸æ˜¯è¾“å…¥yå¹¶æ•²å‡»å›è½¦ï¼‰ï¼Œå› æ­¤é“¾æ¥å¹¶æœªè¢«åˆ é™¤ã€‚
2. ç„¶åï¼Œæ‚¨å°è¯•ä½¿ç”¨ `ln -s /usr/local/python/bin/python3.10 /usr/bin/python` å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°çš„ç¬¦å·é“¾æ¥ï¼Œä½¿ `/usr/bin/python` é“¾æ¥åˆ° `/usr/local/python/bin/python3.10`ã€‚ä½†æ˜¯å› ä¸º `/usr/bin/python` æ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œå¹¶æ²¡æœ‰è¢«å…ˆå‰çš„ `rm` å‘½ä»¤åˆ é™¤ï¼Œæ‰€ä»¥ç³»ç»ŸæŠ¥å‘Šè¯´ "File exists"ï¼Œå³æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ— æ³•åˆ›å»ºæ–°é“¾æ¥ã€‚
3. å½“æ‚¨å°è¯•è¿è¡Œ `/usr/bin/python` æ—¶ï¼ŒPython 3.6.8 è¢«å¯åŠ¨ã€‚è¿™è¯´æ˜ `/usr/bin/python` æŒ‡å‘ Python 3.6.8 çš„å®‰è£…ä½ç½®ã€‚

è¦è§£å†³è¿™ç§æƒ…å†µï¼Œå¹¶æˆåŠŸåˆ›å»ºæ–°çš„ç¬¦å·é“¾æ¥ï¼Œæ‚¨éœ€è¦é¦–å…ˆç¡®è®¤å¹¶åˆ é™¤ç°æœ‰çš„ `/usr/bin/python` ç¬¦å·é“¾æ¥ï¼š

ä¸€æ—¦é“¾æ¥è¢«åˆ é™¤ï¼Œæ‚¨å°±å¯ä»¥åˆ›å»ºæ–°çš„ç¬¦å·é“¾æ¥äº†ï¼š

```
shellå¤åˆ¶ä»£ç ln -s /usr/local/python/bin/python3.10 /usr/bin/python
```

pip install --upgrade pip 

pip install --upgrade setuptools --user

pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

## pip install è®¾ç½®æ¸…åæº

pip install torch==2.2.2

pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple

pip install onnxruntime

## pipæŸ¥çœ‹ç‰ˆæœ¬

pip show onnxruntime

åˆ›å»ºpython3åˆ°pythonçš„è½¯é“¾æ¥

mkdir -p ~/bin

ln -s /usr/bin/python3 ~/bin/python

export PATH="$HOME/bin:$PATH"

# CentOSä¸Šå®‰è£…Minicondaçš„æ­¥éª¤

åœ¨CentOSä¸Šå®‰è£…Minicondaçš„æ­¥éª¤å¦‚ä¸‹ï¼š

## 1.**è®¾ç½®æ‰§è¡Œæƒé™**: è®©ä¸‹è½½çš„è„šæœ¬å¯æ‰§è¡Œã€‚

chmod +x Miniconda3-py310_24.4.0-0-Linux-x86_64.sh

## 2**è¿è¡Œå®‰è£…è„šæœ¬**: æ‰§è¡Œè¯¥è„šæœ¬æ¥å®‰è£…Miniconadaã€‚

./Miniconda3-py310_24.4.0-0-Linux-x86_64.sh

1. **éµå¾ªå®‰è£…è¿‡ç¨‹**: å®‰è£…ç¨‹åºä¼šè¯¢é—®ä½ å‡ ä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬å®‰è£…ä½ç½®å’Œæ˜¯å¦å°†condaåˆå§‹åŒ–åˆ°ä½ çš„shellä¸­ã€‚å¦‚æœä¸ç¡®å®šï¼Œé€šå¸¸é€‰æ‹©é»˜è®¤å€¼å³å¯ã€‚å½“æç¤ºæ—¶ï¼Œå¯ä»¥æ•²å‡»`Enter`é”®æ¥å—é»˜è®¤å®‰è£…ä½ç½®ï¼Œæˆ–è€…æŒ‡å®šæ–°çš„å®‰è£…è·¯å¾„ã€‚

   å½“è¯¢é—®æ˜¯å¦å°†condaåˆå§‹åŒ–åˆ°ä½ çš„Shellæ—¶ï¼Œè¾“å…¥`yes`ç¡®ä¿ä½ å¯ä»¥ç›´æ¥åœ¨å½“å‰Shellä¸­ä½¿ç”¨condaå‘½ä»¤ã€‚

2. **æ¿€æ´»å®‰è£…**: å®‰è£…å®Œæˆåï¼Œä½ å¯èƒ½éœ€è¦æ‰‹åŠ¨æ¿€æ´»Condaç¯å¢ƒæˆ–é‡æ–°æ‰“å¼€ä¸€ä¸ªæ–°çš„ç»ˆç«¯ï¼ˆShellï¼‰çª—å£ä»¥ä½¿æ”¹åŠ¨ç”Ÿæ•ˆã€‚å¦‚æœå®‰è£…æ—¶åŒæ„äº†è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆæ­¥éª¤4ï¼‰ï¼Œå¯èƒ½éœ€æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤åŠ è½½æ–°çš„ç¯å¢ƒå˜é‡ï¼š

   å¯¹äºbash Shellï¼š

## 3.æ¿€æ´»

source ~/.bashrc

## 4.æµ‹è¯•

conda --version

## 5.åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

conda create --name myenv python=3.8

conda activate myenv

conda deactivate



## **æŸ¥çœ‹å½“å‰æ¿€æ´»çš„ç¯å¢ƒ**ï¼š 

###### å¦‚æœä½ å·²ç»æ¿€æ´»äº†ä¸€ä¸ªCondaç¯å¢ƒï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥æŸ¥çœ‹å½“å‰ç¯å¢ƒçš„è·¯å¾„ï¼š

```
bash
conda info
```

pip install torch æŠ¥é”™ note: This error originates from a subprocess, and is likely not a problem with pip. æ€ä¹ˆè§£å†³

è¿™å°†æ˜¾ç¤ºå½“å‰æ¿€æ´»çš„ç¯å¢ƒä¿¡æ¯ï¼ŒåŒ…æ‹¬å…¶è·¯å¾„

1. **32ä½Pythonä¸æ”¯æŒ**ï¼šPyTorch éœ€è¦64ä½çš„Pythonç¯å¢ƒã€‚ä½ éœ€è¦ç¡®è®¤ä½ çš„Pythonç¯å¢ƒæ˜¯64ä½çš„ã€‚å¯ä»¥é€šè¿‡åœ¨å‘½ä»¤è¡Œä¸­è¿è¡Œ `python -c "import platform; print(platform.architecture()[0])"` æ¥æ£€æŸ¥ä½ çš„Pythonæ˜¯32ä½è¿˜æ˜¯64ä½ã€‚
2. **pipç‰ˆæœ¬é—®é¢˜**ï¼šç¡®ä¿ä½ çš„pipæ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚å¯ä»¥é€šè¿‡è¿è¡Œ `pip install --upgrade pip` æ¥æ›´æ–°pipã€‚
3. **å®‰è£…åŒ…æ ¼å¼é—®é¢˜**ï¼šé€šå¸¸ï¼Œæˆ‘ä»¬ä¸ä¼šä¸‹è½½ `.tar.gz` æ ¼å¼çš„PyTorchå®‰è£…åŒ…æ¥ä½¿ç”¨ `pip` å®‰è£…ã€‚PyTorchçš„å®˜æ–¹æ¨èæ˜¯é€šè¿‡ `pip install torch torchvision` æˆ–è€… `conda install pytorch torchvision -c pytorch` æ¥å®‰è£…ã€‚å¦‚æœä½ éœ€è¦ç‰¹å®šç‰ˆæœ¬çš„PyTorchï¼Œä½ å¯ä»¥è®¿é—® PyTorch å®˜ç½‘çš„å®‰è£…æŒ‡å—ï¼Œæ ¹æ®ä½ çš„ç³»ç»Ÿé…ç½®é€‰æ‹©åˆé€‚çš„å®‰è£…å‘½ä»¤ã€‚
4. **ç½‘ç»œé—®é¢˜**ï¼šä½ ä½¿ç”¨çš„æ˜¯æ¸…åå¤§å­¦çš„é•œåƒæºï¼Œå¦‚æœè¯¥æºä¸å¯ç”¨æˆ–è®¿é—®é€Ÿåº¦æ…¢ï¼Œå¯èƒ½ä¼šå¯¼è‡´å®‰è£…å¤±è´¥ã€‚ä½ å¯ä»¥å°è¯•æ›´æ¢ä¸ºå…¶ä»–é•œåƒæºï¼Œå¦‚ä½¿ç”¨é»˜è®¤çš„PyPIæºï¼Œæˆ–è€…ä½¿ç”¨é˜¿é‡Œäº‘ã€ä¸­å›½ç§‘æŠ€å¤§å­¦ç­‰å…¶ä»–é•œåƒæºã€‚
5. **ä¾èµ–é—®é¢˜**ï¼šæœ‰æ—¶å€™ï¼Œå®‰è£…PyTorchä¹‹å‰éœ€è¦å®‰è£…ä¸€äº›ä¾èµ–åº“ï¼Œæ¯”å¦‚ `Microsoft Visual C++ 14.0` æˆ–æ›´æ–°çš„ç‰ˆæœ¬ã€‚ç¡®ä¿è¿™äº›ä¾èµ–åº“å·²ç»å®‰è£…åœ¨ä½ çš„ç³»ç»Ÿä¸Šã€‚
6. **æƒé™é—®é¢˜**ï¼šå¦‚æœä½ æ²¡æœ‰è¶³å¤Ÿçš„æƒé™æ¥å®‰è£…PyTorchï¼Œå¯èƒ½ä¹Ÿä¼šå¯¼è‡´å®‰è£…å¤±è´¥ã€‚ç¡®ä¿ä½ ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œå‘½ä»¤æç¤ºç¬¦æˆ–PowerShellã€‚

å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½ä¸èƒ½è§£å†³é—®é¢˜ï¼Œä½ å¯ä»¥å°è¯•åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…PyTorchï¼š

æ‰€ä»¥ åªèƒ½ç”¨anconda åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼Œç„¶åç”¨ancondaçš„è™šæ‹Ÿç¯å¢ƒ 



## windows venåˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```shell
python3.8 -m venv env æˆ–
python -m venv intenll åˆ›å»ºäº† è™šæ‹Ÿç¯å¢ƒintenll
åœ¨ Windows ä¸Šï¼ˆå‘½ä»¤æç¤ºç¬¦æˆ– PowerShellï¼‰:
intenll\Scripts\activate
åœ¨ macOS æˆ– Linux ä¸Šï¼ˆBash shellï¼‰:
source intenll/bin/activate
```

# gofasté‚®ç®±

jia.sufast@gmail.com

 https://g01.club

å¦‚æœåœ°å€æ›´æ–°: https://g01.club è®°ä½æˆ‘ä»¬å¤‡ç”¨é‚®ç®±: jia.sufast@gmail.com æ¥ä¿¡å¤‡æ³¨ GoFast, å¦åˆ™æ²¡äººå¤„ç†



# è®¾ç½®git è´¦å·å’Œé‚®ç®±

git config --global user.name "greatheart1000"

git config --global user.email greatheart1000@163.com

è®¾ç½®HuggingFace çš„  Access Token

export HUGGING_FACE_HUB_TOKEN=your_token

export HUGGING_FACE_HUB_TOKEN="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"

åœ¨ Python ä»£ç ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥è®¾ç½®è¿™ä¸ªä»¤ç‰Œï¼š

```python
import os 

os.environ['HUGGING_FACE_HUB_TOKEN'] = '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'
```

```shell
git clone https://greatheart1000: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@huggingface.co/h94/IP-Adapter
```

```bash
export HF_ENDPOINT="https://hf-mirror.com"

```

git clone https://greatheart1000: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@huggingface.co/meta-llama/Llama-2-7b-chat-hf

huggingface-cli download --resume-download OpenGVLab/InternVL2-8B --local-dir ./InternVL2-8B

```undefined
./hdf.sh briaai/RMBG-1.4 --tool aria2c -x 4
```

# æŸ¥çœ‹è‡ªå·±linuxæ˜¯ubuntuè¿˜æ˜¯centosçš„å‘½ä»¤

1. `cat /etc/*release`: è¿™ä¸ªå‘½ä»¤å°†ä¼šæ˜¾ç¤ºä¸€ä¸ªåŒ…å«å‘è¡Œç‰ˆä¿¡æ¯çš„æ–‡ä»¶çš„å†…å®¹ã€‚å®ƒé€šå¸¸åŒ…å«äº†å‘è¡Œç‰ˆçš„åç§°ã€ç‰ˆæœ¬å·å’Œå…¶ä»–ä¸€äº›è¯¦æƒ…ã€‚

2. `lsb_release -a`: è¿™ä¸ªå‘½ä»¤å°†ä¼šå‘Šè¯‰ä½ å…³äº Linux å‘è¡Œç‰ˆçš„è¯¦ç»†ä¿¡æ¯ã€‚å¦‚æœä½ çš„ç³»ç»Ÿä¸Šå®‰è£…äº† LSBï¼ˆLinux Standard Baseï¼‰åŒ…ï¼Œä½ å¯ä»¥ä½¿ç”¨å®ƒã€‚

ssh -p 48297 root@connect.cqa1.seetacloud.com

oP7sHDvsSG9v





# å¦‚ä½•å¿«é€Ÿä¸‹è½½huggingfaceæ¨¡å‹

```bash
ä¸“ç”¨å¤šçº¿ç¨‹ä¸‹è½½å™¨ hfd
hfd æ˜¯åŸºäº Git å’Œ aria2 å®ç°çš„ä¸“ç”¨äºhuggingface ä¸‹è½½çš„å‘½ä»¤è¡Œè„šæœ¬ï¼š hfd.shï¼ˆGitsté“¾æ¥ï¼‰ã€‚hfd ç›¸æ¯” huggingface-cli ï¼Œé²æ£’æ€§æ›´å¥½ï¼Œå¾ˆå°‘ä¼šæœ‰å¥‡å¥‡æ€ªæ€ªçš„æŠ¥é”™ï¼Œæ­¤å¤–å¤šçº¿ç¨‹æ§åˆ¶åŠ›åº¦ä¹Ÿæ›´ç»†ï¼Œå¯ä»¥è®¾ç½®çº¿ç¨‹æ•°é‡ã€‚ç¼ºç‚¹æ˜¯ç›®å‰ä»…é€‚ç”¨äº Linuxã€‚

å…¶åŸç†æ˜¯ Step1ï¼šGit clone é¡¹ç›®ä»“åº“ä¸­lfsæ–‡ä»¶ä¹‹å¤–çš„æ‰€æœ‰æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨è·å– lfs æ–‡ä»¶çš„ urlï¼›Step2ï¼šåˆ©ç”¨ aria2 å¤šçº¿ç¨‹ä¸‹è½½æ–‡ä»¶ã€‚
ç¬¬ä¸€æ­¥:
export HF_ENDPOINT="https://hf-mirror.com"

ç¬¬äºŒæ­¥:
./hfd.sh bigscience/bloom-560m æˆ–è€… ./hdf.sh bigscience/bloom-560m --tool aria2c -x 4

ç¬¬ä¸‰æ­¥
$ ./hfd.sh -h
Usage:
hfd <model_id> [--include include_pattern] [--exclude exclude_pattern] [--hf_username username] [--hf_token token] [--tool wget|aria2c] [-x threads] [--dataset]

Description:
ä½¿ç”¨æä¾›çš„æ¨¡å‹IDä»Hugging Faceä¸‹è½½æ¨¡å‹æˆ–æ•°æ®é›†ã€‚

Parameters:
model_id Hugging Faceæ¨¡å‹IDï¼Œæ ¼å¼ä¸º'repo/model_name'ã€‚
--include ï¼ˆå¯é€‰ï¼‰æ ‡å¿—ï¼Œç”¨äºæŒ‡å®šè¦åŒ…æ‹¬åœ¨ä¸‹è½½ä¸­çš„æ–‡ä»¶çš„å­—ç¬¦ä¸²æ¨¡å¼ã€‚
--exclude ï¼ˆå¯é€‰ï¼‰æ ‡å¿—ï¼Œç”¨äºæŒ‡å®šè¦ä»ä¸‹è½½ä¸­æ’é™¤çš„æ–‡ä»¶çš„å­—ç¬¦ä¸²æ¨¡å¼ã€‚
exclude_pattern åŒ¹é…æ–‡ä»¶åä»¥æ’é™¤çš„æ¨¡å¼ã€‚
--hf_username ï¼ˆå¯é€‰ï¼‰Hugging Faceç”¨æˆ·åï¼Œç”¨äºèº«ä»½éªŒè¯ã€‚
--hf_token ï¼ˆå¯é€‰ï¼‰Hugging Faceä»¤ç‰Œï¼Œç”¨äºèº«ä»½éªŒè¯ã€‚
--tool ï¼ˆå¯é€‰ï¼‰ä½¿ç”¨çš„ä¸‹è½½å·¥å…·ã€‚å¯ä»¥æ˜¯wgetï¼ˆé»˜è®¤ï¼‰æˆ–aria2cã€‚
-x ï¼ˆå¯é€‰ï¼‰aria2cçš„ä¸‹è½½çº¿ç¨‹æ•°ã€‚
--dataset ï¼ˆå¯é€‰ï¼‰æ ‡å¿—ï¼Œè¡¨ç¤ºä¸‹è½½æ•°æ®é›†ã€‚
ç¤ºä¾‹ï¼š
hfd bigscience/bloom-560m --exclude safetensors
hfd meta-llama/Llama-2-7b --hf_username myuser --hf_token mytoken --tool aria2c -x 8
hfd lavita/medical-qa-shared-task-v1-toy --dataset
```

 h94/IP-Adapter 

[å¦‚ä½•å¿«é€Ÿä¸‹è½½huggingfaceæ¨¡å‹â€”â€”å…¨æ–¹æ³•æ€»ç»“ - ç®€ä¹¦ (jianshu.com)](https://www.jianshu.com/p/86c4a45f0a18)

./hfd.sh h94/IP-Adapter --tool aria2c -x 4

nohup ./hfd.sh h94/IP-Adapter --tool aria2c -x 4 > download.log 2>&1 &

nohup ./hfd.sh TMElyralab/MuseV --tool aria2c -x 4 > download.log 2>&1 &

nohup ./hfd.sh TMElyralab/MuseV --tool aria2c -x 4 > download.log 2>&1 &

```
huggingface-cli download --resume-download TechxGenus/Meta-Llama-3-8B-Instruct-AWQ --local-dir /root/models/Meta-Llama-3-8B-Instruct-AWQ
```

```
huggingface-cli download --resume-download briaai/RMBG-1.4 --local-dir /root/models/RMBG-1.4
```

```rust
huggingface-cli download --resume-download --repo-type dataset greatheart/7b
```

# GPUè®¡ç®—å‹å®ä¾‹ä¸­å®‰è£…Teslaé©±åŠ¨ï¼ˆLinuxï¼‰

nvidia-smi NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.

https://help.aliyun.com/zh/egs/user-guide/install-a-gpu-driver-on-a-gpu-accelerated-compute-optimized-linux-instance?spm=a2c4g.11186623.0.0.7dba2723u1lv7j

å®‰è£… Tesla V100 

```shell
wget https://cn.download.nvidia.com/tesla/470.161.03/NVIDIA-Linux-x86_64-470.161.03.run
```

### **æ­¥éª¤äºŒï¼šå®‰è£…NVIDIA Teslaé©±åŠ¨**

ä¸åŒæ“ä½œç³»ç»Ÿå®ä¾‹ï¼Œå®‰è£…Teslaé©±åŠ¨çš„æ–¹æ³•æœ‰æ‰€ä¸åŒï¼Œå…·ä½“æ“ä½œå¦‚ä¸‹æ‰€ç¤ºã€‚

CentOSæ“ä½œç³»ç»Ÿ

Ubuntuç­‰å…¶ä»–æ“ä½œç³»ç»Ÿ

1. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼ŒæŸ¥è¯¢GPUå®ä¾‹ä¸­æ˜¯å¦å®‰è£…kernel-develå’Œkernel-headersåŒ…ã€‚

    

   ```shell
   rpm  -qa | grep $(uname -r)
   ```

   - å¦‚æœå›æ˜¾ç±»ä¼¼å¦‚ä¸‹ä¿¡æ¯ï¼Œå³åŒ…å«äº†kernel-develå’Œkernel-headersåŒ…çš„ç‰ˆæœ¬ä¿¡æ¯ï¼Œè¡¨ç¤ºå·²å®‰è£…ã€‚

      

     ```shell
     kernel-3.10.0-1062.18.1.el7.x86_64
     kernel-devel-3.10.0-1062.18.1.el7.x86_64
     kernel-headers-3.10.0-1062.18.1.el7.x86_64
     ```

   - å¦‚æœåœ¨å›æ˜¾ä¿¡æ¯ä¸­ï¼Œæ‚¨æ²¡æœ‰æ‰¾åˆ°*kernel-devel-**å’Œ*kernel-headers-**å†…å®¹ï¼Œæ‚¨éœ€è¦è‡ªè¡Œä¸‹è½½å¹¶å®‰è£…kernelå¯¹åº”ç‰ˆæœ¬çš„[kernel-devel](https://pkgs.org/download/kernel-devel)å’Œ[kernel-headers](https://www.rpmfind.net/linux/rpm2html/search.php?query=kernel-headers)åŒ…ã€‚

     **é‡è¦**

     kernel-develå’Œkernelç‰ˆæœ¬ä¸ä¸€è‡´ä¼šå¯¼è‡´åœ¨å®‰è£…driver rpmè¿‡ç¨‹ä¸­driverç¼–è¯‘å‡ºé”™ã€‚å› æ­¤ï¼Œè¯·æ‚¨ç¡®è®¤å›æ˜¾ä¿¡æ¯ä¸­*kernel-**çš„ç‰ˆæœ¬å·åï¼Œå†ä¸‹è½½å¯¹åº”ç‰ˆæœ¬çš„kernel-develã€‚åœ¨ç¤ºä¾‹å›æ˜¾ä¿¡æ¯ä¸­ï¼Œkernelçš„ç‰ˆæœ¬å·ä¸º3.10.0-1062.18.1.el7.x86_64ã€‚

2. æˆæƒå¹¶å®‰è£…Teslaé©±åŠ¨ã€‚

   ä»¥æ“ä½œç³»ç»Ÿæ˜¯Linux 64-bitçš„é©±åŠ¨ä¸ºä¾‹ï¼Œæ¨èæ‚¨ä½¿ç”¨.runå½¢å¼çš„Teslaé©±åŠ¨ï¼Œä¾‹å¦‚ï¼šNVIDIA-Linux-x86_64-xxxx.runã€‚åˆ†åˆ«æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œæˆæƒå¹¶å®‰è£…Teslaé©±åŠ¨ã€‚

   **è¯´æ˜**

   å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯.debæˆ–.rpmç­‰å…¶ä»–å½¢å¼çš„Teslaé©±åŠ¨ï¼Œå…·ä½“å®‰è£…æ–¹æ³•ï¼Œè¯·å‚è§[NVIDIA CUDA Installation Guide for Linux](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/)ã€‚

    

   ```shell
   chmod +x NVIDIA-Linux-x86_64-xxxx.run
   ```

    

   ```shell
   sh NVIDIA-Linux-x86_64-xxxx.run
   ```

3. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼ŒæŸ¥çœ‹Teslaé©±åŠ¨æ˜¯å¦å®‰è£…æˆåŠŸã€‚

    

   ```shell
   nvidia-smi
   ```

## å¸è½½æ—§çš„nvidia driver

#### å¸è½½Teslaé©±åŠ¨

å¦‚æœæ‚¨åœ¨Ubuntuã€CentOSã€Alibaba Cloud Linuxã€SUSEæ“ä½œç³»ç»Ÿä¸­ä½¿ç”¨ä¸åŒå®‰è£…åŒ…ï¼ˆä¾‹å¦‚runå®‰è£…åŒ…ã€debå®‰è£…åŒ…ã€rpmå®‰è£…åŒ…ï¼‰å®‰è£…äº†Teslaé©±åŠ¨ï¼Œå› æŸç§åŸå› éœ€è¦æ‰‹åŠ¨å¸è½½æ—¶ï¼Œè¯·æŒ‰ç…§Teslaé©±åŠ¨çš„ä¸åŒå®‰è£…åœºæ™¯é€‰æ‹©å¯¹åº”çš„å¸è½½æ–¹æ³•ã€‚

https://help.aliyun.com/zh/egs/user-guide/uninstall-a-gpu-driver?spm=a2c4g.11186623.0.0.63716577Iwh3GV

**åœºæ™¯1ï¼šä½¿ç”¨runå®‰è£…åŒ…å®‰è£…äº†Teslaé©±åŠ¨**

å¦‚æœåœ¨åˆ›å»ºGPUå®ä¾‹æ—¶åŒæ—¶è‡ªåŠ¨å®‰è£…äº†Teslaé©±åŠ¨ï¼Œåˆ™è¯¥é©±åŠ¨çš„å¸è½½éœ€é€‰æ‹©é€šè¿‡runå®‰è£…åŒ…çš„å¸è½½æ–¹å¼ã€‚ä»¥Teslaé©±åŠ¨470.161.03ã€CUDA 11.4.1ä¸ºä¾‹ï¼Œå…·ä½“æ“ä½œå¦‚ä¸‹æ‰€ç¤ºã€‚

1. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¸è½½Teslaé©±åŠ¨ã€‚

    

   ```shell
   /usr/bin/nvidia-uninstall
   ```

2. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¸è½½CUDAã€‚

    

   ```shell
   /usr/local/cuda/bin/cuda-uninstaller
   rm -rf /usr/local/cuda-11.4
   ```

   ![runæ–¹å¼](https://help-static-aliyun-doc.aliyuncs.com/assets/img/zh-CN/4751683861/p646009.png)

   **è¯´æ˜**

   ä¸åŒCUDAç‰ˆæœ¬ï¼Œå¸è½½å‘½ä»¤å¯èƒ½å­˜åœ¨å·®åˆ«ï¼Œå¦‚æœæœªæ‰¾åˆ°cuda-uninstalleræ–‡ä»¶ï¼Œè¯·åˆ°/usr/local/cuda/bin/ç›®å½•ä¸‹æŸ¥çœ‹æ˜¯å¦å­˜åœ¨uninstall_cudaå¼€å¤´çš„æ–‡ä»¶ã€‚å¦‚æœæœ‰ï¼Œåˆ™å°†å‘½ä»¤ä¸­çš„cuda-uninstalleræ›¿æ¢ä¸ºè¯¥æ–‡ä»¶åã€‚

3. æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œé‡å¯å®ä¾‹ã€‚

    

   ```shell
   reboot
   ```

## å®‰è£…cuda 

```
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.runsudo sh cuda_12.1.0_530.30.02_linux.run
```

ç½‘å€ ï¼š[CUDA Toolkit 12.1 Downloads | NVIDIA Developer](https://developer.nvidia.com/cuda-12-1-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=CentOS&target_version=7&target_type=runfile_local)



## å¾®è°ƒå¤š

## è½®å¯¹è¯è„šæœ¬

Experimental environment: V100, A10, 3090

13GB GPU memory

```shell
PYTHONPATH=../../.. \
CUDA_VISIBLE_DEVICES=0 \
python llm_sft.py \
    --model_id_or_path qwen/Qwen-7B-Chat-Int4 \
    --model_revision master \
    --sft_type lora \
    --tuner_backend peft \
    --template_type qwen \
    --dtype fp16 \
    --output_dir output \
    --dataset "/root/swift/examples/pytorch/llm/train.jsonl" \
    --train_dataset_sample -1 \
    --num_train_epochs 1 \
    --max_length 2048 \
    --check_dataset_strategy warning \
    --lora_rank 8 \
    --lora_alpha 32 \
    --lora_dropout_p 0.05 \
    --lora_target_modules ALL \
    --gradient_checkpointing true \
    --batch_size 1 \
    --weight_decay 0.1 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 16 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 10 \
    --use_flash_attn false \
```

V100å¯ä»¥è¿™ä¹ˆè¿è¡Œ ä½†æ˜¯ä¸èƒ½è¿è¡Œä¸Šé¢çš„

```
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch

from swift.llm import (
    DatasetName, InferArguments, ModelType, SftArguments,
    infer_main, sft_main, app_ui_main
)

model_type = ModelType.qwen_7b_chat
sft_args = SftArguments(
    model_type=model_type,
    dataset=[f'{DatasetName.blossom_math_zh}#2000'],
    output_dir='output')
result = sft_main(sft_args)
best_model_checkpoint = result['best_model_checkpoint']
print(f'best_model_checkpoint: {best_model_checkpoint}')
torch.cuda.empty_cache()
```





# Pythonè¯»å–xlslæ–‡ä»¶

## è¯»å–xlslæ–‡ä»¶

```python
import pandas as pd
# æ›¿æ¢ä¸‹é¢çš„è·¯å¾„ä¸ºä½ çš„.xlsæ–‡ä»¶çš„å®é™…è·¯å¾„
file_path = r'C:\Users\13527\Desktop\æ— æ ‡é¢˜.xlsx'
# è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼Œå¦‚æœéœ€è¦è¯»å–å…¶ä»–å·¥ä½œè¡¨ï¼Œå¯ä»¥é€šè¿‡sheet_nameå‚æ•°æŒ‡å®š
df = pd.read_excel(file_path, engine='openpyxl')
# æ˜¾ç¤ºæ•°æ®å¸§å†…å®¹
print(df.columns)
df = df.rename(columns={'é—®é¢˜': 'problem', 'ç­”æ¡ˆ': 'answer', 'ç”¨æˆ·ç¼–å·,ç›¸åŒä»£è¡¨ä¸€ä¸ªç”¨æˆ·': 'id'})
```

## è¯»å–`.xls`æ–‡ä»¶

```python
import pandas as pd

# æ›¿æ¢ä¸‹é¢çš„è·¯å¾„ä¸ºä½ çš„.xlsæ–‡ä»¶çš„å®é™…è·¯å¾„
file_path = 'path_to_your_xls_file.xls'

# è¯»å–ç¬¬ä¸€ä¸ªå·¥ä½œè¡¨ï¼Œå¦‚æœéœ€è¦è¯»å–å…¶ä»–å·¥ä½œè¡¨ï¼Œå¯ä»¥é€šè¿‡sheet_nameå‚æ•°æŒ‡å®š
df = pd.read_excel(file_path)

# æ˜¾ç¤ºæ•°æ®å¸§å†…å®¹
print(df)
```

# Pythonå°†åˆ—è¡¨ä¿å­˜ä¸ºjsonlæ–‡ä»¶

```python
import json

# æ‚¨çš„æ•°æ®åˆ—è¡¨
data_list = ['æœ‰è´§å—ï¼Ÿ', 'äº²ï¼Œèƒ½æ‹ä¸‹ä»˜æ¬¾çš„å°±æ˜¯æœ‰è´§çš„å“¦~é¢„å®šæ¬¾åœ¨å®è´é¡µé¢ä¼šæœ‰æ³¨æ˜å“¦~', 'ç°åœ¨ä¹°ä»€ä¹ˆæ—¶å€™æ‰èƒ½å‘è´§', 'äº²äº²ï¼Œæ˜¥èŠ‚æœŸé—´åº—é“ºæš‚æ—¶ä¸å‘è´§å“¦ï¼ŒèŠ‚åå›æ¥ä¼šæŒ‰ç…§ä¸‹å•é¡ºåºè¿›è¡Œå‘è´§å“¦ï½ç¥æ‚¨æ–°å¹´å¿«ä¹ï½']

# æ–‡ä»¶åç§°
filename = 'data.jsonl'

# æ‰“å¼€æ–‡ä»¶ä»¥å†™å…¥
with open(filename, 'w', encoding='utf-8') as file:
    # éå†åˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ 
    for item in data_list:
        # å°†å…ƒç´ è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œå¹¶ç¡®ä¿ä½¿ç”¨UTF-8ç¼–ç 
        json_string = json.dumps(item, ensure_ascii=False)
        # å†™å…¥æ–‡ä»¶å¹¶æ·»åŠ æ¢è¡Œç¬¦
        file.write(json_string + '\n')
```

![image-20240605192110916](C:\Users\13527\AppData\Roaming\Typora\typora-user-images\image-20240605192110916.png)

python è¯»å–json

```python
import json

# æ‰“å¼€JSONæ–‡ä»¶  
data = []
with open('data/llama3.json', 'r') as file:
    # ä½¿ç”¨json.load()æ–¹æ³•è¯»å–æ–‡ä»¶å†…å®¹  
    data = json.load(file)
for i in data:
    instruction = i['instruction']
    output =i['output']
```



# python  requests å‘é€è¯·æ±‚

```
import requests  
# å®šä¹‰è¯·æ±‚çš„æ•°æ®ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ª JSON æ ¼å¼çš„å­—å…¸  
data = {  
    "query": "åŒ»é™¢ç”µè¯",  
}   
# è®¾ç½®è¯·æ±‚å¤´ï¼ŒæŒ‡å®šå†…å®¹ç±»å‹ä¸º JSON  
headers = {  
    "Content-Type": "application/json"  
}  
# å‘é€ POST è¯·æ±‚  
response = requests.post(  
    'http://192.168.1.40:3000/predict',  
    json=data,  # requests åº“ä¼šè‡ªåŠ¨å°†å­—å…¸ç¼–ç ä¸º JSON æ ¼å¼  
    headers=headers  
)  
# æ‰“å°å“åº”å†…å®¹  
print(response.text)  
# æ£€æŸ¥å“åº”çŠ¶æ€ç   
print(response.status_code)
```



{
    "query": "é¢„çº¦æŒ‚å·çš„ç”µè¯å¤šå°‘?åŒ»é™¢åœ°å€",
    "CustomerTypeCode": "008",

â€‹	"type":"",

â€‹    "userId": "2",

â€‹	"message":[],

}



# swift å¤šè½®å¯¹è¯å¾®è°ƒè„šæœ¬ æ­£å¸¸è¿è¡Œæœ¬åœ°æ•°æ®é›†

```shell
PYTHONPATH=../../.. \
CUDA_VISIBLE_DEVICES=0 \
python llm_sft.py \
    --model_type qwen1half-7b-chat-int4 \
    --sft_type lora \
    --tuner_backend peft \
    --dtype AUTO \
    --output_dir output \
    --dataset "/mnt/workspace/swift/examples/pytorch/llm/total.jsonl" \
    --train_dataset_sample -1 \
    --num_train_epochs 1 \
    --max_length 1024 \
    --check_dataset_strategy warning \
    --lora_rank 8 \
    --lora_alpha 32 \
    --lora_dropout_p 0.05 \
    --lora_target_modules ALL \
    --gradient_checkpointing true \
    --batch_size 1 \
    --weight_decay 0.1 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 16 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 10 \
    --use_flash_attn false \
semantic-16G è„šæœ¬    qwen_muti_turn.sh

PYTHONPATH=../../.. \
CUDA_VISIBLE_DEVICES=0 \
python llm_sft.py \
    --model_type qwen-7b-chat-int4 \
    --sft_type lora \
    --tuner_backend peft \
    --dtype AUTO \
    --output_dir output \
    --dataset "/root/swift/examples/pytorch/llm/muti_turn.jsonl" \
    --train_dataset_sample -1 \
    --num_train_epochs 1 \
    --max_length 1024 \
    --check_dataset_strategy warning \
    --lora_rank 8 \
    --lora_alpha 32 \
    --lora_dropout_p 0.05 \
    --lora_target_modules ALL \
    --gradient_checkpointing true \
    --batch_size 1 \
    --weight_decay 0.1 \
    --learning_rate 1e-4 \
    --gradient_accumulation_steps 16 \
    --max_grad_norm 0.5 \
    --warmup_ratio 0.03 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 10 \
    --use_flash_attn false \
```



## vllmè¿›è¡Œæ¨ç†

vllm_qwen_7b_chat.py å¯ä»¥è¿è¡Œ

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    ModelType, get_vllm_engine, get_default_template_type,
    get_template, inference_vllm
)

model_type = ModelType.qwen_7b_chat
llm_engine = get_vllm_engine(model_type)
template_type = get_default_template_type(model_type)
template = get_template(template_type, llm_engine.hf_tokenizer)
# ä¸`transformers.GenerationConfig`ç±»ä¼¼çš„æ¥å£
llm_engine.generation_config.max_new_tokens = 256

request_list = [{'query': 'ä½ å¥½!'}, {'query': 'æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ'}]
resp_list = inference_vllm(llm_engine, template, request_list)
for request, resp in zip(request_list, resp_list):
    print(f"query: {request['query']}")
    print(f"response: {resp['response']}")
```





```
query_list: ['ä½ å¥½!', 'æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ']
...
response_list: ['ä½ å¥½ï¼å¾ˆé«˜å…´ä¸ºä½ æœåŠ¡ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ', 'æµ™æ±Ÿçœä¼šæ˜¯æ­å·å¸‚ã€‚']
```

[

{"query": ""ä½ å¥½!"  ,   "response": "æ‚¨å¥½ï¼Œæˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”ï¼Ÿ""},

{"query": ""æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ"   ,     "response": "æµ™æ±Ÿçš„çœä¼šæ˜¯æ­å·ã€‚"},

{"query": ""è¿™æœ‰ä»€ä¹ˆå¥½åƒçš„"   ,     "response": "æµ™æ±Ÿæœ‰å¾ˆå¤šç¾é£Ÿ,å…¶ä¸­ä¸€äº›éå¸¸æœ‰åçš„åŒ…æ‹¬æ­å·çš„é¾™äº•è™¾ä»ã€ä¸œå¡è‚‰ã€è¥¿æ¹–é†‹é±¼ã€å«åŒ–ç«¥å­é¸¡ç­‰ã€‚å¦å¤–,æµ™æ±Ÿè¿˜æœ‰å¾ˆå¤šç‰¹è‰²å°åƒå’Œç³•ç‚¹,æ¯”å¦‚å®æ³¢çš„æ±¤å›¢ã€å¹´ç³•,æ¸©å·çš„ç‚’èƒèŸ¹ã€æ¸©å·è‚‰åœ†ç­‰ã€‚"}

]

```
query: ä½ å¥½!
response: æ‚¨å¥½ï¼Œæˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å¸®æ‚¨è§£ç­”ï¼Ÿ
query: æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ
response: æµ™æ±Ÿçš„çœä¼šæ˜¯æ­å·ã€‚
query: è¿™æœ‰ä»€ä¹ˆå¥½åƒçš„
response: æµ™æ±Ÿæœ‰å¾ˆå¤šç¾é£Ÿ,å…¶ä¸­ä¸€äº›éå¸¸æœ‰åçš„åŒ…æ‹¬æ­å·çš„é¾™äº•è™¾ä»ã€ä¸œå¡è‚‰ã€è¥¿æ¹–é†‹é±¼ã€å«åŒ–ç«¥å­é¸¡ç­‰ã€‚å¦å¤–,æµ™æ±Ÿè¿˜æœ‰å¾ˆå¤šç‰¹è‰²å°åƒå’Œç³•ç‚¹,æ¯”å¦‚å®æ³¢çš„æ±¤å›¢ã€å¹´ç³•,æ¸©å·çš„ç‚’èƒèŸ¹ã€æ¸©å·è‚‰åœ†ç­‰ã€‚
```

## VLLMç›´æ¥æ”¯æŒä¼ å…¥é‡åŒ–åçš„æ¨¡å‹è¿›è¡Œæ¨ç†

```python
from vllm import LLM, SamplingParams
import os
import torch
os.environ['VLLM_USE_MODELSCOPE'] = 'True'

#Sample prompts.
prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
#Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

#Create an LLM.
llm = LLM(model="/root/.cache/modelscope/hub/qwen/Qwen-7B-Chat-Int4", quantization="gptq", dtype=torch.float16, trust_remote_code=True)
#Generate texts from the prompts. The output is a list of RequestOutput objects
#that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
#Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
 

```





```python
å‘é€è¯·æ±‚  curl -X POST -H "Content-Type: application/json" -d '{"chatid": "XXXX" ,'timestamp': '2024-06-07 14:15:47'}  http://xxxxxx:port/record

è¿”å›ç»“æœ
{"chatid":"xxxxxxxxxx","userID":"yyyyyyyyy",
 record":
 [
 {"query": "å¿«é€’è¿˜æ²¡åˆ°ï¼Ÿ","answer": "éå¸¸æŠ±æ­‰è€½è¯¯æ‚¨ä½¿ç”¨äº†å‘¢äº²äº²ç»™æ‚¨å‚¬ä¿ƒäº†å‘¢ï¼Œè¿˜è¯·æ‚¨è€å¿ƒç­‰å¾…ä¸‹å“¦~",'timestamp': '2024-06-07 14:15:47'},   {"query": "å¿«é€’è¿˜æ²¡åˆ°ï¼Ÿ","answer": "éå¸¸æŠ±æ­‰è€½è¯¯æ‚¨ä½¿ç”¨äº†å‘¢äº²äº²ç»™æ‚¨å‚¬ä¿ƒäº†å‘¢ï¼Œè¿˜è¯·æ‚¨è€å¿ƒç­‰å¾…ä¸‹å“¦~",'timestamp': '2024-06-07 14:15:47'},
{"query": "å¿«é€’è¿˜æ²¡åˆ°ï¼Ÿ","answer": "éå¸¸æŠ±æ­‰è€½è¯¯æ‚¨ä½¿ç”¨äº†å‘¢äº²äº²ç»™æ‚¨å‚¬ä¿ƒäº†å‘¢ï¼Œè¿˜è¯·æ‚¨è€å¿ƒç­‰å¾…ä¸‹å“¦~",'timestamp': '2024-06-07 14:15:47'},
{"query": "å¿«é€’è¿˜æ²¡åˆ°ï¼Ÿ","answer": "éå¸¸æŠ±æ­‰è€½è¯¯æ‚¨ä½¿ç”¨äº†å‘¢äº²äº²ç»™æ‚¨å‚¬ä¿ƒäº†å‘¢ï¼Œè¿˜è¯·æ‚¨è€å¿ƒç­‰å¾…ä¸‹å“¦~",'timestamp': '2024-06-07 14:15:47'},
{"query": "å¿«é€’è¿˜æ²¡åˆ°ï¼Ÿ","answer": "éå¸¸æŠ±æ­‰è€½è¯¯æ‚¨ä½¿ç”¨äº†å‘¢äº²äº²ç»™æ‚¨å‚¬ä¿ƒäº†å‘¢ï¼Œè¿˜è¯·æ‚¨è€å¿ƒç­‰å¾…ä¸‹å“¦~",'timestamp': '2024-06-07 14:15:47'}
]
}



```

 

#### å‘é€è¯·æ±‚  å•æ„å›¾æ ·ä¾‹

curl -X POST -H "Content-Type: application/json" -d '{"query": "å¯ä»¥å‘é¡ºä¸°å—"}' http://127.0.0.1:5000/get_semantic

å•æ„å›¾è¿”å›ç»“æœ  {"query":"å¯ä»¥å‘é¡ºä¸°å—","semantic":[{"query":"å¯ä»¥å‘é¡ºä¸°å—","semantic":"å‘ä»€ä¹ˆå¿«é€’"}],"type":"single","time":timestamp}

#### å‘é€è¯·æ±‚  å¤šæ„å›¾

curl -X POST -H "Content-Type: application/json" -d '{"query": "å¯ä»¥å‘é¡ºä¸°å—ä»€ä¹ˆæ—¶å€™å‘è´§"}' http://127.0.0.1:5000/get_semantic

å•æ„å›¾è¿”å›ç»“æœ  {"query":"å¯ä»¥å‘é¡ºä¸°å—ä»€ä¹ˆæ—¶å€™å‘è´§","semantic":[{"query":"å¯ä»¥å‘é¡ºä¸°å—","semantic":"å‘ä»€ä¹ˆå¿«é€’"},{"query":"ä»€ä¹ˆæ—¶å€™å‘è´§","semantic":"å‘è´§æ—¶é—´"}],"type":"mutli","time":timestamp}

é•¿å…´è¡—é“å²‘æ‘åŒ—è¡— 16-1è½»èˆå…¬å¯“ 



é¦–å…ˆ conda activate vllm æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ

ç¬¬äºŒæ­¥ è¿›å…¥ cd /root/swift/pytorch/llmç›®å½• 

python app.py   python INfer.py



curl -X POST "http://192.168.1.40:8000/human_reply" -H "Content-Type: application/json" -d '{    "query": "å‘é¡ºä¸°å¿«é€’å—","answer":  "å‘ç”³é€šï¼Œä¸æ”¯æŒæŒ‡å®š"   ,   "CustomerTypeCode": "00",  "type":"qianwen",  "CustomerTypeCode":001,"CustomerTypeName":"ç”µå•†è¡Œä¸š",userId": "8888",    "message":

[{"query":"1111","answer":"22222","timestamp":"---------------"},{"query":"1111","answer":"22222","timestamp":"---------------"},{"query":"1111","answer":"22222","timestamp":"---------------"},{"query":"1111","answer":"22222","timestamp":"---------------"} ]

} '



# äººå·¥æ™ºèƒ½è§†é¢‘

https://space.bilibili.com/65742859?spm_id_from=333.788.0.0

## ã€å¤§æ¨¡å‹é‡åŒ–ã€‘ä½¿ç”¨llama.cppè¿›è¡Œé‡åŒ–å’Œéƒ¨ç½²



# ollama vs vllm å¹¶å‘æ¯”è¾ƒ

https://blog.csdn.net/arkohut/article/details/139076652   æ˜¯æ•™ç¨‹

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

```
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

```

[ollamaéƒ¨ç½²å¸¸è§é—®é¢˜è§£ç­” - AIÂ·ä½ æ‰€çˆ± (linxkon.github.io)](https://linxkon.github.io/ollamaéƒ¨ç½²æŒ‡å—.html)

ollamaçš„å„ç§å‘½ä»¤

## 2.dockeréƒ¨ç½²ollama web ui

docker run -d -p 8090:8090 --add-host=host.docker.internal:host-gateway --name ollama-webui --restart always ghcr.io/ollama-webui/ollama-webui:main

## 3.ä½¿ç”¨dockerä¸­çš„ollamaä¸‹è½½å¹¶è¿è¡ŒAIæ¨¡å‹ï¼ˆç¤ºä¾‹ä¸ºé˜¿é‡Œé€šä¹‰åƒé—®4b-chatï¼‰

```shell
docker exec -it ollama ollama run qwen:4b-chat
```

#### dockerä½¿ç”¨GPUè¿è¡Œollama

```shell
docker run --gpus all -d -v /opt/ai/ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Ollamaå¸¸ç”¨å‘½ä»¤**

ä»¥ä¸‹æ˜¯Ollamaä¸€äº›å¸¸ç”¨å‘½ä»¤ï¼š

- è¿è¡Œä¸€ä¸ªæŒ‡å®šå¤§æ¨¡å‹ï¼š`ollama run llama3:8b-text`
- æŸ¥çœ‹æœ¬åœ°å¤§æ¨¡å‹åˆ—è¡¨ï¼š`ollama list`
- æŸ¥çœ‹è¿è¡Œä¸­çš„å¤§æ¨¡å‹ï¼š`ollama ps`
- åˆ é™¤æœ¬åœ°æŒ‡å®šå¤§æ¨¡å‹ï¼š`ollama rm llama3:8b-text`

curl http://localhost:11434/api/chat -d '{
  "model": "qwen2:latest",
  "messages": [
    { "role": "user", "content": "ä½ å¥½ï¼Œä½ æ˜¯è°" }
  ],
  "stream": false
}'



curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2:1b",
  "messages": [
    { "role": "user", "content": "ä½ çŸ¥é“æ­¦æ±‰å—" }
  ],
  "stream": false
}'

```
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "é­”å…½äº‰éœ¸"
}'
```

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("nomic-ai/nomic-embed-text-v1", trust_remote_code=True)
sentences = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']
embeddings = model.encode(sentences)
print(embeddings)
```





# vLLM éƒ¨ç½²å¤§æ¨¡å‹

pip3 install vllm

## 2.1 çº¿ä¸‹æ‰¹é‡æ¨ç†

çº¿ä¸‹æ‰¹é‡æ¨ç†ï¼šä¸ºè¾“å…¥çš„promptsåˆ—è¡¨ï¼Œä½¿ç”¨vLLMç”Ÿæˆç­”æ¡ˆã€‚

```
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "6,7"

from vllm import LLM, SamplingParams

 llm = LLM('/root/models/LLaMA3-8B-Chinese-Chat-merged',dtype=torch.float16)

INFO 01-18 08:13:26 llm_engine.py:70] Initializing an LLM engine with config: model='/data-ai/model/llama2/llama2_hf/Llama-2-13b-chat-hf', tokenizer='/data-ai/model/llama2/llama2_hf/Llama-2-13b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, enforce_eager=False, seed=0)
INFO 01-18 08:13:37 llm_engine.py:275] # GPU blocks: 3418, # CPU blocks: 327
INFO 01-18 08:13:39 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 01-18 08:13:39 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode.
INFO 01-18 08:13:44 model_runner.py:547] Graph capturing finished in 5 secs.

prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 11.76it/s]

Prompt: 'Hello, my name is', Generated text: " Sherry and I'm a stay at home mom of three beautiful children."
Prompt: 'The president of the United States is', Generated text: ' one of the most powerful people in the world, and yet, many people do'
Prompt: 'The capital of France is', Generated text: ' Paris. This is a fact that is well known to most people, but there'
Prompt: 'The future of AI is', Generated text: ' likely to be shaped by a combination of technological advancements and soci'

CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.api_server --model /root/models/LLaMA3-8B-Chinese-Chat-merged
CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.api_server --model /root/models/LLaMA3-8B-Chinese-Chat-merged --dtype half

OpenAIé£æ ¼çš„APIæœåŠ¡
CUDA_VISIBLE_DEVICES=0 python -m vllm.entrypoints.openai.api_server --model /root/models/LLaMA3-8B-Chinese-Chat-merged --served-model-name LLaMA3 --dtype half

```

```
huggingface-cli download --resume-download TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ --local-dir /root/models/Meta-Llama-3-8B-Instruct-GPTQ
```

## æŸ¥çœ‹vllmå¯æŸ¥çœ‹æ”¯æŒçš„è„šæœ¬å‚æ•°

ä½¿ç”¨å‘½ä»¤ python -m vllm.entrypoints.api_server --help å¯æŸ¥çœ‹æ”¯æŒçš„è„šæœ¬å‚æ•°

python -m vllm.entrypoints.api_server --model /root/models/Meta-Llama-3-8B-Instruct-AWQ/ --qlora-adapter-name-or-path /root/LLaMA-Factory/saves/llama3-8b/lora/sft æŠ¥é”™

ValueError: BitsAndBytes quantization and QLoRA adapter only support 'bitsandbytes' load format, but got auto



prompts = [
    "Hello, my name is",
    "The president of the United States is",
    "The capital of France is",
    "The future of AI is",
]

sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

outputs = llm.generate(prompts, sampling_params)

# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")



prompts = [
    "ä½ å¥½ï¼Œæˆ‘æœ‰ä¸€æ®µæ–‡æœ¬éœ€è¦å¤„ç†,è¯·ä½ å°†æ–‡æœ¬ä»¥å¥å­ä¸ºå•ä½è¿›è¡Œåˆ‡åˆ†,éœ€è¦ç‰¹åˆ«æ³¨æ„ä¸­æ–‡çš„å¥å­è¾¹ç•Œå¯èƒ½ä¸ä»…ä»…ä¾èµ–äºæ ‡ç‚¹ç¬¦å·ï¼Œè¿˜éœ€è¦è€ƒè™‘è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹,å„è‡ªåˆ†å‰²ä¼šçªå‡ºå„ä¸ªå¥å­çš„ç‹¬ç«‹æ„ä¹‰ã€‚ä½ èƒ½å¸®æˆ‘åˆ†å‰²å®ƒå—ï¼Ÿåœ¨æ¯ä¸ªå¥å­ç»“æŸåï¼Œä½¿ç”¨æ¢è¡Œç¬¦æ¥åŒºåˆ†ã€‚è°¢è°¢ï¼ä»¥ä¸‹æ˜¯æˆ‘ç»™å‡ºçš„ç¤ºä¾‹:\nè¾“å…¥:ä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿæœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nå¥å­åˆ†å‰²:\nä½ ä»¬æä¾›å‘ç¥¨å—ï¼Ÿ\næœ€æ—©ä»€ä¹ˆæ—¶å€™å‘è´§\nè¾“å…¥:ä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢ä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nå¥å­åˆ†å‰²:\nä¸ºä»€ä¹ˆä¼šæ¯”å…¶ä»–åšä¸»ä¾¿å®œå‘¢\nä½ å¥½è¿™ä¸ªç”µå­å‘ç¥¨æ€ä¹ˆä¸‹è½½\nè¾“å…¥:æˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nå¥å­åˆ†å‰²:\næˆ‘å°±æƒ³ä»Šå¤©å‘ï¼Œå¯ä»¥å—\nè¾“å…¥:å‘å•¥å¿«é€’\nå¥å­åˆ†å‰²:\nå‘å•¥å¿«é€’\nè¯·ä½ å‚è€ƒæˆ‘ç»™å‡ºçš„ç¤ºä¾‹å¹¶ç»“åˆæ ‡ç‚¹ç¬¦å·,è¯­å¢ƒå’Œè¯­ä¹‰å†…å®¹è¿›è¡Œç»™å®šæ–‡æœ¬çš„å¥å­åˆ†å‰²ã€‚ç°ç»™å‡ºçš„æ–‡æœ¬æ˜¯[æˆ‘æ€æ ·æ‰èƒ½æˆä¸ºVIPå®¢æˆ·ï¼Œå¹¶äº«å—ç‰¹æƒæœåŠ¡å‘¢ï¼Ÿ]",
]



```
qwen-7b-chat-Int4
CUDA_VISIBLE_DEVICES=0 swift infer \
--model_type qwen-7b-chat-Int4 \
--model_id_or_path qwen/Qwen-7B-Chat-Int4 \
--ckpt_dir output/qwen-7b-chat-int4/v12-20240617-180429/checkpoint-430 \
--infer_backend vllm \
--vllm_enable_lora true \
--vllm_max_lora_rank 16

qwen-7b-chat
CUDA_VISIBLE_DEVICES=0 swift infer \
--model_type qwen-7b-chat \
--model_id_or_path qwen/qwen/Qwen-7B-Chat \
--ckpt_dir output/qwen-7b-chat/v12-20240607-114403/checkpoint-600 \ 
--infer_backend vllm \
--vllm_enable_lora true \
--vllm_max_lora_rank 16


CUDA_VISIBLE_DEVICES=0 swift infer \
    --model_type qwen-7b-chat-int4 \
    --quant_method bnb \
    --quantization_bit 4
    
   /root/swift/examples/pytorch/llm/output/qwen-7b-chat-int4/v12-20240617-180429/checkpoint-430
```

# vllm åŠ è½½LoRA adapters

[Using LoRA adapters â€” vLLM](https://docs.vllm.ai/en/stable/models/lora.html)

https://docs.vllm.ai/en/stable/models/lora.html

```
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

llm = LLM(model="/home/data/llm/qwen/qwen/Qwen-7B-Chat", enable_lora=True, trust_remote_code=True)

text_format_lora_path = '/home/data/llm/LLaMA-Factory-qwen1.5/qwen1_lora'
sampling_params = SamplingParams(
temperature=0,
max_tokens=256
)

prompts = [
"test",
]

outputs = llm.generate(
prompts,
sampling_params,
lora_request=LoRARequest("text_format_adapter", 1, text_format_lora_path)
)

print(outputs)

OMP_NUM_THREADS=14 CUDA_VISIBLE_DEVICES=0 swift export \
    --model_type qwen-7b-chat --quant_bits 4 \
    --model_id_or_path /root/swift/examples/pytorch/llm/output/qwen-7b-chat/v12-20240607-114403/checkpoint-681-merged \
    --dataset alpaca-zh --quant_method gptq --merge_lora true --ckpt_dirckpt_dir "output/qwen-7b-chat/v12-20240607-114403/checkpoint-681"

å•æ ·æœ¬æ¨ç†:

ä½¿ç”¨LoRAè¿›è¡Œå¾®è°ƒçš„æ¨¡å‹ä½ éœ€è¦å…ˆmerge-lora, äº§ç”Ÿå®Œæ•´çš„checkpointç›®å½•.
ä½¿ç”¨å…¨å‚æ•°å¾®è°ƒçš„æ¨¡å‹å¯ä»¥æ— ç¼ä½¿ç”¨VLLMè¿›è¡Œæ¨ç†åŠ é€Ÿ.
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    ModelType, get_vllm_engine, get_default_template_type,
    get_template, inference_vllm
)

ckpt_dir = 'vx-xxx/checkpoint-100-merged'
model_type = ModelType.qwen_7b_chat
template_type = get_default_template_type(model_type)

llm_engine = get_vllm_engine(model_type, model_id_or_path=ckpt_dir)
tokenizer = llm_engine.hf_tokenizer
template = get_template(template_type, tokenizer)
query = 'ä½ å¥½'
resp = inference_vllm(llm_engine, template, [{'query': query}])[0]
print(f"response: {resp['response']}")
print(f"history: {resp['history']}")
```

## VLLM & LoRA

VLLM & LoRAæ”¯æŒçš„æ¨¡å‹å¯ä»¥æŸ¥çœ‹: https://docs.vllm.ai/en/latest/models/supported_models.html

### å‚è€ƒèµ„æ–™ 

https://cloud.tencent.com/developer/article/2422872

vllm æ¨ç†è„šæœ¬ æ¶ˆè€—16Gæ˜¾å­˜

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    ModelType, get_vllm_engine, get_default_template_type,
    get_template, inference_vllm
)
ckpt_dir = 'output/qwen-7b-chat/v12-20240607-114403/qwen-7b-chat-gptq-int4'
model_type = ModelType.qwen_7b_chat_int4
template_type = get_default_template_type(model_type)
llm_engine = get_vllm_engine(model_type,model_id_or_path=ckpt_dir,gpu_memory_utilization=0.5,max_model_len=1024)
template_type = get_default_template_type(model_type)
template = get_template(template_type, llm_engine.hf_tokenizer)
# ä¸`transformers.GenerationConfig`ç±»ä¼¼çš„æ¥å£
llm_engine.generation_config.max_new_tokens = 256

request_list = [{'query': 'ä½ å¥½!'}, {'query': 'æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ'}]
resp_list = inference_vllm(llm_engine, template, request_list)
for request, resp in zip(request_list, resp_list):
    print(f"query: {request['query']}")
    print(f"response: {resp['response']}")
    
CUDA_VISIBLE_DEVICES=0 swift export \
    --ckpt_dir output/baichuan2-7b/v16-20240618-213516/checkpoint-1293 \
    --to_peft_format true

CUDA_VISIBLE_DEVICES=0 swift infer \
    --ckpt_dir output/baichuan2-7b/v16-20240618-213516/checkpoint-1293-peft \
    --infer_backend vllm \
    --quantization_bit 4 \
    --vllm_enable_lora true

CUDA_VISIBLE_DEVICES=0 swift export \
    --ckpt_dir 'output/baichuan2-7b/v16-20240618-213516/checkpoint-1293' --merge_lora true

```



https://github.com/echonoshy/cgft-llm

ä½¿ç”¨llama-factoryå¾®è°ƒllama3

llama.cppè¿›è¡Œè½»é‡åŒ–éƒ¨ç½²å’Œé‡åŒ–

ChatTTSå®ç°è¯­éŸ³ç”Ÿæˆ

Ollamaå¤§æ¨¡å‹éƒ¨ç½²å·¥å…·ä»‹ç»



# vllm å‹æµ‹è„šæœ¬ é“¾æ¥

 https://blog.csdn.net/arkohut/article/details/139076652

# Qwen2 å¾®è°ƒé‡åŒ–  awq  gptq  vllméƒ¨ç½²

https://qwen.readthedocs.io/zh-cn/latest/quantization/gptq.html

https://docs.vllm.ai/en/latest/quantization/auto_awq.html

```python
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer
import json
# Specify paths and hyperparameters for quantization
model_path = "/root/models/Qwen2-7B-Instruct-merged"
quant_path = "/root/models/Qwen2-7B-Instruct-awq"
quant_config = { "zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM" }

# Load your tokenizer and model with AutoAWQ
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoAWQForCausalLM.from_pretrained(model_path, device_map="auto", safetensors=True)
data = []
with open('data/llama3.json', 'r') as file:
    # ä½¿ç”¨json.load()æ–¹æ³•è¯»å–æ–‡ä»¶å†…å®¹
    alist = json.load(file)
for i in alist:
    instruction = i['instruction']
    output =i['output']
    msg=[
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": instruction},
    {"role": "assistant", "content": output}
            ]
    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)
    data.append(text.strip())
model.quantize(tokenizer, quant_config=quant_config, calib_data=data)
model.save_quantized(quant_path, safetensors=True, shard_size="4GB")
tokenizer.save_pretrained(quant_path)
~                                       
```

V100 ä¸æ”¯æŒvllm æ¨ç†awq A10æ¨ç†æ”¯æŒawq   V100 æ”¯æŒgptqæ¨ç†

```
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from transformers import AutoTokenizer
import json
import torch
# Specify paths and hyperparameters for quantization
model_path = "/root/models/Qwen2-7B-Instruct-merged"
quant_path = "/root/models/Qwen2-7B-Instruct-gptq"
quantize_config = BaseQuantizeConfig(
    bits=4, # 4 or 8
    group_size=128,
    damp_percent=0.01,
    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
    static_groups=False,
    sym=True,
    true_sequential=True,
    model_name_or_path=None,
    model_file_base_name="model"
)
max_len = 1024
# Load your tokenizer and model with AutoAWQ
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)
data = []
with open('data/llama3.json', 'r') as file:
    # ä½¿ç”¨json.load()æ–¹æ³•è¯»å–æ–‡ä»¶å†…å®¹
    alist = json.load(file)
for i in alist:
    instruction = i['instruction']
    output =i['output']
    msg=[
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": instruction},
    {"role": "assistant", "content": output}
            ]
    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)
    model_inputs = tokenizer([text])
    input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)
    data.append(dict(input_ids=input_ids, attention_mask=input_ids.ne(tokenizer.pad_token_id)))

import logging

logging.basicConfig(
    format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
)
model.quantize(data, cache_examples_on_gpu=False)

model.save_quantized(quant_path, use_safetensors=True)
tokenizer.save_pretrained(quant_path)
```





å…¬å¸ECSè´¦å·ä¿¡æ¯

```
å‘é‡æ£€ç´¢æœåŠ¡DashVector  sk-ay5CiUpu4shQpO5OcgNOXUcSoPIUt27FFD251ADCA11EEBB06B696248210CF

å…¬å¸é˜¿é‡Œäº‘è´¦å·å¯†ç 
ç”¨æˆ·ç™»å½•åç§° tech@szby.onaliyun.com 
ç™»å½•å¯†ç  SZBY#202310@pro
ç™»å½•åœ°å€ï¼šhttps://signin.aliyun.com/szby.onaliyun.com/login.htm?spm=5176.28366559.0.0.211e336aaM5l2Y#/main

ç”¨æˆ·ç™»å½•åç§° tech@szby.onaliyun.com 
ç™»å½•å¯†ç  SZBY#202310@pro
ç™»å½•åœ°å€ï¼šhttps://signin.aliyun.com/szby.onaliyun.com/login.htm?spm=5176.28366559.0.0.211e336aaM5l2Y#/main

docker run -it --name baichuan registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.1 /bin/bash

 HF_TOKEN  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
swift å¤§æ¨¡å‹ä»‹ç»
https://github.com/modelscope/modelscope-classroom/blob/main/LLM-tutorial/C.%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-prompt%20engineering.md


https://chatai.shenzhouby.com/#/chatWindow/index
17683857486
123456

saaa:https://chatai.shenzhouby.com/#/chatWindow/index
è´¦å·ï¼šalincy
å¯†ç ï¼š123456

æ³¨é‡Šè¯´æ˜ï¼šè¯·ä¸è¦ä¿®æ”¹è¯¥åŒºåŸŸä¸‹ä»»ä½•å†…å®¹ï¼Œæ­¤å¤„ä¸ºçŸ¥è¯†æ¨¡ç‰ˆè¯´æ˜ä»‹ç»ï¼Œä¸ä¼šå½±å“çŸ¥è¯†å¯¼å…¥çš„è´¨é‡ã€‚å»ºè®®æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ¥å¡«å…¥æ¨¡æ¿å†…å®¹ï¼š
1. ä¸€çº§ç›®å½•è¯·å°½é‡æŒ‰ç…§é€šç”¨æ„å›¾æ¥æ„å»ºï¼Œç›®å‰æ¨¡æ¿ä¸­ä½“ç°çš„æ˜¯ç”µå•†è¡Œä¸šä¸­å·²ç»è¿‡è°ƒä¼˜åçš„è¡Œä¸šæ„å›¾
2. äºŒçº§ç›®å½•ç”¨äºå±•ç¤ºé€šç”¨æ„å›¾ä¸‹çš„å­æ„å›¾é—®é¢˜
3. Q&Aå¯¹åº”æ˜¯è¯¥å…·ä½“é—®é¢˜åœ¨æ„å›¾ä¸‹çš„å…·ä½“è§£å†³æ–¹æ³•ã€‚
ä¸¥ç¦ä¿®æ”¹æœ¬æ–‡æ¡£æ ¼å¼ï¼Œè¡¨å¤´ç›®å½•ä¸å¯ä½œä¿®æ”¹ï¼Œå¦åˆ™ä¸Šä¼ æ— æ•ˆï¼


pip install csrc/layer_norm

è¿è¥ç®¡ç†ç³»ç»Ÿ
https://system.shenzhouby.com
szby
szby20231229

##### LD_LIBRARY_PATH=/usr/local/cuda-11.7/lib64

export LD_LIBRARY_PATH=/root/miniconda3/envs/moe/lib/python3.10/site-packages/nvidia/nccl/lib:$LD_LIBRARY_PATH


## baichuan24G     ESCDtuxD0wvEE5MWcNyA85gnNbEDM3

Colossal-AI å¯†ç  ESCDtuxD0wvEE5MWcNyA85gnNbEDM3colossal@

curl -X POST -H "Content-Type: application/json" -d '{"query": "å¯ä»¥å‘é¡ºä¸°è´§åˆ°ä»˜æ¬¾å—"}' http://127.0.0.1:5000/get_semantic

curl -X POST -H "Content-Type: application/json" -d '{"query": "ä»å“ªé‡Œå‘è´§"}' http://192.168.1.40:8000/get_semantic

curl -X POST -H "Content-Type: application/json" -d '{"query": "ä»å“ªé‡Œå‘è´§","answer":"å®‰å¾½äº³å·"}' http://192.168.1.40:8000/human_reply

curl -X POST -H "Content-Type: application/json" -d '{"query": "ä»å“ªé‡Œå‘è´§","answer":"å®‰å¾½äº³å·"}' http://192.168.1.40:8000/human_reply

ps aux | grep gunicorn

python -m gunicorn -w 2 -b 192.168.1.40:8000 app:app
nohup python -m gunicorn -w 2 -b 192.168.1.40:8000 app:app > output.log 2>&1 &

nohup python -m gunicorn -w 2 -b 192.168.1.40:6000 app_infer:app > infer.log 2>&1 &  å¯åŠ¨è¯­ä¹‰ç†è§£ å’Œå•æ„å›¾å¤šæ„å›¾    curl -X POST -H "Content-Type: application/json" -d '{"query": "å¯ä»¥å‘é¡ºä¸°è´§åˆ°ä»˜æ¬¾å—"}' http://192.168.1.40:6000/segment

nohup python -m gunicorn -w 2 -b 192.168.1.40:8001 medical_app:app > hospital.log 2>&1 &  å¯åŠ¨è¯­ä¹‰ç†è§£ å’Œå•æ„å›¾å¤šæ„å›¾
nohup python delopy.py  > hospital.log 2>&1 & 
```

# è‹±ä¼Ÿè¾¾NeMo

https://github.com/NVIDIA/NeMo



# è¯­éŸ³åˆæˆTTS

edge-tts

### 1. **å®‰è£… edge-tts**

é¦–å…ˆï¼Œç¡®ä¿æ‚¨å·²å®‰è£… `edge-tts`ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…ï¼š

```
pip install edge-tts

2. åˆ—å‡ºå¯ç”¨è¯­éŸ³
ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹å¯ç”¨çš„ä¸­æ–‡è¯­éŸ³ï¼š
edge-tts --list-voices
```

### 3. **ç”Ÿæˆä¸­æ–‡è¯­éŸ³**

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ç”Ÿæˆä¸­æ–‡è¯­éŸ³ã€‚å‡è®¾æ‚¨æ‰¾åˆ°çš„ä¸­æ–‡è¯­éŸ³åç§°ä¸º `zh-CN-XiaoxiaoNeural`ï¼Œæ‚¨å¯ä»¥è¿è¡Œï¼š

edge-tts --voice zh-CN-XiaoxiaoNeural --text "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­æ–‡è¯­éŸ³çš„æµ‹è¯•ã€‚" --write-media output.mp3 --write-subtitles output.vtt

### 4. **æ’­æ”¾ç”Ÿæˆçš„è¯­éŸ³**

å¦‚æœæ‚¨æƒ³ç«‹å³æ’­æ”¾ç”Ÿæˆçš„è¯­éŸ³ï¼Œå¯ä»¥ä½¿ç”¨ `edge-playback` å‘½ä»¤ï¼š

edge-playback --voice zh-CN-XiaoxiaoNeural --text "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸­æ–‡è¯­éŸ³çš„æµ‹è¯•ã€‚"

edge-playback --voice zh-CN-XiaoxiaoNeural --text "ä½ å¥½ï¼Œç”Ÿæˆè¯­éŸ³å¹¶ä¿å­˜åˆ°æœ¬åœ°"

# einopsç”¨æ³•  repeat,rearrange

https://blog.csdn.net/qq_37297763/article/details/120348764

```python
from einops import repeat,rearrange
import numpy as np
images=[ np.random.randn(30,40,3) for _ in range(32)]

print(rearrange(images,'b h w c-> b c h w').shape) # 32,30,40,3 -->32,3,30,40

print(rearrange(stacked_images,'b h w c -> (b h) w c').shape)  (960, 40, 3)
print(rearrange(images,'b h w c-> b c (h w) ').shape)   (32, 3, 1200)


```

## einops.repeat()å¢åŠ ç»´åº¦

```
import numpy as np

images=np.random.randn(32,32)
print(repeat(images,'h w -> h w c',c =3).shape)  (32, 32, 3)

æ‰©å¢heightï¼Œå˜ä¸ºåŸæ¥çš„2å€
print(repeat(images,'h w -> (repeat h) w',repeat =2).shape) (64,32)
æ‰©å¢weightï¼Œå˜ä¸ºåŸæ¥çš„2å€
print(repeat(images,'h w -> h (repeat w)',repeat=2).shape) (32,64)
æŠŠæ¯ä¸€ä¸ªpixelæ‰©å……4å€
print(repeat(images,'h w -> (h h1) (w w1)',h1=2,w1=2).shape)
```

1. å…ˆä¸‹é‡‡æ ·ï¼Œç„¶åä¸Šé‡‡æ ·

pixelate image first by downsampling by 2x, then upsamplingï¼š(30, 40)

downsampled = reduce(image, '(h h2) (w w2) -> h w', 'mean', h2=2, w2=2)
print(repeat(downsampled, 'h w -> (h h2) (w w2)', h2=2, w2=2).shape)

### reduceå‡å°‘ä¸€ç»´

```
from einops import reduce
images =np.random.randn(3,30,40)
print(reduce(images,'c h w -> h w','mean').shape)
```

### reduce æ¨¡æ‹Ÿæœ€å¤§æ± åŒ–åŠŸèƒ½

```
x = np.random.randn(10, 20, 30, 40)
# 2d max-pooling with kernel size = 2 * 2 for image processing:(10, 20, 15, 20)
y1 = reduce(x, 'b c (h1 h2) (w1 w2) -> b c h1 w1', 'max', h2=2, w2=2)
print(y1.shape)
# Output
# (10, 20, 15, 20)
images=np.random.randn(3,3,30,40)

print(reduce(images,'b c (h1 h2) (w1 w2) -> b c h1 w1','max',h2=2,w2=2).shape)


```

### reduceå…¨å±€å¹³å‡æ± åŒ–

```
images=np.random.randn(3,3,30,40)
print(reduce(images,'b c h w -> b c','mean').shape)  (3, 3)
```

### torch.stack å’Œtorch.catç”¨æ³• 

```
# å‡è®¾æ˜¯æ—¶é—´æ­¥T1çš„è¾“å‡º
T1 = torch.tensor([[1, 2, 3],
        		[4, 5, 6],
        		[7, 8, 9]])
# å‡è®¾æ˜¯æ—¶é—´æ­¥T2çš„è¾“å‡º
T2 = torch.tensor([[10, 20, 30],
        		[40, 50, 60],
        		[70, 80, 90]])

t=torch.stack((T1,T2),dim=0)
print(t.shape)  torch.Size([2, 3, 3])

tensor([[[ 1,  2,  3],
         [ 4,  5,  6],
         [ 7,  8,  9]],

        [[10, 20, 30],
         [40, 50, 60],
         [70, 80, 90]]])

t=torch.stack((T1,T2),dim=1)    torch.Size([3, 2, 3])
print(t)
tensor([[[ 1,  2,  3],
         [10, 20, 30]],

        [[ 4,  5,  6],
         [40, 50, 60]],

        [[ 7,  8,  9],
         [70, 80, 90]]])
         
t=torch.stack((T1,T2),dim=2)
print(t)     torch.Size([3, 3, 2])
tensor([[[ 1, 10],
         [ 2, 20],
         [ 3, 30]],

        [[ 4, 40],
         [ 5, 50],
         [ 6, 60]],

        [[ 7, 70],
         [ 8, 80],
         [ 9, 90]]])
         
 x1 = torch.tensor([[11, 21, 31], [21, 31, 41]], dtype=torch.int)
 x2 = torch.tensor([[12, 22, 32], [22, 32, 42]], dtype=torch.int)
 inputs = [x1, x2]
 R0 = torch.cat(inputs, dim=0)
 print("R0.shape:\n", R0.shape)
'''
R0:
 tensor([[11, 21, 31],
        [21, 31, 41],
        [12, 22, 32],
        [22, 32, 42]])
R0.shape:
 torch.Size([4, 3])
 
 R1 = torch.cat(inputs, dim=1)
print("R1:\n", R1)
print("R1.shape:\n", R1.shape)
'''
R1:
 tensor([[11, 21, 31, 12, 22, 32],
        [21, 31, 41, 22, 32, 42]])
R1.shape:
 torch.Size([2, 6])

```

### HfArgumentParserç”¨æ³• 

```
from transformers import HfArgumentParser
from dataclasses import dataclass,field
from typing import Any,Dict,Optional,OrderedDict,Union,Tuple,DefaultDict

@dataclass
class ModelArgs():
	a:str =field(default='alice')

@dataclass
class DataArgs():
	b:str =field(default='bbb')

@dataclass
class LLmArgs():
	model_path:field(default=None)

parser = HfArgumentParser((ModelArgs,DataArgs,LLmArgs))
model_args,data_args,llm_args = parser.parse_args_into_dataclasses()
print(model_args,data_args,llm_args)

```



### softmax å…¬å¼

```
def softmax(x,axis=1):
    # ä¸ºäº†é˜²æ­¢æº¢å‡ºï¼Œå‡å»æœ€å¤§å€¼
    ex =np.exp(x-np.max(x,axis=axis,keepdims=True))
    return ex/ex.sum(axis=axis,keepdims=True)

scores = np.array([[2.0, 1.0, 0.1],
                   [1.0, 2.0, 3.0]])
probabilities = softmax(scores, axis=1)
print(probabilities)
```

### çŸ¥é“äº†softmaxçš„å…¬å¼ è¿›è¡Œåå‘ä¼ æ’­

```python
def softmax_backward(softmax_output,y_true):
	# softmax_output: softmax å‡½æ•°çš„è¾“å‡º
    # y_true: çœŸå®æ ‡ç­¾ï¼Œone-hot ç¼–ç 
    batch_size =y_true.shape[0]
    #è®¡ç®—æ¢¯åº¦
    grad = (softmax_output -y_true)
    #å¹³å‡æ¢¯åº¦
    grad =grad/batch_size
    return grad 
	z = np.array([2.0, 1.0, 0.1])
    
    # è®¡ç®— softmax è¾“å‡º
    softmax_output = softmax(z)
    print("Softmax Output:", softmax_output)
    
    # çœŸå®æ ‡ç­¾ï¼ˆone-hot ç¼–ç ï¼‰
    y_true = np.array([[1, 0, 0]])  # å‡è®¾ç¬¬ä¸€ç±»ä¸ºçœŸå®æ ‡ç­¾
    # åå‘ä¼ æ’­
    grad = softmax_backward(softmax_output.reshape(1, -1), y_true)
    print("Gradient:", grad)
    
    
z=np.array([2,1,0.2])
## çœŸå®æ ‡ç­¾ï¼ˆone-hot ç¼–ç ï¼‰
y_true =np.array([1.0,0,0])
softmax_output =  softmax(z)
loss = cross_entropy_loss(y_true,softmax_output)

def softmax_backward(softmax_output,y_true):
    batch_size =y_true[0]
    grad =softmax_output -y_true
    grad = grad /batch_size
    return grad
learn_rate =1e-5
```

### äºŒåˆ†ç±»äº¤å‰ç†µä»£ç 

```
def binary_cross_entropy(y_true,y_pred):
    epsilon =1e-12 # # é˜²æ­¢å¯¹æ•°è®¡ç®—ä¸­çš„æ•°å€¼é—®é¢˜
    y_pred =np.clip(y_pred,epsilon,1-epsilon)  # é™åˆ¶é¢„æµ‹å€¼èŒƒå›´
    loss = -(y_true*np.log(y_pred)+(1-y_true)*np.log(1-y_pred))
    return np.mean(loss)
y_true = np.array([1, 0, 1, 0])
# æ¨¡å‹é¢„æµ‹æ¦‚ç‡
y_pred = np.array([0.9, 0.2, 0.8, 0.1])
# è®¡ç®—æŸå¤±
loss = binary_cross_entropy(y_true, y_pred)
print("Binary Cross-Entropy Loss:", loss)

```



# loggingä¿å­˜æ—¥å¿—

```python
import logging
import sys
from transformers.utils.logging import (enable_default_handler,
                                        enable_explicit_format, set_verbosity)
logger = logging.getLogger(__name__)
logging.basicConfig(
        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
        datefmt='%m/%d/%Y %H:%M:%S',
        handlers=[logging.StreamHandler(sys.stdout),
                  logging.FileHandler('logfile.log') ],
    )
log_level='info'
logger.setLevel(logging.INFO)
set_verbosity(logging.INFO)
enable_default_handler()
enable_explicit_format()
logger.info(f'Training/evaluation parameters ')
```



# å„ç§modules

(è¿™ä¸ªä»£ç æ˜¯ VisCPMé‡Œé¢çš„ )

## 1. Linear 

```python

import torch
import math
import torch.nn.functional as F

class Linear(torch.nn.Module):
    def __init__(
        self,
        dim_in: int,
        dim_out: int,
        dtype: torch.dtype = torch.half,
        init_mean: float = 0.0,
        init_std: float = 1,
        scale_before: bool = False,
    ):
        super().__init__()
        self.dim_in = self.in_features = dim_in
        self.dim_out = self.out_features = dim_out
        self.scale_before = scale_before

        self.weight = torch.nn.parameter.Parameter(torch.empty((dim_out, dim_in), dtype=dtype))

    def forward(self, x: torch.Tensor):
        """
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_in)``): The input of linear layer
        Returns:
            :obj:`torch.Tensor` of shape ``(batch, seq_len, dim_out)``: The output of the linear transform y.
        """  # noqa: E501
        if self.scale_before:
            x = x / math.sqrt(self.dim_in)
            x = F.linear(x, self.weight)
        else:
            x = F.linear(x, self.weight)
            x = x / math.sqrt(self.dim_in)
        return x

```

è¿™ä¸ªè‡ªå®šä¹‰çš„ Linear ç±»ä¸ PyTorch çš„æ ‡å‡† Linear å±‚ç›¸æ¯”ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŒºåˆ«å’Œç‰¹ç‚¹ï¼šç¼©æ”¾åŠŸèƒ½:
è¯¥ç±»é€šè¿‡ scale_before å‚æ•°æ§åˆ¶è¾“å…¥ x æ˜¯å¦åœ¨è¿›è¡Œçº¿æ€§å˜æ¢å‰è¿›è¡Œç¼©æ”¾ï¼ˆé™¤ä»¥ \sqrt{\text{dim_in}}ï¼‰ã€‚
è¿™ç§ç¼©æ”¾å¯ä»¥å¸®åŠ©ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œå°¤å…¶åœ¨æ·±åº¦ç½‘ç»œä¸­ã€‚
å‚æ•°åˆå§‹åŒ–:
åœ¨è¿™ä¸ªç±»ä¸­ï¼Œæƒé‡å‚æ•°çš„åˆå§‹åŒ–æ˜¯å®šä¹‰çš„ï¼Œä½†æ²¡æœ‰åœ¨ __init__ æ–¹æ³•ä¸­æ˜ç¡®è¿›è¡Œåˆå§‹åŒ–ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåˆå§‹åŒ–å¯èƒ½éœ€è¦è°ƒç”¨ torch.nn.init å‡½æ•°æ¥ç¡®ä¿æƒé‡åˆå§‹åŒ–å¾—å½“ã€‚
è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶:
è¾“å…¥å½¢çŠ¶ä¸º (batch, seq_len, dim_in)ï¼Œè¾“å‡ºå½¢çŠ¶ä¸º (batch, seq_len, dim_out)ï¼Œè¿™ç§è®¾è®¡é€‚åˆå¤„ç†åºåˆ—æ•°æ®ï¼ˆå¦‚ RNNã€Transformer ç­‰ï¼‰ã€‚
æ•°æ®ç±»å‹æ”¯æŒ:
dtype å‚æ•°å…è®¸é€‰æ‹©æƒé‡å‚æ•°çš„æ•°æ®ç±»å‹ï¼Œæ”¯æŒå¦‚ torch.half ç­‰ä¸åŒçš„ç²¾åº¦ï¼Œè¿™åœ¨æ¨¡å‹è®­ç»ƒä¸­å¯ä»¥å¸®åŠ©èŠ‚çœå†…å­˜ã€‚
æ€»ç»“
è¿™ä¸ªè‡ªå®šä¹‰çš„ Linear å±‚æä¾›äº†è¾“å…¥ç¼©æ”¾çš„åŠŸèƒ½ï¼Œé€‚åˆç‰¹å®šåœºæ™¯çš„éœ€æ±‚ï¼ŒåŒæ—¶ä¿ç•™äº† PyTorch çš„çº¿æ€§å±‚çš„åŸºæœ¬ç‰¹æ€§ã€‚æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯ï¼Œè¿™ç§è®¾è®¡å¯ä»¥å¸®åŠ©æé«˜æ¨¡å‹çš„è®­ç»ƒæ•ˆæœå’Œç¨³å®šæ€§ã€‚



## 2.LayerNorm

```python
import torch
#ä½¿ç”¨ @torch.jit.script è£…é¥°å™¨ï¼Œä½¿å¾—è¿™ä¸ªå‡½æ•°å¯ä»¥è¢« TorchScript ç¼–è¯‘ï¼Œä»è€Œæé«˜æ‰§è¡Œæ•ˆç‡å’Œå¯éƒ¨ç½²æ€§ã€‚
@torch.jit.script  # type: ignore
def rms_layernorm(hidden: torch.Tensor, weight: torch.Tensor, eps: float):
    old_dtype = hidden.dtype
    variance = hidden.to(torch.float32).pow(2).mean(dim=-1, keepdim=True)
    hidden = (hidden * torch.rsqrt(variance + eps)).to(old_dtype)
    return hidden * weight

class LayerNorm(torch.nn.Module):
    """RMS LayerNorm"""

    def __init__(
        self,
        dim_norm: int,
        dtype: torch.dtype = torch.half,
        eps: float = 1e-6,
        init_var: float = 1.0,
    ):
        super().__init__()
        self.eps = eps
        self.dim_norm = dim_norm
        self.weight = torch.nn.parameter.Parameter(torch.full((dim_norm,), init_var, dtype=dtype))

    def forward(self, x: torch.Tensor):
        """
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch_size, seq_len, dim_norm)``): Input tensor that need to be normalized.
        Return:
            :obj:`torch.Tensor` of shape ``(batch_size, seq_len, dim_norm)``: The layernorm output.
        """  # noqa: E501
        assert x.size(-1) == self.dim_norm
        return rms_layernorm(x, self.weight, self.eps)
    

  class Layer(torch.nn.Module):
    def __init__(self,hidden_size,eps=1e-6):
        super.__init__()
        self.eps =eps
        self.weight =torch.nn.parameter(torch.ones(hidden_size))

    def forward(self,x):
        input_type = x.dtype
        x = x.to(torch.float32)
        variance = x.pow(2).mean(dim=-1,keepdim = True)
        hidden_state = x*torch.rsqrt(variance+self.eps)
        return self.weight*hidden_state.to(input_type)
```

ä½¿ç”¨ @torch.jit.script è£…é¥°å™¨ï¼Œä½¿å¾—è¿™ä¸ªå‡½æ•°å¯ä»¥è¢« TorchScript ç¼–è¯‘ï¼Œä»è€Œæé«˜æ‰§è¡Œæ•ˆç‡å’Œå¯éƒ¨ç½²æ€§ã€‚

python

å¤åˆ¶
variance = hidden.to(torch.float32).pow(2).mean(keepdim=True, dim=-1)

1. mean å‡½æ•°
åŠŸèƒ½: è®¡ç®—è¾“å…¥å¼ é‡æ²¿æŒ‡å®šç»´åº¦çš„å‡å€¼ã€‚
ç”¨æ³•: mean(dim) ä¼šå¯¹æŒ‡å®šçš„ç»´åº¦ dim è¿›è¡Œæ±‚å‡å€¼æ“ä½œã€‚
2. keepdim=True
åŠŸèƒ½: ä¿æŒè¾“å‡ºå¼ é‡çš„ç»´åº¦ä¸è¾“å…¥å¼ é‡ç›¸åŒã€‚
æ•ˆæœ: å½“ keepdim=True æ—¶ï¼Œæ±‚å‡å€¼çš„ç»´åº¦å°†è¢«ä¿ç•™ä¸º 1ï¼Œè€Œä¸æ˜¯è¢«åˆ é™¤ã€‚è¿™æœ‰åŠ©äºåç»­çš„å¼ é‡æ“ä½œï¼Œä½¿ç»´åº¦åŒ¹é…ã€‚
ç¤ºä¾‹
å‡è®¾ hidden çš„å½¢çŠ¶ä¸º (batch_size, seq_len, features)ï¼Œåˆ™ï¼š

hidden.to(torch.float32).pow(2) å°†è®¡ç®—æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹ï¼Œç»“æœå½¢çŠ¶ä¿æŒä¸å˜ã€‚
ä½¿ç”¨ mean(dim=-1) è®¡ç®—æœ€åä¸€ç»´ï¼ˆfeaturesï¼‰çš„å‡å€¼ï¼Œç»“æœå½¢çŠ¶ä¸º (batch_size, seq_len)ã€‚
keepdim=True ä¼šå°†ç»“æœå½¢çŠ¶ä¿æŒä¸º (batch_size, seq_len, 1)ã€‚

```
hidden=torch.randn(2,32,32)
ss= hidden.to(torch.float32).pow(2).mean(dim=-1,keepdim=True)
print(ss.shape)  torch.Size([2, 32, 1])
```

## 3.config

```python
import json
import os
import copy
from typing import Any, Dict, Union

class Config(object):
    """model configuration"""

    def __init__(self,**kwargs):
        super().__init__()
        for key, value in kwargs.items():
            setattr(self, key, value)
    @classmethod
    def from_json_file(cls, json_file: Union[str, os.PathLike], **args):
        config_dict = cls._dict_from_json_file(json_file, **args)
        return cls(**config_dict)

    @classmethod
    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike], **args):
        with open(json_file, "r", encoding="utf-8") as reader:
            text = reader.read()
        res = json.loads(text)
        for key in args:
            res[key] = args[key]
        return res

    def to_json_file(self, json_file_path: Union[str, os.PathLike]):
        with open(json_file_path, "w", encoding="utf-8") as writer:
            writer.write(self.to_json_string())
    def to_json_string(self) -> str:
        config_dict = self.to_dict()
        return json.dumps(config_dict, indent=2, sort_keys=True) + "\n"

    def to_dict(self) -> Dict[str, Any]:
        output = copy.deepcopy(self.__dict__)
        return output
è°ƒç”¨æ–¹æ³•
config =Config.from_json_file('zero2.json')
print(config.to_dict())
{'fp16': {'enabled': 'auto', 'loss_scale': 0, 'loss_scale_window': 1000, 'initial_scale_power': 16, 'hysteresis': 2, 'min_loss_scale': 1}, 'bf16': {'enabled': 'auto'}, 'train_micro_batch_size_per_gpu': 'auto', 'train_batch_size': 'auto', 'gradient_accumulation_steps': 'auto', 'zero_optimization': {'stage': 2, 'overlap_comm': True, 'contiguous_gradients': True, 'sub_group_size': 1000000000.0, 'reduce_bucket_size': 'auto'}}

config.to_json_file(json_file_path='config.json') #ä¿å­˜ä¸ºconfig.json æ–‡ä»¶
```

## 4.ç»´åº¦æ‰©å±• x=x[...,None]

```python
x =torch.randn(3,4)
x =x[...,None]
print(x.shape)
torch.Size([3, 4, 1])

x =torch.randn(3,4) # ä¸­é—´å¢åŠ ä¸€ä¸ªç»´åº¦ 
x =x[:,None,:]
print(x.shape) 
torch.Size([3, 1, 4])

x =torch.randn(3,4,4)
x =x[:,None,:,:]
print(x.shape)
torch.Size([3, 1, 4, 4])

```

```python
s =torch.ones(3,4,5)
res=s[:,:2,:]
res =s[...,:2,:]
print(res.shape)  torch.Size([3, 2, 5])

def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)

s[..., :2]:
... è¡¨ç¤ºä¿ç•™å‰é¢çš„æ‰€æœ‰ç»´åº¦ï¼ˆå³ 3 å’Œ 4ï¼‰ã€‚
:2 è¡¨ç¤ºåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸­é€‰æ‹©å‰ä¸¤ä¸ªå…ƒç´ ã€‚

s =torch.randn(4,8,12,16)
batch,seq_len,h,w =s.shape
s =s[:,:,None,:,:]
repeat_dim =6
print(s.expand(batch, seq_len, repeat_dim, h, w).shape) torch.Size([4, 8, 6, 12, 16])
åˆ‡ç‰‡æ“ä½œ [:, :, None, :, :]:
åœ¨ç¬¬ä¸‰ä¸ªç»´åº¦æ’å…¥ä¸€ä¸ªæ–°çš„ç»´åº¦ï¼Œç»“æœå½¢çŠ¶å¯èƒ½å˜ä¸º (batch, num_heads, 1, seq_len, head_dim)ã€‚

```







## 5. cos()   sin() ç”¨æ³•

```
x =torch.tensor([1,2,3,4],dtype=torch.float32)
print(x.sin())
print(x.cos())
tensor([ 0.8415,  0.9093,  0.1411, -0.7568])
tensor([ 0.5403, -0.4161, -0.9900, -0.6536])
```

## 6. position_embedding   RotaryEmbedding

```
position_embedding

class RotaryEmbedding(torch.nn.Module):
    def __init__(
        self,
        dim,
        base=10000,
        distance_scale: Union[int, float] = 1,
        dtype: torch.dtype = torch.half,
    ):
        super().__init__()
        inv_freq = 1.0 / (
            base ** (torch.arange(0, dim, 2, device="cuda", dtype=torch.float32) / dim)
        ) #inv_freq: è®¡ç®—åé¢‘ç‡ï¼ˆinverse frequencyï¼‰ï¼Œç”¨äºç”Ÿæˆæ—‹è½¬åµŒå…¥çš„é¢‘ç‡ç‰¹å¾
        inv_freq = inv_freq.to(dtype)
        self.distance_scale = distance_scale
        self.dtype = dtype
        self.inv_freq = inv_freq

    def forward(self, x: torch.Tensor, x_pos: torch.Tensor):
        """
        Args:
            x (:obj:`torch.Tensor` of shape ``(..., dim)``): Inputs.
            x_pos (:obj:`torch.Tensor` of shape ``(...)``): Positions of inputs.
        """
        x_pos = x_pos * self.distance_scale
        #x_pos è¢«ç¼©æ”¾ï¼Œä»¥è°ƒæ•´å…¶åœ¨åµŒå…¥ä¸­çš„å½±å“ã€‚ 
        freqs = x_pos[..., None].to(self.dtype) * self.inv_freq[None, :]  # (..., dim/2)

        # the same implementation as sat
        emb = torch.cat((freqs, freqs), dim=-1)  # (..., dim)
        emb_cos = emb.cos()  # (..., dim)
        emb_sin = emb.sin()  # (..., dim)

        rotate_x = torch.cat(
            [-x[..., x.size(-1) // 2 :], x[..., : x.size(-1) // 2]], dim=-1
        )  # (..., dim)

        return x * emb_cos + rotate_x * emb_sin

```



## 7.feedforward

```python
import torch
import math
import torch.nn.functional as F

class Linear(torch.nn.Module):
    def __init__(
        self,
        dim_in: int,
        dim_out: int,
        dtype: torch.dtype = torch.float32,
        init_mean: float = 0.0,
        init_std: float = 1,
        scale_before: bool = False,
    ):
        super().__init__()
        self.dim_in = self.in_features = dim_in
        self.dim_out = self.out_features = dim_out
        self.scale_before = scale_before

        self.weight = torch.nn.parameter.Parameter(torch.empty((dim_out, dim_in), dtype=dtype))

    def forward(self, x: torch.Tensor):
        """
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_in)``): The input of linear layer
        Returns:
            :obj:`torch.Tensor` of shape ``(batch, seq_len, dim_out)``: The output of the linear transform y.
        """  # noqa: E501
        if self.scale_before:
            x = x / math.sqrt(self.dim_in)
            x = F.linear(x, self.weight)
        else:
            x = F.linear(x, self.weight)
            x = x / math.sqrt(self.dim_in)
        return x

class DenseGatedACT(torch.nn.Module):
    def __init__(
        self,
        dim_in: int,
        dim_ff: int,
        dtype=torch.float32,
    ):
        super().__init__()

        self.w_0 = Linear(
            dim_in=dim_in,
            dim_out=dim_ff,
            dtype=dtype,
            scale_before=False,
        )
        self.w_1 = Linear(
            dim_in=dim_in,
            dim_out=dim_ff,
            dtype=dtype,
            scale_before=False,
        )
        self.act = torch.nn.GELU()
    def forward(self, x: torch.Tensor):
        """Transform an input tensor from one feature space to another via a nonlinear operation
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_in)``): Tensor that will be subject to nonlinear operations.
        Return:
            out (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_ff)``)
        """  # noqa: E501
        gate_score = self.act(self.w_0(x))
        x = self.w_1(x)
        x = gate_score * x
        return x
dense = DenseGatedACT(dim_in=12,dim_ff=24,dtype=torch.float32)
x=torch.randn(3,12,12)
print(dense(x).shape)

1. è¾“å…¥å¼ é‡ x
å½¢çŠ¶ï¼š(batch, seq_len, dim_in)ï¼Œåœ¨è¿™ä¸ªä¾‹å­ä¸­ä¸º (3, 12, 12)ã€‚
batch = 3ï¼šè¡¨ç¤ºæœ‰ 3 ä¸ªæ ·æœ¬ã€‚
seq_len = 12ï¼šè¡¨ç¤ºåºåˆ—é•¿åº¦ä¸º 12ã€‚
dim_in = 12ï¼šè¡¨ç¤ºè¾“å…¥ç‰¹å¾çš„ç»´åº¦ä¸º 12ã€‚
2. ç¬¬ä¸€å±‚ w_0
çº¿æ€§å˜æ¢ï¼š
self.w_0 = Linear(dim_in=12, dim_out=24)ï¼šå°†è¾“å…¥çš„æœ€åä¸€ç»´ï¼ˆdim_in = 12ï¼‰æ˜ å°„åˆ° dim_out = 24ã€‚
è®¡ç®—ï¼š
ä½¿ç”¨ F.linear(x, self.weight)ï¼Œæ­¤æ—¶è¾“å…¥ x å½¢çŠ¶ä¸º (3, 12, 12)ï¼Œæƒé‡ self.weight å½¢çŠ¶ä¸º (24, 12)ã€‚
è¾“å‡ºå½¢çŠ¶ï¼š
è®¡ç®—åï¼Œgate_score çš„å½¢çŠ¶ä¸º (3, 12, 24)ã€‚
3. ç¬¬äºŒå±‚ w_1
çº¿æ€§å˜æ¢ï¼š
self.w_1 = Linear(dim_in=12, dim_out=24)ï¼šåŒæ ·å°†è¾“å…¥çš„æœ€åä¸€ç»´æ˜ å°„åˆ° dim_out = 24ã€‚
è®¡ç®—ï¼š
å†æ¬¡è°ƒç”¨ F.linear(x, self.weight)ï¼Œæ­¤æ—¶è¾“å…¥ x å½¢çŠ¶ä¸º (3, 12, 12)ï¼Œæƒé‡ self.weight å½¢çŠ¶ä¸º (24, 12)ã€‚
è¾“å‡ºå½¢çŠ¶ï¼š
è®¡ç®—åï¼Œx çš„å½¢çŠ¶ä¸º (3, 12, 24)ã€‚
4. Gate Score å’Œæœ€ç»ˆè¾“å‡º
Gate Scoreï¼š
gate_score çš„å½¢çŠ¶ä¸º (3, 12, 24)ï¼Œä»ç¬¬ä¸€å±‚å¾—åˆ°ã€‚
Multiply Operationï¼š
x = gate_score * xï¼šæ­¤æ—¶ x çš„å½¢çŠ¶ä¸º (3, 12, 24)ï¼Œå¹¶ä¸” gate_score ä¹Ÿæ˜¯ (3, 12, 24)ï¼Œæ‰€ä»¥å¯ä»¥è¿›è¡Œé€å…ƒç´ ç›¸ä¹˜ã€‚
è¾“å‡ºå½¢çŠ¶ï¼š
æœ€ç»ˆ x çš„å½¢çŠ¶ä»ç„¶æ˜¯ (3, 12, 24)ã€‚
æ€»ç»“
åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œç»´åº¦å˜åŒ–å¦‚ä¸‹ï¼š
è¾“å…¥ xï¼š(3, 12, 12)
ç¬¬ä¸€å±‚è¾“å‡º gate_scoreï¼š(3, 12, 24)
ç¬¬äºŒå±‚è¾“å‡º xï¼š(3, 12, 24)
æœ€ç»ˆè¾“å‡º xï¼š(3, 12, 24)
æœ€ç»ˆï¼Œè¾“å‡ºçš„å½¢çŠ¶æ˜¯ (3, 12, 24)ï¼Œè¡¨ç¤º 3 ä¸ªæ ·æœ¬ã€æ¯ä¸ªæ ·æœ¬æœ‰ 12 ä¸ªæ—¶é—´æ­¥ï¼Œæ¯ä¸ªæ—¶é—´æ­¥æœ‰ 24 ä¸ªç‰¹å¾ã€‚


class FeedForward(torch.nn.Module):
    r"""FeedForward module
    Args:
        dim_in (int): input dimension.
        dim_ff (int): middle dimension.
        dim_out (int, optional): output dimension. Defaults to None, which means dim_in = dim_out.
        dtype (optional): Defaults to torch.half.
        init_mean (float, optional): mean of :math:`\mathbf{W}\sim\mathcal{N}(\text{mean}, \text{std}^2)` for fully-connected module used in feed-forward layer. Defaults to 0.
        init_std (float, optional): std of :math:`\mathbf{W}\sim\mathcal{N}(\text{mean}, \text{std}^2)` for fully-connected module used in feed-forward layer. Defaults to 0.02.
        bias (bool, optional): whether to use bias term in fully-connected layers used in feed-forward module. Defaults to False.
        activate_fn (str, optional): Defaults to `gated_gelu`.
        dropout_p (int, optional): Defaults to 0.
    """  # noqa: E501

    def __init__(
        self,
        dim_model: int,
        dim_ff: int,
        dtype=torch.half,
        dropout_p: Optional[float] = None,
    ):
        super().__init__()
        self.w_in = DenseGatedACT(
            dim_in=dim_model,
            dim_ff=dim_ff,
            dtype=dtype,
        )
        if dropout_p is not None:
            self.dropout = torch.nn.Dropout(dropout_p)
        else:
            self.dropout = None

        self.w_out = Linear(
            dim_in=dim_ff,
            dim_out=dim_model,
            dtype=dtype,
            scale_before=True,
        )

    def forward(self, x: torch.Tensor):
        """
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_in)``): The input of feed-forward module.

        Return:
            :obj:`torch.Tensor` of shape ``(batch, seq_len, dim_out)``: The output of feed-forward module.
        """  # noqa: E501
        x = self.w_in(x)
        if self.dropout is not None:
            x = self.dropout(x)
        x = self.w_out(x)
        return x
```

##  8.`F.linear` çš„ç”¨æ³•

```
import torch.nn.functional as F
input =torch.randn(5,6)
weight =torch.nn.parameter.Parameter(torch.empty(10,6))
output= F.linear(input,weight)
print(output.shape)

import torch
import torch.nn.functional as F

# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¾“å…¥å¼ é‡ xï¼Œå…¶ç»´åº¦ä¸º (batch_size, input_features)
x = torch.randn(10, 5)  # 10ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰5ä¸ªç‰¹å¾

# å®šä¹‰æƒé‡çŸ©é˜µï¼Œå…¶ç»´åº¦åº”ä¸º (output_features, input_features)
# å‡è®¾æˆ‘ä»¬è¦å°†è¾“å…¥ç‰¹å¾æ˜ å°„åˆ°8ä¸ªè¾“å‡ºç‰¹å¾
weight = torch.randn(8, 5)

# ä½¿ç”¨ F.linear è¿›è¡Œçº¿æ€§å˜æ¢
# æ³¨æ„ï¼šF.linear å‡½æ•°é»˜è®¤å·²ç»åŒ…å«äº†æƒé‡çŸ©é˜µçš„è½¬ç½®æ“ä½œï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦æ‰‹åŠ¨è½¬æƒé‡
output = F.linear(x, weight)

print("Input Tensor:", x)
print("Weight Matrix:", weight)
print("Output Tensor:", output)
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼š

x æ˜¯è¾“å…¥å¼ é‡ï¼Œå…¶å½¢çŠ¶ä¸º (10, 5)ï¼Œè¡¨ç¤ºæœ‰10ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰5ä¸ªç‰¹å¾ã€‚
weight æ˜¯æƒé‡çŸ©é˜µï¼Œå…¶å½¢çŠ¶ä¸º (8, 5)ï¼Œè¡¨ç¤ºå°†5ä¸ªè¾“å…¥ç‰¹å¾æ˜ å°„åˆ°8ä¸ªè¾“å‡ºç‰¹å¾ã€‚
F.linear(x, weight) è®¡ç®—è¾“å…¥å¼ é‡ x å’Œæƒé‡çŸ©é˜µ weight çš„çŸ©é˜µä¹˜æ³•ï¼Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶å°†æ˜¯ (10, 8)ã€‚
è¯·æ³¨æ„ï¼ŒF.linear å‡½æ•°å®é™…ä¸Šç­‰ä»·äº torch.matmul(x, weight.t())ï¼Œå…¶ä¸­ weight.t() æ˜¯æƒé‡çŸ©é˜µçš„è½¬ç½®ã€‚ä½†æ˜¯ä½¿ç”¨ F.linear æ›´ä¸ºç®€æ´ï¼Œä¸” PyTorch å·²ç»å¯¹å†…éƒ¨æ“ä½œè¿›è¡Œäº†ä¼˜åŒ–ã€‚
```

## F.embeddingçš„ç”¨æ³• 

```
 F.embedding
 
 # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯æ±‡è¡¨å¤§å°ä¸º 10ï¼ŒåµŒå…¥ç»´åº¦ä¸º 4
num_embeddings = 10
embedding_dim = 4
ids = torch.tensor([1, 2, 3])  # ç¤ºä¾‹è¾“å…¥ ID
# åˆå§‹åŒ–åµŒå…¥æƒé‡
weight = torch.randn(num_embeddings, embedding_dim)
# è·å–åµŒå…¥å¹¶ç¼©æ”¾
embeds = F.embedding(ids, weight) / math.sqrt(embedding_dim)
print(embeds.shape)  torch.Size([3, 4])
```

F.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)

å‚æ•°
input: ä¸€ä¸ªåŒ…å«ç´¢å¼•çš„å¼ é‡ï¼Œé€šå¸¸ä¸ºé•¿æ•´å‹ï¼ˆtorch.LongTensorï¼‰ã€‚
weight: åµŒå…¥çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (num_embeddings, embedding_dim)ã€‚
padding_idx: å¦‚æœæŒ‡å®šï¼Œè¾“å…¥ä¸­å¯¹åº”çš„ç´¢å¼•å°†è¿”å›å…¨é›¶å‘é‡ã€‚
max_norm: å¦‚æœæŒ‡å®šï¼Œå°†å¯¹åµŒå…¥å‘é‡æ–½åŠ æœ€å¤§èŒƒæ•°çº¦æŸã€‚
norm_type: ç”¨äºè®¡ç®—èŒƒæ•°çš„ç±»å‹ï¼Œé»˜è®¤æ˜¯ 2ã€‚
scale_grad_by_freq: å¦‚æœä¸º Trueï¼Œå°†æ ¹æ®é¢‘ç‡ç¼©æ”¾æ¢¯åº¦ã€‚
sparse: å¦‚æœä¸º Trueï¼Œä½¿ç”¨ç¨€ç–æ›´æ–°ã€‚

```
import torch

# åˆ›å»ºä¸€ä¸ªåµŒå…¥çŸ©é˜µ
embedding_matrix = torch.tensor([[0.1, 0.2],
                                  [0.3, 0.4],
                                  [0.5, 0.6]])

# è¾“å…¥ç´¢å¼•
input_indices = torch.tensor([0, 2, 1])

# ä½¿ç”¨ F.embedding æŸ¥æ‰¾åµŒå…¥å‘é‡
embeddings = F.embedding(input_indices, embedding_matrix)

print(embeddings)
tensor([[0.1000, 0.2000],
        [0.5000, 0.6000],
        [0.3000, 0.4000]])
        
# åˆ›å»ºåµŒå…¥çŸ©é˜µ
embedding_matrix = torch.tensor([[0.1, 0.2],
                                  [0.3, 0.4],
                                  [0.5, 0.6]])
# è¾“å…¥ç´¢å¼•ï¼ŒåŒ…å« padding index
input_indices = torch.tensor([0, 2, 1, 0])
# ä½¿ç”¨ padding_idx
embeddings_with_padding = F.embedding(input_indices, embedding_matrix, padding_idx=0)
print(embeddings_with_padding)
tensor([[0.1000, 0.2000],
        [0.5000, 0.6000],
        [0.3000, 0.4000],
        [0.1000, 0.2000]])

```



## 9.embedding

```
class Embedding(torch.nn.Module):
    def __init__(
        self,
        vocab_size: int,
        embedding_size: int,
        dtype: torch.dtype = torch.half,
        init_mean: float = 0.0,
        init_std: float = 1,
    ):

        super().__init__()

        self.dim_model = embedding_size
        self.weight = torch.nn.parameter.Parameter(
            torch.empty(vocab_size, embedding_size, dtype=dtype)
        )
    def forward(self, ids: torch.Tensor):
        """
        Args:
            ids (:obj:`torch.Tensor` of shape ``(batch_size, seq_len)``): Indices of input sequence tokens.
        Return:
            :obj:`torch.Tensor` of shape ``(batch_size, seq_len, embedding_size)``: The embedding output.
        """  # noqa: E501

        embeds = F.embedding(ids, self.weight) / math.sqrt(self.dim_model)
        return embeds

    def projection(self, x: torch.Tensor):
        """
        Projection based on embedding's weight. For example, embedding map vocab_size to embed_size, than projection map embed_size back to vocab_size.
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_model)``): Input of projection
        Returns:
            :obj:`torch.Tensor` of shape ``(batch, seq_len, vocab_output_size)``: The projection output.
        """  # noqa: E501
        logits = F.linear(x / math.sqrt(self.dim_model), self.weight)
        return logits
        
 ç±»çš„åŒºåˆ«
1. Embedding ç±»
åŠŸèƒ½: åŸºæœ¬çš„åµŒå…¥å±‚ï¼Œç”¨äºå°†è¯æ±‡è¡¨ä¸­çš„ç´¢å¼•æ˜ å°„åˆ°åµŒå…¥å‘é‡ã€‚
æ–¹æ³•:
forward: æ¥å—ä¸€ä¸ªè¾“å…¥å¼ é‡ idsï¼Œè¿”å›åµŒå…¥è¾“å‡ºã€‚
projection: å°†åµŒå…¥å‘é‡æ˜ å°„å›è¯æ±‡è¡¨çš„å¤§å°ã€‚
2. EmbeddingExt ç±»
åŠŸèƒ½: æ‰©å±•çš„åµŒå…¥å±‚ï¼Œé™¤äº†åŸºæœ¬çš„åµŒå…¥ï¼Œè¿˜ä½¿ç”¨æ—‹è½¬åµŒå…¥ï¼ˆRotaryEmbeddingï¼‰æ¥å¢å¼ºè¡¨ç¤ºèƒ½åŠ›ã€‚
æ–¹æ³•:
forward: æ¥å—ä¸¤ä¸ªè¾“å…¥å¼ é‡ ids å’Œ ids_subï¼Œè¿”å›å¢å¼ºçš„åµŒå…¥è¾“å‡ºã€‚
projection: ç±»ä¼¼äº Embeddingï¼Œä½†å¯ä»¥ä½¿ç”¨é¢å¤–çš„è¯æ±‡è¡¨ï¼ˆext_tableï¼‰è¿›è¡ŒæŠ•å½±ã€‚
projection æ–¹æ³•çš„ä½œç”¨
ç›®çš„: å°†åµŒå…¥è¾“å‡ºæ˜ å°„å›è¯æ±‡è¡¨å¤§å°çš„ logits å½¢å¼ï¼Œç”¨äºåç»­çš„åˆ†ç±»æˆ–é¢„æµ‹ä»»åŠ¡ã€‚
å·¥ä½œåŸç†: é€šè¿‡çº¿æ€§å˜æ¢ï¼Œå°†åµŒå…¥å‘é‡å‹ç¼©åˆ°è¯æ±‡è¡¨çš„å¤§å°ï¼ˆå³é¢„æµ‹æ¯ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒï¼‰ã€‚åœ¨ EmbeddingExt ä¸­ï¼Œè¿˜å¯ä»¥ç»“åˆå¤–éƒ¨è¯æ±‡è¡¨çš„ logitsã€‚

import torch
# å‚æ•°è®¾ç½®
vocab_size = 10000
embedding_size = 512
# å®ä¾‹åŒ– Embedding ç±»
embedding = Embedding(vocab_size=vocab_size, embedding_size=embedding_size)
# åˆ›å»ºè¾“å…¥å¼ é‡ (batch_size=2, seq_len=5)
input_ids = torch.randint(0, vocab_size, (2, 5))
# å‰å‘ä¼ æ’­
output = embedding(input_ids)
print(output.shape)  # è¾“å‡ºå½¢çŠ¶: (2, 5, 512)

# å‚æ•°è®¾ç½®
distance_scale = 16

# å®ä¾‹åŒ– EmbeddingExt ç±»
embedding_ext = EmbeddingExt(vocab_size=vocab_size, embedding_size=embedding_size, distance_scale=distance_scale)
# åˆ›å»ºè¾“å…¥å¼ é‡ (batch_size=2, seq_len=5)
input_ids = torch.randint(0, vocab_size, (2, 5))
subscript_ids = torch.randint(0, 5, (2,))  # å­è„šæœ¬å¼ é‡
# å‰å‘ä¼ æ’­
output_ext = embedding_ext(input_ids, subscript_ids)
print(output_ext.shape)  # è¾“å‡ºå½¢çŠ¶: (2, 5, 512)

ä½¿ç”¨ projection æ–¹æ³•
# æŠ•å½±æ“ä½œ
logits = embedding.projection(output)
print(logits.shape)  # è¾“å‡ºå½¢çŠ¶: (2, 5, 10000)

# å¯¹äº EmbeddingExt
ext_table = torch.empty(vocab_size, embedding_size)  # å‡è®¾çš„å¤–éƒ¨è¯æ±‡è¡¨
logits_ext = embedding_ext.projection(output_ext, ext_table)
print(logits_ext.shape)  # è¾“å‡ºå½¢çŠ¶: (2, 5, 10000 + ext_table_size)
```



## 10 .attentionæ¨¡å—

```
import torch
import math
from .linear import Linear


class Attention(torch.nn.Module):
    def __init__(
        self,
        dim_model: int,
        num_heads: int,
        dim_head: int,
        dtype: torch.dtype = torch.half,
        dropout_p: Optional[float] = None,
    ) -> None:

        super().__init__()
        self.dim_model = dim_model
        self.num_heads = num_heads
        self.dim_head = dim_head

        self.project_q = Linear(self.dim_model, self.num_heads * self.dim_head, dtype=dtype)
        self.project_k = Linear(self.dim_model, self.num_heads * self.dim_head, dtype=dtype)
        self.project_v = Linear(self.dim_model, self.num_heads * self.dim_head, dtype=dtype)
        self.attention_out = Linear(self.num_heads * self.dim_head, self.dim_model, dtype=dtype)
        self.softmax = torch.nn.Softmax(dim=-1)

        if dropout_p is not None:
            self.dropout = torch.nn.Dropout(p=dropout_p)
        else:
            self.dropout = None

    def forward(
        self,
        hidden_q: torch.Tensor,
        hidden_kv: torch.Tensor,
        attention_mask: torch.BoolTensor,
        position_bias: torch.Tensor,
        use_cache: bool = False,
        past_kv: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ):
        """
        Args:
            hidden_q (:obj:`torch.Tensor` of shape ``(batch, len_q, dim_model)``): Indices of input sequence tokens. It will be embedded by model's internal embedding lookup matrix.
            hidden_kv (:obj:`torch.Tensor` of shape ``(batch, len_k, dim_model)``): Length of input sequence before padding.
            attention_mask (:obj:`torch.Tensor` of shape ``(batch, len_q, len_k)``): Used to avoid performing attention on padding token indices.
            position_bias(:obj:`torch.Tensor` of shape ``(num_heads, len_q, len_k)`` or ``(1, num_heads, len_k, len_q)``): Provide positional information about tensor `key_value` and `query`.
        Return:
            out (:obj:`torch.Tensor` of shape ``(batch, len_q, dim_model)``): The attention output.
        """  # noqa: E501

        batch_size = hidden_q.size(0)
        len_q = hidden_q.size(1)
        len_k = hidden_kv.size(1)

        h_q = self.project_q(hidden_q)
        h_k = self.project_k(hidden_kv)
        h_v = self.project_v(hidden_kv)

        h_q = h_q.view(batch_size, len_q, self.num_heads, self.dim_head).permute(0, 2, 1, 3)
        h_k = h_k.view(batch_size, len_k, self.num_heads, self.dim_head).permute(0, 2, 1, 3)
        h_v = h_v.view(batch_size, len_k, self.num_heads, self.dim_head).permute(0, 2, 1, 3)

        if past_kv is not None:
            h_k = torch.cat([past_kv[0], h_k], dim=-2)
            h_v = torch.cat([past_kv[1], h_v], dim=-2)
            len_k = h_k.size(-2)

        # (b, n_h, len_q, d_h) @ (b, n_h, d_h, len_k) -> (b, n_h, len_q, len_k)
        score = torch.matmul(h_q, h_k.transpose(-1, -2)) / math.sqrt(self.dim_head)
        score = score + position_bias
        score = torch.masked_fill(
            score,
            attention_mask.view(batch_size, 1, len_q, len_k) == False,
            torch.scalar_tensor(float("-inf"), device=score.device, dtype=score.dtype),
        )
        score = self.softmax(score)
        score = torch.masked_fill(
            score,
            attention_mask.view(batch_size, 1, len_q, len_k) == False,
            torch.scalar_tensor(0, device=score.device, dtype=score.dtype),
        )
        if self.dropout is not None:
            score = self.dropout(score)
        # (b, n_h, len_q, len_k) @ (b, n_h, len_k, d_h) -> (b, n_h, len_q, d_h)
        score = torch.matmul(score, h_v)
        score = score.view(batch_size, self.num_heads, len_q, self.dim_head).permute(0, 2, 1, 3)
        score = score.contiguous().view(batch_size, len_q, self.num_heads * self.dim_head)
        score = self.attention_out(score)
        if use_cache:
            return score, (h_k, h_v)
        else:
            return score
```

## 11. kwargsç”¨æ³• 

`kwargs.pop` æ˜¯ Python å­—å…¸çš„ä¸€ç§æ–¹æ³•ï¼Œç”¨äºä»å­—å…¸ä¸­ç§»é™¤æŒ‡å®šçš„é”®ï¼Œå¹¶è¿”å›å¯¹åº”çš„å€¼ã€‚åœ¨ `EncoderConfig` ç±»çš„ `__init__` æ–¹æ³•ä¸­ï¼Œå®ƒçš„ä½œç”¨æ˜¯æå–å¹¶ç§»é™¤ä¼ å…¥çš„å…³é”®å­—å‚æ•°ã€‚

1. **kwargs**:

   - `**kwargs` æ˜¯ä¸€ç§åœ¨å‡½æ•°å®šä¹‰ä¸­æ¥å—å¯å˜æ•°é‡å…³é”®å­—å‚æ•°çš„æ–¹å¼ã€‚å®ƒå…è®¸ä½ ä¼ å…¥ä»»æ„æ•°é‡çš„å‘½åå‚æ•°ï¼Œè¿™äº›å‚æ•°ä¼šè¢«å­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ã€‚

2. **pop æ–¹æ³•**:

   - `kwargs.pop("key", default)` ä¼šä» `kwargs` å­—å…¸ä¸­ç§»é™¤é”®ä¸º `"key"` çš„é¡¹ï¼Œå¹¶è¿”å›å…¶å€¼ã€‚å¦‚æœè¯¥é”®ä¸å­˜åœ¨ï¼Œåˆ™è¿”å› `default` å€¼ï¼ˆå¦‚æœæä¾›äº†çš„è¯ï¼‰ã€‚

   self.encoder_embed_dim = kwargs.pop("encoder_embed_dim", 768)

   è¿™è¡Œä»£ç ä» kwargs ä¸­æŸ¥æ‰¾ "encoder_embed_dim" é”®ï¼š
   å¦‚æœæ‰¾åˆ°ï¼Œå®ƒå°†å€¼èµ‹ç»™ self.encoder_embed_dimï¼Œå¹¶ä» kwargs ä¸­ç§»é™¤è¯¥é”®ã€‚
   å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå®ƒä¼šä½¿ç”¨é»˜è®¤å€¼ 768ã€‚

   çµæ´»æ€§: å…è®¸ç”¨æˆ·åªä¼ å…¥éœ€è¦çš„å‚æ•°ï¼Œæœªä¼ å…¥çš„å‚æ•°ä½¿ç”¨é»˜è®¤å€¼ã€‚
   æ¸…æ™°æ€§: ä» kwargs ä¸­æå–å‚æ•°åï¼Œå¯é¿å…åç»­ä»£ç ä¸­å¤„ç†æœªä½¿ç”¨çš„å‚æ•°ã€‚

```
class EncoderConfig(object):
    def __init__(self, **kwargs):
        self.encoder_embed_dim = kwargs.pop("encoder_embed_dim", 768)
        self.encoder_attention_heads = kwargs.pop("encoder_attention_heads", 12)
        self.encoder_ffn_embed_dim = kwargs.pop("encoder_ffn_embed_dim", 3072)
        self.encoder_layers = kwargs.pop("encoder_layers", 12)
        self.encoder_normalize_before = kwargs.pop("encoder_normalize_before", True)
        self.normalize_output = kwargs.pop("normalize_output", True)
        self.activation_fn = kwargs.pop("activation_fn", "gelu")
        self.dropout = kwargs.pop("dropout", 0.0)
        self.drop_path_rate = kwargs.pop("drop_path_rate", 0.0)
        self.attention_dropout = kwargs.pop("attention_dropout", 0.0)
        self.activation_dropout = kwargs.pop("activation_dropout", 0.0)
       
```



### permuteç”¨æ³• 

```
x =torch.randn(3,10,24)
print(x.permute(0,2,1).shape)
torch.Size([3, 24, 10])
```

### torch.masked_fillç”¨æ³•

torch.masked_fill æ˜¯ PyTorch ä¸­ç”¨äºæ ¹æ®å¸ƒå°”æ©ç å¡«å……å¼ é‡çš„å‡½æ•°ã€‚å®ƒå¯ä»¥ç”¨æŒ‡å®šçš„å€¼æ›¿æ¢å¼ é‡ä¸­æ»¡è¶³ç»™å®šæ¡ä»¶çš„å…ƒç´ ã€‚

torch.masked_fill(input, mask, value)

å‚æ•°

- **input**: åŸå§‹å¼ é‡ã€‚
- **mask**: å¸ƒå°”å¼ é‡ï¼ŒæŒ‡ç¤ºå“ªäº›ä½ç½®çš„å…ƒç´ å°†è¢«æ›¿æ¢ã€‚
- **value**: ç”¨äºå¡«å……çš„å€¼ã€‚
- 

```
import torch

# åˆ›å»ºä¸€ä¸ªå¼ é‡
input_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])
# åˆ›å»ºå¸ƒå°”æ©ç 
mask = input_tensor > 3
# ä½¿ç”¨ masked_fill æ›¿æ¢æ»¡è¶³æ¡ä»¶çš„å…ƒç´ 
result = input_tensor.masked_fill(mask, -1)
print(result)
# è¾“å‡º:
# tensor([[ 1,  2,  3],
#         [-1, -1, -1]])

# åˆ›å»ºä¸€ä¸ªæ›´å¤§çš„å¼ é‡
input_tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# åˆ›å»ºå¸ƒå°”æ©ç 
mask_2d = input_tensor_2d % 2 == 0  # æ‰¾åˆ°æ‰€æœ‰å¶æ•°
# ä½¿ç”¨ masked_fill æ›¿æ¢æ»¡è¶³æ¡ä»¶çš„å…ƒç´ 
result_2d = input_tensor_2d.masked_fill(mask_2d, 0)

print(result_2d)
# è¾“å‡º:
# tensor([[1, 0, 3],
#         [0, 5, 0],
#         [7, 0, 9]])
```

```
mask = torch.tril(torch.ones((seq_length,seq_length)))
mask =mask.bool()
attention_scores =torch.randn(5,5)  å‡è®¾æœ‰ä¸€ä¸ªæ³¨æ„åŠ›åˆ†æ•°å¼ é‡

attention_scores = attention_scores.masked_fill(~mask,float('-inf')) # åº”ç”¨æ©ç 
attention_weights =torch.softmax(attention_scores,dim=-1)  # è®¡ç®—æ³¨æ„åŠ›æƒé‡
print(attention_weights)
```





### torch.gatherç”¨æ³• 

torch.gather æ˜¯ PyTorch ä¸­ç”¨äºä»ä¸€ä¸ªå¼ é‡ä¸­æ ¹æ®ç»™å®šçš„ç´¢å¼•é€‰æ‹©å…ƒç´ çš„å‡½æ•°ã€‚å®ƒå…è®¸ä½ åœ¨æŒ‡å®šçš„ç»´åº¦ä¸Šæ”¶é›†æ•°æ®ã€‚

torch.gather(input, dim, index)

å‚æ•°
input: è¾“å…¥å¼ é‡ã€‚
dim: æŒ‡å®šè¦ä»ä¸­æ”¶é›†æ•°æ®çš„ç»´åº¦ã€‚
index: ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«è¦æ”¶é›†çš„å…ƒç´ çš„ç´¢å¼•ã€‚

```
import torch

# åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å¼ é‡
input_tensor = torch.tensor([[10, 20, 30], [40, 50, 60]])

# æŒ‡å®šç´¢å¼•
index_tensor = torch.tensor([[0, 2], [1, 0]])

# åœ¨ç»´åº¦ 1 ä¸Šæ”¶é›†æ•°æ®
result = torch.gather(input_tensor, dim=1, index=index_tensor)

print(result)
# è¾“å‡º:
# tensor([[10, 30],
#         [50, 40]])

ä½¿ç”¨äºæ›´é«˜ç»´åº¦çš„å¼ é‡
# åˆ›å»ºä¸€ä¸ª 3D å¼ é‡
input_tensor_3d = torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])

# æŒ‡å®šç´¢å¼•
index_tensor_3d = torch.tensor([[[0, 1]], [[1, 0]]])

# åœ¨ç»´åº¦ 2 ä¸Šæ”¶é›†æ•°æ®
result_3d = torch.gather(input_tensor_3d, dim=2, index=index_tensor_3d)

print(result_3d)
# è¾“å‡º:
# tensor([[[1, 4]],
#         [[6, 5]]])
```

torch.gather æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å·¥å…·ï¼Œé€‚ç”¨äºä»å¼ é‡ä¸­æ ¹æ®ç´¢å¼•é€‰æ‹©å…ƒç´ ï¼Œå¹¿æ³›åº”ç”¨äºæ¨¡å‹çš„è¾“å‡ºé€‰æ‹©ã€åºåˆ—å¤„ç†ç­‰åœºæ™¯ã€‚



### torch.whereç”¨æ³• 

torch.where æ˜¯ PyTorch ä¸­ç”¨äºæ ¹æ®æ¡ä»¶é€‰æ‹©å¼ é‡å…ƒç´ çš„å‡½æ•°ã€‚å®ƒå¯ä»¥ç”¨äºåˆ›å»ºä¸€ä¸ªæ–°å¼ é‡ï¼Œæˆ–åœ¨ç»™å®šçš„æ¡ä»¶ä¸‹è¿”å›ç¬¦åˆæ¡ä»¶çš„å…ƒç´ çš„ç´¢å¼•ã€‚

torch.where(condition, x, y)

- **condition**: ä¸€ä¸ªå¸ƒå°”å¼ é‡ï¼ŒæŒ‡ç¤ºåœ¨ä½•å¤„é€‰æ‹© `x` æˆ– `y` çš„å…ƒç´ ã€‚
- **x**: å½“ `condition` ä¸º True æ—¶é€‰æ‹©çš„å¼ é‡ã€‚
- **y**: å½“ `condition` ä¸º False æ—¶é€‰æ‹©çš„å¼ é‡ã€‚

```
import torch

# åˆ›å»ºä¸¤ä¸ªå¼ é‡
x = torch.tensor([1, 2, 3, 4])
y = torch.tensor([10, 20, 30, 40])

# åˆ›å»ºå¸ƒå°”æ¡ä»¶
condition = x > 2

# ä½¿ç”¨ torch.where åˆ›å»ºæ–°å¼ é‡
result = torch.where(condition, x, y)

print(result)  # è¾“å‡º: tensor([10, 20, 3, 4])
åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œresult å¼ é‡ä¸­ï¼Œå½“ x çš„å…ƒç´ å¤§äº 2 æ—¶é€‰å– x çš„å…ƒç´ ï¼Œå¦åˆ™é€‰å– y çš„å…ƒç´ ã€‚

import torch

# åˆ›å»ºä¸€ä¸ªå¼ é‡
a = torch.tensor([[1, 2, 3], [4, 5, 6]])
# æ‰¾åˆ°å…ƒç´ å¤§äº 3 çš„ç´¢å¼•
indices = torch.where(a > 3)

print(indices)  # è¾“å‡º: (tensor([1, 1, 1]), tensor([0, 1, 2]))
torch.where(a > 3) è¿”å›ä¸¤ä¸ªå¼ é‡ï¼Œç¬¬ä¸€ä¸ªæ˜¯è¡Œç´¢å¼•ï¼Œç¬¬äºŒä¸ªæ˜¯åˆ—ç´¢å¼•ã€‚
è¿™æ ·å¯ä»¥ç”¨äºåç»­æ“ä½œï¼Œä¾‹å¦‚è®¿é—®è¿™äº›å…ƒç´ ã€‚
```



### np.fullç”¨æ³•

np.full æ˜¯ NumPy ä¸­ç”¨äºåˆ›å»ºä¸€ä¸ªæŒ‡å®šå½¢çŠ¶å’Œå¡«å……å€¼çš„æ•°ç»„çš„å‡½æ•°ã€‚ä»¥ä¸‹æ˜¯ np.full çš„åŸºæœ¬ç”¨æ³•åŠç¤ºä¾‹ï¼š

åŸºæœ¬è¯­æ³•
numpy.full(shape, fill_value, dtype=None)
shape: æ•°ç»„çš„å½¢çŠ¶ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªæ•´æ•°æˆ–å…ƒç»„ã€‚
fill_value: ç”¨äºå¡«å……æ•°ç»„çš„å€¼ã€‚
dtype: å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šæ•°ç»„çš„æ•°æ®ç±»å‹ã€‚

```
batch_size = 4
max_length = 10

# åˆ›å»ºä¸€ä¸ª 4x10 çš„æ•°ç»„ï¼Œæ‰€æœ‰å…ƒç´ ä¸º -100
tgt = np.full((batch_size, max_length), -100, dtype=np.int32)
print(tgt)
# è¾“å‡º:
# [[-100 -100 -100 -100 -100 -100 -100 -100 -100 -100]
#  [-100 -100 -100 -100 -100 -100 -100 -100 -100 -100]
#  [-100 -100 -100 -100 -100 -100 -100 -100 -100 -100]
#  [-100 -100 -100 -100 -100 -100 -100 -100 -100 -100]]
```

[VisualBERT](https://arxiv.org/abs/1908.03557)

## VisualBERT



## 11.Bottleneckçš„ç”¨æ³•

```python
Bottleneck

class Bottleneck(nn.Module):
    expansion = 4
    def __init__(self, inplanes, planes, stride=1):
        super().__init__()
        # all conv layers have stride 1.an avgpool is performed after the second convolution when stride > 1
        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.relu2 = nn.ReLU(inplace=True)
        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()
        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu3 = nn.ReLU(inplace=True)

        self.downsample = None
        self.stride = stride

        if stride > 1 or inplanes != planes * Bottleneck.expansion:
            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1
            self.downsample = nn.Sequential(OrderedDict([
                ("-1", nn.AvgPool2d(stride)),
                ("0", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),
                ("1", nn.BatchNorm2d(planes * self.expansion))
            ]))

    def forward(self, x: torch.Tensor):
        identity = x
        
        out = self.relu1(self.bn1(self.conv1(x)))
        print('out1',out.shape)  # out1 torch.Size([1, 32, 28, 28])
        out = self.relu2(self.bn2(self.conv2(out)))
        print('out2', out.shape) # out2 torch.Size([1, 32, 28, 28])
        out = self.avgpool(out)
        print('out3', out.shape) # out3 torch.Size([1, 32, 14, 14])
        out = self.bn3(self.conv3(out)) 
        print('out4', out.shape) #  out4 torch.Size([1, 128, 14, 14])
        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu3(out)
        return out
bottleneck = Bottleneck(inplanes=64, planes=32, stride=2)
# åˆ›å»ºä¸€ä¸ªè¾“å…¥å¼ é‡ï¼Œå‡è®¾æ‰¹æ¬¡å¤§å°ä¸º1ï¼Œé€šé“æ•°ä¸º64ï¼Œç‰¹å¾å›¾å¤§å°ä¸º28x28
input_tensor = torch.randn(1, 64, 28, 28)
# è°ƒç”¨Bottleneckæ¨¡å—çš„forwardæ–¹æ³•
output_tensor = bottleneck(input_tensor)
# è¾“å‡ºå¼ é‡çš„å½¢çŠ¶åº”è¯¥ä¸è¾“å…¥å¼ é‡ç›¸åŒ
print("Output Tensor Shape:", output_tensor.shape)

ç±»æˆå‘˜å’Œæ–¹æ³•è§£é‡Šï¼š
expansionï¼šç±»å±æ€§ï¼Œè¡¨ç¤ºè¾“å‡ºé€šé“æ•°æ˜¯è¾“å…¥é€šé“æ•°çš„å€æ•°ã€‚
__init__ï¼šæ„é€ å‡½æ•°ï¼Œåˆå§‹åŒ–æ¨¡å—çš„å±‚ã€‚
inplanesï¼šè¾“å…¥é€šé“æ•°ã€‚
planesï¼šå·ç§¯å±‚çš„é€šé“æ•°ã€‚
strideï¼šå·ç§¯çš„æ­¥é•¿ï¼Œé»˜è®¤ä¸º1ã€‚
conv1, bn1, relu1ï¼šç¬¬ä¸€ä¸ªå·ç§¯å±‚åŠå…¶æ‰¹é‡å½’ä¸€åŒ–ï¼ˆBatchNormï¼‰å’ŒReLUæ¿€æ´»å‡½æ•°ã€‚
conv2, bn2, relu2ï¼šç¬¬äºŒä¸ªå·ç§¯å±‚åŠå…¶æ‰¹é‡å½’ä¸€åŒ–å’ŒReLUæ¿€æ´»å‡½æ•°ã€‚
avgpoolï¼šå¹³å‡æ± åŒ–å±‚ï¼Œå½“ stride > 1 æ—¶ä½¿ç”¨ï¼Œä»¥å‡å°‘ç‰¹å¾å›¾çš„ç©ºé—´ç»´åº¦ã€‚
conv3, bn3, relu3ï¼šç¬¬ä¸‰ä¸ªå·ç§¯å±‚åŠå…¶æ‰¹é‡å½’ä¸€åŒ–å’ŒReLUæ¿€æ´»å‡½æ•°ï¼Œè¾“å‡ºé€šé“æ•°ä¸º planes * expansionã€‚
downsampleï¼šå¯é€‰çš„ä¸‹é‡‡æ ·å±‚ï¼Œå½“è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°ä¸åŒ¹é…æˆ–æ­¥é•¿å¤§äº1æ—¶ä½¿ç”¨ã€‚
forwardï¼šå‰å‘ä¼ æ’­å‡½æ•°ï¼Œå®šä¹‰äº†æ•°æ®å¦‚ä½•é€šè¿‡ç½‘ç»œæµåŠ¨ã€‚
å…¥å‚å’Œå‡ºå‚ï¼š
å…¥å‚ï¼šx æ˜¯ä¸€ä¸ª torch.Tensor å¯¹è±¡ï¼Œè¡¨ç¤ºè¾“å…¥çš„å¼ é‡ï¼Œå…¶å½¢çŠ¶é€šå¸¸æ˜¯ (N, C, H, W)ï¼Œå…¶ä¸­ N æ˜¯æ‰¹æ¬¡å¤§å°ï¼ŒC æ˜¯é€šé“æ•°ï¼ŒH å’Œ W åˆ†åˆ«æ˜¯ç‰¹å¾å›¾çš„é«˜åº¦å’Œå®½åº¦ã€‚
å‡ºå‚ï¼šå‡½æ•°è¿”å›ä¸€ä¸ª torch.Tensor å¯¹è±¡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ x ç›¸åŒï¼Œè¡¨ç¤ºç»è¿‡æ®‹å·®å—å¤„ç†åçš„è¾“å‡ºã€‚
```

## torch.matmulç”¨æ³•

torch.matmul æ˜¯ PyTorch ä¸­ç”¨äºæ‰§è¡ŒçŸ©é˜µä¹˜æ³•çš„å‡½æ•°ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼ï¼ŒåŒ…æ‹¬å‘é‡ã€çŸ©é˜µå’Œé«˜ç»´å¼ é‡çš„ä¹˜æ³•ã€‚

torch.matmul(input, other)

#### 1. å‘é‡ä¸å‘é‡çš„ä¹˜æ³•

```
a =torch.tensor([1,2,3])
b =torch.tensor([2,3,4])
c = torch.matmul(a,b)
print(c)  tensor(20)
```

#### 2. çŸ©é˜µä¸çŸ©é˜µçš„ä¹˜æ³•

```
a =torch.tensor([[1,2],
                 [3,4]])
b =torch.tensor([[1,2],
                 [3,4]])
c = torch.matmul(a,b)
print(c)

tensor([[ 7, 10],
        [15, 22]])
```

#### 3. çŸ©é˜µä¸å‘é‡çš„ä¹˜æ³•

```
# åˆ›å»ºä¸€ä¸ªçŸ©é˜µå’Œä¸€ä¸ªå‘é‡
C = torch.tensor([[1, 2, 3], [4, 5, 6]])
d = torch.tensor([7, 8, 9])

# è®¡ç®—çŸ©é˜µä¸å‘é‡çš„ä¹˜æ³•
result = torch.matmul(C, d)
print(result)  # è¾“å‡º: tensor([  70,  173])
```

#### 4. é«˜ç»´å¼ é‡çš„ä¹˜æ³•

å¯¹äºé«˜ç»´å¼ é‡ï¼Œ`torch.matmul` ä¼šè¿›è¡Œå¹¿æ’­å’Œé€‚å½“çš„ç»´åº¦åŒ¹é…ã€‚

```
# åˆ›å»ºä¸¤ä¸ªä¸‰ç»´å¼ é‡
x = torch.rand(2, 3, 4)  # å½¢çŠ¶ (2, 3, 4)
y = torch.rand(2, 4, 5)  # å½¢çŠ¶ (2, 4, 5)

# è®¡ç®—é«˜ç»´å¼ é‡çš„ä¹˜æ³•
result = torch.matmul(x, y)  # ç»“æœå½¢çŠ¶ä¸º (2, 3, 5)
print(result.shape)  # è¾“å‡º: torch.Size([2, 3, 5])
```



## 12 .SelfAttentionBlockæ¨¡å— 

```
SelfAttentionBlock
```

```
from typing import Optional, Tuple
import torch
from .layernorm import LayerNorm
from .attention import Attention
from .feedforward import FeedForward


class SelfAttentionBlock(torch.nn.Module):
    """The whole cross-attention block. A sequence of operation. Consists of layernorm, self-attention and residual connection.

    Args:
        dim_model (int): main dimension of modules in transformer blocks.
        num_heads (int): num_heads used in :py:class:`model_center.layer.Attention`.
        dim_head (int): dim_head used in :py:class:`model_center.layer.Attention`.
        dtype (optional): Defaults to torch.half.
        eps (float, optional): eps used in :py:class:`model_center.layer.LayerNorm`. Defaults to 1e-5.
        dropout_p (float, optional): Defaults to 0.
    """  # noqa: E501

    def __init__(
        self,
        dim_model: int,
        num_heads: int,
        dim_head: int,
        dtype=torch.half,
        eps: float = 1e-6,
        dropout_p: Optional[float] = None,
    ):

        super().__init__()

        self.layernorm_before_attention = LayerNorm(
            dim_model,
            dtype=dtype,
            eps=eps,
        )

        self.self_attention = Attention(
            dim_model=dim_model,
            num_heads=num_heads,
            dim_head=dim_head,
            dtype=dtype,
            dropout_p=dropout_p,
        )

        if dropout_p:
            self.dropout = torch.nn.Dropout(dropout_p)
        else:
            self.dropout = None

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: torch.Tensor,
        position_bias: Optional[torch.Tensor] = None,
        use_cache: bool = False,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ):
        """
        Args:
            hidden_states (:obj:`torch.Tensor` of shape ``(batch, seq_self, dim_model)``): Input of self-attention block. It can be the embedding of a batch of sequences.
            attention_mask (:obj:`torch.Tensor` of shape ``(batch, seq_self, seq_self)``): Avoid invalid areas to participate in the calculation.
            position_bias (:obj:`torch.Tensor` of shape ``(num_heads, seq_self, seq_self)``): Provide positional information to self-attention block.

        Return:
            :obj:`torch.Tensor` of shape ``(batch, seq_self, dim_model)``: The output of attention block.

        """  # noqa: E501
        x = self.layernorm_before_attention(hidden_states)
        x = self.self_attention(x, x, attention_mask, position_bias, use_cache, past_key_value)
        if use_cache:
            x, current_key_value = x
        else:
            current_key_value = None

        if self.dropout is not None:
            x = self.dropout(x)
        hidden_states = (hidden_states + x) / 1.05

        if use_cache:
            return hidden_states, current_key_value
        else:
            return hidden_states
```

SelfAttentionBlock æ˜¯ä¸€ä¸ªå®ç°è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ PyTorch æ¨¡å—ï¼Œé€šå¸¸ç”¨äº Transformer æ¨¡å‹ä¸­ã€‚å®ƒåŒ…å«äº†å±‚å½’ä¸€åŒ–ã€æ³¨æ„åŠ›è®¡ç®—å’Œæ®‹å·®è¿æ¥ã€‚ä¸‹é¢æ˜¯å¦‚ä½•è°ƒç”¨å’Œä½¿ç”¨è¿™ä¸ªæ¨¡å—çš„è¯´æ˜ã€‚

æ¨¡å—çš„ä½œç”¨
å±‚å½’ä¸€åŒ–: é€šè¿‡ LayerNorm å¯¹è¾“å…¥çš„éšè—çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¸®åŠ©æ¨¡å‹æ›´å¿«æ”¶æ•›ã€‚
è‡ªæ³¨æ„åŠ›è®¡ç®—: é€šè¿‡ Attention æ¨¡å—è®¡ç®—è¾“å…¥åºåˆ—ä¹‹é—´çš„æ³¨æ„åŠ›æƒé‡ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…³æ³¨ä¸åŒä½ç½®çš„ä¿¡æ¯ã€‚
æ®‹å·®è¿æ¥: å°†è¾“å…¥çš„éšè—çŠ¶æ€ä¸æ³¨æ„åŠ›è¾“å‡ºç›¸åŠ ï¼Œä¿ƒè¿›ä¿¡æ¯æµåŠ¨å’Œæ¢¯åº¦ä¼ é€’ã€‚
å¯é€‰çš„ dropout: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
å¦‚ä½•è°ƒç”¨
1. åˆå§‹åŒ–æ¨¡å—
é¦–å…ˆï¼Œéœ€è¦å®ä¾‹åŒ– SelfAttentionBlock ç±»ã€‚ä½ éœ€è¦æä¾›ä»¥ä¸‹å‚æ•°ï¼š

dim_model: æ¨¡å‹çš„ä¸»ç»´åº¦ï¼ˆé€šå¸¸æ˜¯åµŒå…¥ç»´åº¦ï¼‰ã€‚
num_heads: æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚
dim_head: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ã€‚
dtype: æ•°æ®ç±»å‹ï¼ˆé»˜è®¤ä¸º torch.halfï¼‰ã€‚
eps: ç”¨äºå±‚å½’ä¸€åŒ–çš„ epsilon å€¼ï¼ˆé»˜è®¤ä¸º 1e-6ï¼‰ã€‚
dropout_p: dropout æ¦‚ç‡ï¼ˆé»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºä¸ä½¿ç”¨ dropoutï¼‰ã€‚

```python
import torch

# å‚æ•°è®¾ç½®
dim_model = 512
num_heads = 8
dim_head = 64
dropout_p = 0.1

# å®ä¾‹åŒ– SelfAttentionBlock
self_attention_block = SelfAttentionBlock(
    dim_model=dim_model,
    num_heads=num_heads,
    dim_head=dim_head,
    dropout_p=dropout_p
)
2. å‡†å¤‡è¾“å…¥æ•°æ®
è¾“å…¥æ•°æ®åŒ…æ‹¬ï¼š

hidden_states: å½¢çŠ¶ä¸º (batch, seq_self, dim_model) çš„å¼ é‡ï¼Œè¡¨ç¤ºä¸€æ‰¹åºåˆ—çš„åµŒå…¥ã€‚
attention_mask: å½¢çŠ¶ä¸º (batch, seq_self, seq_self) çš„å¼ é‡ï¼Œç”¨äºé¿å…æ— æ•ˆåŒºåŸŸå‚ä¸è®¡ç®—ã€‚
position_bias: å¯é€‰ï¼Œå½¢çŠ¶ä¸º (num_heads, seq_self, seq_self) çš„å¼ é‡ï¼Œç”¨äºæä¾›ä½ç½®ä¿¡æ¯ã€‚
# åˆ›å»ºç¤ºä¾‹è¾“å…¥
batch_size = 2
seq_length = 10

hidden_states = torch.rand(batch_size, seq_length, dim_model)
attention_mask = torch.ones(batch_size, seq_length, seq_length)  # å…¨éƒ¨ä¸º1è¡¨ç¤ºæ²¡æœ‰æ©ç 
position_bias = torch.rand(num_heads, seq_length, seq_length)  # å¯é€‰
# è°ƒç”¨æ¨¡å—
output = self_attention_block(hidden_states, attention_mask, position_bias)
```





## 13 .FFNBlockæ¨¡å—

```

class FFNBlock(torch.nn.Module):
    """The whole feed-forward block. A sequence of operation. Consists of layernorm, feed-forward and residual connection.

    Args:
        dim_model (int): main dimension of modules in transformer blocks.
        dim_ff (int): dim_ff used in :py:class:`model_center.layer.FeedForward`.
        dtype (optional): Defaults to torch.half.
        eps (float, optional): eps used in :py:class:`model_center.layer.LayerNorm`. Defaults to 1e-5.
        dropout_p (float, optional): Defaults to 0.
    """  # noqa: E501

    def __init__(
        self,
        dim_model: int,
        dim_ff: int,
        dtype=torch.half,
        eps: float = 1e-6,
        dropout_p: Optional[float] = 0,
    ):
        super().__init__()

        self.layernorm_before_ffn = LayerNorm(
            dim_model,
            dtype=dtype,
            eps=eps,
        )
        self.ffn = FeedForward(
            dim_model,
            dim_ff,
            dtype=dtype,
            dropout_p=dropout_p,
        )

        if dropout_p:
            self.dropout = torch.nn.Dropout(dropout_p)
        else:
            self.dropout = None

    def forward(
        self,
        hidden_states: torch.Tensor,
    ):
        """
        Args:
            hidden_states (:obj:`torch.Tensor` of shape ``(batch, seq_self, dim_model)``): Hidden states before feed forward layer.

        Return:
            :obj:`torch.Tensor` of shape ``(batch, seq_self, dim_model)``: The output of feed-forward block
        """  # noqa: E501
        x = self.layernorm_before_ffn(hidden_states)
        x = self.ffn(x)
        if self.dropout is not None:
            x = self.dropout(x)
        hidden_states = (hidden_states + x) / 1.05
        return hidden_states
```

FFNBlock æ˜¯ä¸€ä¸ªå®ç°å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰æ¨¡å—çš„ PyTorch ç±»ï¼Œé€šå¸¸ç”¨äº Transformer æ¨¡å‹çš„ç»“æ„ä¸­ã€‚å®ƒçš„ä¸»è¦åŠŸèƒ½æ˜¯å¯¹è¾“å…¥çš„éšè—çŠ¶æ€è¿›è¡Œå˜æ¢å’Œå¢å¼ºï¼ŒåŒ…å«å±‚å½’ä¸€åŒ–ã€å‰é¦ˆå±‚ä»¥åŠæ®‹å·®è¿æ¥ã€‚

æ¨¡å—çš„ä½œç”¨
å±‚å½’ä¸€åŒ–: åœ¨è¿›å…¥å‰é¦ˆå±‚ä¹‹å‰å¯¹è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œå¸®åŠ©æå‡è®­ç»ƒçš„ç¨³å®šæ€§ã€‚
å‰é¦ˆç¥ç»ç½‘ç»œ: é€šè¿‡ FeedForward æ¨¡å—å¯¹å½’ä¸€åŒ–åçš„è¾“å…¥è¿›è¡Œéçº¿æ€§å˜æ¢ï¼Œå¢å¼ºæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›ã€‚
æ®‹å·®è¿æ¥: å°†è¾“å…¥çš„éšè—çŠ¶æ€ä¸å‰é¦ˆå±‚çš„è¾“å‡ºç›¸åŠ ï¼Œä¿ƒè¿›ä¿¡æ¯æµåŠ¨å¹¶ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚
å¯é€‰çš„ dropout: åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
å¦‚ä½•è°ƒç”¨
1. åˆå§‹åŒ–æ¨¡å—
é¦–å…ˆï¼Œéœ€è¦å®ä¾‹åŒ– FFNBlock ç±»ã€‚ä½ éœ€è¦æä¾›ä»¥ä¸‹å‚æ•°ï¼š

dim_model: æ¨¡å‹çš„ä¸»ç»´åº¦ï¼ˆé€šå¸¸æ˜¯åµŒå…¥ç»´åº¦ï¼‰ã€‚
dim_ff: å‰é¦ˆç½‘ç»œçš„ç»´åº¦ã€‚
dtype: æ•°æ®ç±»å‹ï¼ˆé»˜è®¤ä¸º torch.halfï¼‰ã€‚
eps: ç”¨äºå±‚å½’ä¸€åŒ–çš„ epsilon å€¼ï¼ˆé»˜è®¤ä¸º 1e-6ï¼‰ã€‚
dropout_p: dropout æ¦‚ç‡ï¼ˆé»˜è®¤ä¸º 0ï¼‰ã€‚

```
import torch
# å‚æ•°è®¾ç½®
dim_model = 512
dim_ff = 2048
dropout_p = 0.1

# å®ä¾‹åŒ– FFNBlock
ffn_block = FFNBlock(
    dim_model=dim_model,
    dim_ff=dim_ff,
    dropout_p=dropout_p
)

# åˆ›å»ºç¤ºä¾‹è¾“å…¥
batch_size = 2
seq_length = 10

hidden_states = torch.rand(batch_size, seq_length, dim_model)

# è°ƒç”¨æ¨¡å—
output = ffn_block(hidden_states)
FFNBlock çš„è¾“å‡ºæ˜¯å½¢çŠ¶ä¸º (batch, seq_self, dim_model) çš„å¼ é‡ï¼Œè¡¨ç¤ºç»è¿‡å‰é¦ˆç½‘ç»œå¤„ç†åçš„éšè—çŠ¶æ€ã€‚
æ€»ç»“
FFNBlock æ¨¡å—é€šè¿‡å±‚å½’ä¸€åŒ–ã€å‰é¦ˆç½‘ç»œå’Œæ®‹å·®è¿æ¥çš„ç»„åˆï¼Œå¢å¼ºäº† Transformer æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›å’Œè®­ç»ƒç¨³å®šæ€§ã€‚é€šè¿‡æ­£ç¡®åˆå§‹åŒ–å¹¶è°ƒç”¨è¯¥æ¨¡å—ï¼Œå¯ä»¥åœ¨ Transformer æ¨¡å‹ä¸­å®ç°å‰é¦ˆç¥ç»ç½‘ç»œçš„åŠŸèƒ½ã€‚
```



## 14 .TransformerBlockæ¨¡å—

```
 TransformerBlock
```

```
class TransformerBlock(torch.nn.Module):
    """The whole transformer block. A sequence of operation. Consists of self-attention block[, cross-attention block] and feed-forward block.
    Args:
        dim_model (int): main dimension of modules in transformer blocks.
        dim_ff (int): dim_ff used in :py:class:`model_center.layer.FeedForward`.
        num_heads (int): num_heads used in :py:class:`model_center.layer.Attention`.
        dim_head (int): dim_head used in :py:class:`model_center.layer.Attention`.
        dtype (optional): Defaults to torch.half.
        eps (float, optional): eps used in :py:class:`model_center.layer.LayerNorm`. Defaults to 1e-5.
        dropout_p (float, optional): Defaults to 0.
    """  # noqa: E501

    def __init__(
        self,
        dim_model: int,
        dim_ff: int,
        num_heads: int,
        dim_head: int,
        dtype=torch.half,
        eps: float = 1e-6,
        dropout_p: Optional[float] = None,
        mask_att: bool = False,
        mask_ffn: bool = False,
    ):
        super().__init__()
        self.mask_att = mask_att
        self.mask_ffn = mask_ffn

        if not self.mask_att:
            self.self_att = SelfAttentionBlock(
                dim_model=dim_model,
                num_heads=num_heads,
                dim_head=dim_head,
                dtype=dtype,
                eps=eps,
                dropout_p=dropout_p,
            )

        if not self.mask_ffn:
            self.ffn = FFNBlock(
                dim_model=dim_model,
                dim_ff=dim_ff,
                dtype=dtype,
                eps=eps,
                dropout_p=dropout_p,
            )

    def forward(
        self,
        self_hidden_states: torch.Tensor,
        self_attention_mask: torch.Tensor,
        self_position_bias: Optional[torch.Tensor] = None,
        use_cache: bool = False,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
    ):
        """
        Args:
            self_hidden_states (:obj:`torch.Tensor` of shape ``(batch, seq_self, dim_model)``): Input of transformer block(self-attention block). It can be the raw embedding of a batch of sequences.
            self_attention_mask (:obj:`torch.Tensor` of shape ``(batch, seq_self, seq_self)``): Avoid invalid areas to participate in the calculation of self-attention.
            self_position_bias (:obj:`torch.Tensor` of shape ``(num_heads, seq_self, seq_self)``): Provide positional information to self-attention block.

        Return:
            :obj:`torch.Tensor` of shape ``(batch, seq_self, dim_model)``: The output of transformer block.

        """  # noqa: E501
        # (batch, dim_model, seq_self)
        current_key_value = None
        if not self.mask_att:
            hidden_states = self.self_att(
                self_hidden_states,
                attention_mask=self_attention_mask,
                position_bias=self_position_bias,
                use_cache=use_cache,
                past_key_value=past_key_value,
            )
            if use_cache:
                hidden_states, current_key_value = hidden_states
        else:
            hidden_states = self_hidden_states

        # (batch, dim_model, seq_self)
        if not self.mask_ffn:
            hidden_states = self.ffn(hidden_states)

        if use_cache:
            return hidden_states, current_key_value
        else:
            return hidden_states
```

`TransformerBlock` æ˜¯ä¸€ä¸ªå®ç° Transformer æ¨¡å—çš„ PyTorch ç±»ï¼Œç»“åˆäº†è‡ªæ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚å®ƒé€šå¸¸ç”¨äº Transformer æ¨¡å‹çš„ç»“æ„ä¸­ã€‚

æ¨¡å—çš„ä½œç”¨

1. è‡ªæ³¨æ„åŠ›æœºåˆ¶: é€šè¿‡ SelfAttentionBlock å¤„ç†è¾“å…¥çš„éšè—çŠ¶æ€ï¼Œæ•æ‰åºåˆ—ä¸­ä¸åŒä½ç½®ä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚
2. å‰é¦ˆç¥ç»ç½‘ç»œ: é€šè¿‡ FFNBlock å¯¹ç»è¿‡è‡ªæ³¨æ„åŠ›å¤„ç†çš„éšè—çŠ¶æ€è¿›è¡Œè¿›ä¸€æ­¥çš„éçº¿æ€§å˜æ¢ã€‚
3. æ©ç åŠŸèƒ½: æä¾›é€‰æ‹©æ€§çš„æ©ç åŠŸèƒ½ï¼Œå…è®¸ç”¨æˆ·åœ¨æŸäº›æƒ…å†µä¸‹è·³è¿‡è‡ªæ³¨æ„åŠ›æˆ–å‰é¦ˆç½‘ç»œè®¡ç®—ï¼Œé€‚ç”¨äºç‰¹å®šä»»åŠ¡ï¼ˆå¦‚è®­ç»ƒæˆ–æ¨ç†æ—¶çš„ä¼˜åŒ–ï¼‰ã€‚
4. æ®‹å·®è¿æ¥: ç»“åˆè¾“å…¥å’Œæ¯ä¸ªå­æ¨¡å—çš„è¾“å‡ºï¼Œå¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ ã€‚

å¦‚ä½•è°ƒç”¨

1. åˆå§‹åŒ–æ¨¡å—

é¦–å…ˆï¼Œéœ€è¦å®ä¾‹åŒ– `TransformerBlock` ç±»ã€‚ä½ éœ€è¦æä¾›ä»¥ä¸‹å‚æ•°ï¼š

- `dim_model`: æ¨¡å‹çš„ä¸»ç»´åº¦ï¼ˆé€šå¸¸æ˜¯åµŒå…¥ç»´åº¦ï¼‰ã€‚
- `dim_ff`: å‰é¦ˆç½‘ç»œçš„ç»´åº¦ã€‚
- `num_heads`: æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚
- `dim_head`: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ã€‚
- `dtype`: æ•°æ®ç±»å‹ï¼ˆé»˜è®¤ä¸º `torch.half`ï¼‰ã€‚
- `eps`: ç”¨äºå±‚å½’ä¸€åŒ–çš„ epsilon å€¼ï¼ˆé»˜è®¤ä¸º `1e-6`ï¼‰ã€‚
- `dropout_p`: dropout æ¦‚ç‡ï¼ˆé»˜è®¤ä¸º `None`ï¼‰ã€‚
- `mask_att` å’Œ `mask_ffn`: å¸ƒå°”å€¼ï¼ŒæŒ‡ç¤ºæ˜¯å¦æ©ç›–è‡ªæ³¨æ„åŠ›æˆ–å‰é¦ˆç½‘ç»œçš„è®¡ç®—ã€‚

python

å¤åˆ¶

```
import torch

# å‚æ•°è®¾ç½®
dim_model = 512
dim_ff = 2048
num_heads = 8
dim_head = 64
dropout_p = 0.1

# å®ä¾‹åŒ– TransformerBlock
transformer_block = TransformerBlock(
    dim_model=dim_model,
    dim_ff=dim_ff,
    num_heads=num_heads,
    dim_head=dim_head,
    dropout_p=dropout_p
)
```

2. å‡†å¤‡è¾“å…¥æ•°æ®

è¾“å…¥æ•°æ®åŒ…æ‹¬ï¼š

- `self_hidden_states`: å½¢çŠ¶ä¸º `(batch, seq_self, dim_model)` çš„å¼ é‡ï¼Œè¡¨ç¤ºä¸€æ‰¹åºåˆ—çš„åµŒå…¥ã€‚
- `self_attention_mask`: å½¢çŠ¶ä¸º `(batch, seq_self, seq_self)` çš„å¼ é‡ï¼Œç”¨äºé¿å…æ— æ•ˆåŒºåŸŸå‚ä¸è‡ªæ³¨æ„åŠ›è®¡ç®—ã€‚
- `self_position_bias`: å¯é€‰ï¼Œå½¢çŠ¶ä¸º `(num_heads, seq_self, seq_self)` çš„å¼ é‡ï¼Œç”¨äºæä¾›ä½ç½®ä¿¡æ¯ã€‚



## 15.AttentionPool2d

```
class AttentionPool2d(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)

åœ¨è¿™æ®µä»£ç ä¸­ï¼Œtorch.randn(spacial_dim ** 2 + 1, embed_dim) ç”¨äºç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º (spacial_dim ** 2 + 1, embed_dim) çš„éšæœºå¼ é‡ï¼Œè¿™ä¸ªå¼ é‡ç¨åä¼šè¢«ç”¨ä½œä½ç½®ç¼–ç ï¼ˆpositional encodingï¼‰ã€‚
è¿™é‡Œçš„ +1 æ˜¯ä¸ºäº†åœ¨åŸæœ‰çš„ä½ç½®ç¼–ç åŸºç¡€ä¸Šæ·»åŠ ä¸€ä¸ªé¢å¤–çš„ç»´åº¦ï¼Œè¿™ä¸ªç»´åº¦é€šå¸¸ç”¨äºæ•è·æ•´ä¸ªåºåˆ—çš„å…¨å±€ä¿¡æ¯ã€‚åœ¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œé™¤äº†å¯¹æ¯ä¸ªä½ç½®çš„å±€éƒ¨ç‰¹å¾è¿›è¡Œç¼–ç å¤–ï¼Œæœ‰æ—¶è¿˜éœ€è¦ä¸€ä¸ªé¢å¤–çš„å…ƒç´ æ¥è¡¨ç¤ºåºåˆ—çš„æ•´ä½“ä¸Šä¸‹æ–‡ã€‚è¿™ä¸ªé¢å¤–çš„å…ƒç´ æœ‰æ—¶è¢«ç§°ä¸ºâ€œå…¨å±€æ ‡è®°â€ï¼ˆglobal tokenï¼‰æˆ–â€œåˆ†ç±»æ ‡è®°â€ï¼ˆclassification tokenï¼‰ï¼Œåœ¨ä¸åŒçš„ä¸Šä¸‹æ–‡ä¸­æœ‰ä¸åŒçš„ç”¨é€”ï¼š

åœ¨æŸäº›ä»»åŠ¡ä¸­ï¼Œå¦‚å›¾åƒåˆ†ç±»ï¼Œè¿™ä¸ªé¢å¤–çš„æ ‡è®°å¯èƒ½ç”¨äºå­˜å‚¨å›¾åƒçº§åˆ«çš„ä¿¡æ¯æˆ–èšåˆç‰¹å¾ã€‚
åœ¨Transformeræ¶æ„ä¸­ï¼Œè¿™ä¸ªæ ‡è®°å¸¸ç”¨äºè¡¨ç¤ºåºåˆ—çš„å¼€å§‹ï¼Œæœ‰æ—¶ä¹Ÿç”¨äºåˆ†ç±»ä»»åŠ¡çš„è¾“å‡ºã€‚
é€šè¿‡æ·»åŠ è¿™ä¸ªé¢å¤–çš„ç»´åº¦ï¼Œæ¨¡å‹å¯ä»¥å­¦ä¹ åˆ°å¦‚ä½•å°†è¿™ä¸ªå…¨å±€æ ‡è®°ä¸åºåˆ—ä¸­å…¶ä»–ä½ç½®çš„ç‰¹å¾è¿›è¡Œäº¤äº’ï¼Œä»è€Œæ•è·æ›´å…¨é¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

æ­¤å¤–ï¼Œ/ embed_dim ** 0.5 æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œç”¨äºå¯¹ä½ç½®ç¼–ç çš„åˆå§‹éšæœºå€¼è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä»¥å¸®åŠ©ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚è¿™ç§ç¼©æ”¾æ˜¯æ ¹æ®åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œçš„ï¼Œæ˜¯ä¸€ç§ç»éªŒæ€§çš„é€‰æ‹©ï¼Œæœ‰åŠ©äºé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸é—®é¢˜ã€‚

æ€»çš„æ¥è¯´ï¼Œspacial_dim ** 2 + 1 è¡¨ç¤ºä½ç½®ç¼–ç çš„æ€»ç»´åº¦æ•°ï¼Œå…¶ä¸­åŒ…æ‹¬äº† spacial_dim ** 2 ä¸ªç©ºé—´ä½ç½®çš„ç¼–ç å’Œä¸€ä¸ªé¢å¤–çš„å…¨å±€æ ‡è®°ã€‚
positional_embedding =nn.Parameter(torch.randn(spacial_dim*spacial_dim+1,embedding_dim)/embedding_dim**0.5)


```

```
x1 =torch.randn(1,3,32,32)
x1= x1.flatten(start_dim=2)
print(x1.shape)  torch.Size([1, 3, 1024])

print(x1.permute(2,0,1).shape)  torch.Size([1024, 1, 3])
```



## 16 .EmbeddingExtæ¨¡å—

```
class EmbeddingExt(torch.nn.Module):
    def __init__(
        self,
        vocab_size: int,
        embedding_size: int,
        dtype: torch.dtype = torch.half,
        init_mean: float = 0.0,
        init_std: float = 1,
        distance_scale: int = 16,
    ):

        super().__init__()

        self.dim_model = embedding_size
        self.rotary_emb = RotaryEmbedding(
            dim=embedding_size, distance_scale=distance_scale, dtype=dtype
        )

        self.weight = torch.nn.parameter.Parameter(
            torch.empty(vocab_size, embedding_size, dtype=dtype),
        )

    def forward(self, ids: torch.Tensor, ids_sub: torch.Tensor):
        """
        Args:
            ids (:obj:`torch.Tensor` of shape ``(batch_size, seq_len)``): Indices of input sequence tokens.
            ids (:obj:`torch.Tensor` of shape ``(batch_size)``): Subscript of input sequence tokens.
        Return:
            :obj:`torch.Tensor` of shape ``(batch_size, seq_len, embedding_size)``: The embedding output.
        """  # noqa: E501

        embeds = F.embedding(ids, self.weight) / math.sqrt(self.dim_model)
        return self.rotary_emb(embeds, ids_sub)

    def projection(self, x: torch.Tensor, ext_table: Optional[torch.Tensor] = None):
        """
        Projection based on embedding's weight. For example, embedding map vocab_size to embed_size, than projection map embed_size back to vocab_size.
        Args:
            x (:obj:`torch.Tensor` of shape ``(batch, seq_len, dim_model)``): Input of projection
            ext_table (:obj:`torch.Tensor` of shape ``(ext_table_size, dim_model)``): Ext vocab table.
        Returns:
            :obj:`torch.Tensor` of shape ``(batch, seq_len, vocab_size + ext_table_size)``: The projection output.
        """  # noqa: E501
        logits = F.linear(x / math.sqrt(self.dim_model), self.weight)
        if ext_table is not None:
            logits_ext = F.linear(x, ext_table)
            logits = torch.cat([logits, logits_ext], dim=-1)
        return logits
```

```
EmbeddingExt ç±»æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰çš„ PyTorch æ¨¡å—ï¼Œä¸»è¦ç”¨äºå®ç°æ‰©å±•çš„åµŒå…¥å±‚ã€‚ä»¥ä¸‹æ˜¯å¯¹è¯¥ç±»çš„è¯¦ç»†è§£æå’Œå…¶ç‰¹æ®Šä¹‹å¤„ï¼š

ä¸»è¦ç»„æˆéƒ¨åˆ†
åˆå§‹åŒ–æ–¹æ³• __init__:
å‚æ•°:
vocab_size: è¯æ±‡è¡¨å¤§å°ã€‚
embedding_size: åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚
dtype: æ•°æ®ç±»å‹ï¼Œé»˜è®¤ä¸º torch.halfã€‚
init_mean å’Œ init_std: ç”¨äºåˆå§‹åŒ–æƒé‡çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆè™½ç„¶åœ¨è¯¥ä»£ç ä¸­æœªä½¿ç”¨ï¼‰ã€‚
distance_scale: ç”¨äºæ—‹è½¬åµŒå…¥çš„è·ç¦»ç¼©æ”¾å› å­ã€‚
æˆå‘˜å˜é‡:
dim_model: å­˜å‚¨åµŒå…¥ç»´åº¦ã€‚
rotary_emb: å®ä¾‹åŒ– RotaryEmbeddingï¼Œç”¨äºæ—‹è½¬åµŒå…¥ã€‚
weight: åµŒå…¥çš„æƒé‡å‚æ•°ã€‚
å‰å‘ä¼ æ’­æ–¹æ³• forward:
æ¥å—ä¸¤ä¸ªå¼ é‡ ids å’Œ ids_subï¼Œåˆ†åˆ«è¡¨ç¤ºè¾“å…¥åºåˆ—çš„ç´¢å¼•å’Œå­ç´¢å¼•ã€‚
ä½¿ç”¨ F.embedding è¿›è¡ŒåµŒå…¥æŸ¥æ‰¾ï¼Œå¹¶è¿›è¡Œç¼©æ”¾ã€‚
é€šè¿‡æ—‹è½¬åµŒå…¥çš„ rotary_emb æ–¹æ³•å¤„ç†åµŒå…¥ï¼Œå¢å¼ºäº†ä½ç½®ä¿¡æ¯çš„è¡¨ç¤ºã€‚
æŠ•å½±æ–¹æ³• projection:
å°†åµŒå…¥è¾“å‡ºæŠ•å½±å›è¯æ±‡ç©ºé—´ã€‚
å¦‚æœæä¾›äº†æ‰©å±•è¡¨ ext_tableï¼Œåˆ™å°†å…¶ä¸åŸå§‹æŠ•å½±ç»“æœè¿æ¥ï¼Œäº§ç”Ÿæ›´å¤§çš„è¾“å‡ºç©ºé—´ã€‚
é€‚ç”¨äºæ‰©å±•è¯æ±‡è¡¨çš„æƒ…å†µï¼Œå¦‚å¤„ç†æœªç™»å½•è¯ï¼ˆOOVï¼‰ã€‚
ç‰¹ç‚¹å’Œä¼˜åŠ¿
æ—‹è½¬åµŒå…¥:
RotaryEmbedding æä¾›äº†ä¸€ç§å¢å¼ºä½ç½®ä¿¡æ¯ç¼–ç çš„æ–¹å¼ï¼Œé€šè¿‡åœ¨åµŒå…¥ä¸­åŠ å…¥æ—‹è½¬æœºåˆ¶ï¼Œæ”¹å–„æ¨¡å‹å¯¹åºåˆ—ä¸­ä½ç½®ä¿¡æ¯çš„å¤„ç†ã€‚
æ‰©å±•çš„è¯æ±‡å¤„ç†:
å…è®¸é€šè¿‡ projection æ–¹æ³•å¤„ç†æ‰©å±•è¯æ±‡è¡¨ï¼Œé€‚ç”¨äºéœ€è¦åŠ¨æ€è¯æ±‡è¡¨çš„åº”ç”¨åœºæ™¯ã€‚
çµæ´»æ€§:
è®¾è®¡ä¸Šå…è®¸ç”¨æˆ·è‡ªå®šä¹‰åµŒå…¥çš„ç»´åº¦å’Œæ•°æ®ç±»å‹ï¼Œé€‚åº”ä¸åŒçš„éœ€æ±‚ã€‚
é«˜æ•ˆæ€§:
ä½¿ç”¨äº† PyTorch çš„é«˜æ•ˆè®¡ç®—ï¼Œé€šè¿‡çº¿æ€§å˜æ¢å’ŒåµŒå…¥æŸ¥æ‰¾å®ç°é«˜æ•ˆçš„å‰å‘ä¼ æ’­ã€‚
æ€»ç»“
EmbeddingExt ç±»æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„åµŒå…¥å±‚å®ç°ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦å¤„ç†å¤šæ¨¡æ€è¾“å…¥å’Œæ‰©å±•è¯æ±‡çš„æ·±åº¦å­¦ä¹ ä»»åŠ¡ã€‚å…¶è®¾è®¡çµæ´»ï¼Œç»“åˆäº†æ—‹è½¬åµŒå…¥æŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹å¯¹ä½ç½®ä¿¡æ¯çš„ç†è§£èƒ½åŠ›ã€‚
```



## 17. CausalLMOutputç”¨æ³• 

```
from transformers.modeling_outputs import CausalLMOutput
from transformers.utils import ModelOutput
```



## 18. ModelOutputç”¨æ³•

from transformers.utils import ModelOutput

```
@dataclass
class CausalVLLMOutput(ModelOutput):
    loss: Optional[torch.FloatTensor] = None
    logits: torch.FloatTensor = None
    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    vision_hidden_states: Optional[Tuple[torch.FloatTensor]] = None
    attentions: Optional[Tuple[torch.FloatTensor]] = None
```



```
åœ¨PyTorchä¸­ï¼Œé™¤äº†permuteæ“ä½œï¼Œè¿˜æœ‰å‡ ç§æ–¹æ³•å¯ä»¥ç”¨æ¥æ”¹å˜å¼ é‡çš„ç»´åº¦é¡ºåºæˆ–è¿›è¡Œç±»ä¼¼çš„æ“ä½œï¼š

transpose æ–¹æ³•ï¼š
transposeæ–¹æ³•å¯ä»¥ç”¨æ¥äº¤æ¢å¼ é‡çš„ä¸¤ä¸ªç»´åº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¼šäº¤æ¢ç¬¬ä¸€ä¸ªå’Œç¬¬äºŒä¸ªç»´åº¦ï¼Œä½†ä½ ä¹Ÿå¯ä»¥æŒ‡å®šè¦äº¤æ¢çš„ç»´åº¦ç´¢å¼•ã€‚

python
x = x.transpose(0, 1)  # äº¤æ¢ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´
t å±æ€§ï¼š
å¯¹äºäºŒç»´å¼ é‡ï¼ˆçŸ©é˜µï¼‰ï¼Œtå±æ€§æ˜¯transposeçš„å¿«æ·æ–¹å¼ï¼Œæ•ˆæœç­‰åŒäºx.transpose(0, 1)ã€‚

python
x = x.t()  # åªé€‚ç”¨äºäºŒç»´å¼ é‡
view æ–¹æ³•ï¼š
viewæ–¹æ³•å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶è€Œä¸æ”¹å˜å…¶æ•°æ®ã€‚è¿™å¯ä»¥ç”¨äºé‡æ–°æ’åˆ—ç»´åº¦ï¼Œåªè¦æ–°å½¢çŠ¶çš„æ€»å…ƒç´ æ•°é‡ä¸åŸå§‹å¼ é‡ç›¸åŒã€‚

python
x = x.view(10, -1)  # å°†å¼ é‡è§†ä½œ10è¡Œï¼Œå‰©ä½™ç»´åº¦ä¼šè‡ªåŠ¨è®¡ç®—
reshape æ–¹æ³•ï¼ˆåœ¨torch.Tensorä¸­ç§°ä¸ºreshapeï¼‰ï¼š
reshapeæ–¹æ³•ä¸viewç±»ä¼¼ï¼Œä¹Ÿå¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ã€‚ä½†æ˜¯ï¼Œreshapeè¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œè€Œä¸ä¼šå°±åœ°ä¿®æ”¹åŸå§‹å¼ é‡ã€‚

python
x = x.reshape(10, -1)  # è¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º10è¡Œï¼Œå‰©ä½™ç»´åº¦ä¼šè‡ªåŠ¨è®¡ç®—
unsqueeze æ–¹æ³•ï¼š
unsqueezeæ–¹æ³•å¯ä»¥åœ¨æŒ‡å®šä½ç½®æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œå…¶å¤§å°ä¸º1ã€‚

python
x = x.unsqueeze(0)  # åœ¨ç¬¬ä¸€ä¸ªç»´åº¦å‰æ·»åŠ ä¸€ä¸ªå¤§å°ä¸º1çš„ç»´åº¦
squeeze æ–¹æ³•ï¼š
squeezeæ–¹æ³•å¯ä»¥ç§»é™¤å¤§å°ä¸º1çš„ç»´åº¦ã€‚è¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥ç”¨æ¥å‡å°‘ç»´åº¦ã€‚
python
x = x.squeeze(0)  # å¦‚æœç¬¬ä¸€ä¸ªç»´åº¦çš„å¤§å°ä¸º1ï¼Œå°†å…¶ç§»é™¤
flatten æ–¹æ³•ï¼š
flattenæˆ–view(-1)å¯ä»¥å°†å¼ é‡å±•å¹³ä¸ºä¸€ç»´ï¼Œæˆ–è€…ä»æŒ‡å®šç»´åº¦å¼€å§‹å±•å¹³ã€‚

python
x = x.flatten(start_dim=1)  # ä»ç¬¬äºŒç»´å¼€å§‹å±•å¹³
roll æ–¹æ³•ï¼š
rollæ–¹æ³•å¯ä»¥å°†å¼ é‡çš„ç»´åº¦æŒ‰ç…§æŒ‡å®šçš„åç§»é‡æ»šåŠ¨ã€‚è¿™å¯ä»¥ç”¨æ¥å¾ªç¯ç§»åŠ¨ç»´åº¦çš„ä½ç½®ã€‚

python
x = x.roll(1, dims=0)  # æ²¿ç€ç¬¬ä¸€ä¸ªç»´åº¦æ»šåŠ¨
ä½¿ç”¨ç´¢å¼•ï¼š
é€šè¿‡ä½¿ç”¨ç´¢å¼•ï¼Œä½ ä¹Ÿå¯ä»¥é‡æ–°æ’åˆ—å¼ é‡çš„ç»´åº¦ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨index_selectæˆ–gatherè¿›è¡Œé«˜çº§ç´¢å¼•æ“ä½œã€‚

è¿™äº›æ–¹æ³•å¯ä»¥æ ¹æ®éœ€è¦ç»„åˆä½¿ç”¨ï¼Œä»¥å®ç°å¤æ‚çš„å¼ é‡å½¢çŠ¶å˜æ¢å’Œç»´åº¦é‡æ–°æ’åˆ—ã€‚åœ¨è¿›è¡Œè¿™äº›æ“ä½œæ—¶ï¼Œéœ€è¦ç¡®ä¿æ“ä½œæ˜¯æœ‰æ•ˆçš„ï¼Œå³æ–°å½¢çŠ¶çš„æ€»å…ƒç´ æ•°é‡å¿…é¡»ä¸åŸå§‹å¼ é‡ç›¸åŒã€‚
```



## 19 . transformersçš„Trainer

```
from transformers import Trainer
Trainer(
    model_init=None,optimizers=None,train_dataset=None,eval_dataset=None,
args=None,tokenizer=None,data_collator=None,compute_metrics=None
)
```



## 20 . ApproximateGELU

```
class ApproximateGELU(nn.Module):
    """
    The approximate form of Gaussian Error Linear Unit (GELU)

    For more details, see section 2: https://arxiv.org/abs/1606.08415
    """

    def __init__(self, dim_in: int, dim_out: int):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out)

    def forward(self, x):
        x = self.proj(x)
        return x * torch.sigmoid(1.702 * x)
```







# torchçš„ä¸€äº›ç”¨æ³•

## 1 [:,None,:] ç”¨æ³• 

```
import torch

# å‡è®¾ä¸€ä¸ªå½¢çŠ¶ä¸º (2, 5, 512) çš„å¼ é‡
positional_embedding = torch.randn(2, 5, 512)

# ä½¿ç”¨ [:, None, :] å¢åŠ ä¸€ä¸ªç»´åº¦
expanded_embedding = positional_embedding[:, None, :]

print(expanded_embedding.shape)  # è¾“å‡º: (2, 1, 5, 512)

# åœ¨æœ€åä¸€ç»´åå¢åŠ ä¸€ä¸ªç»´åº¦
expanded_embedding = positional_embedding[..., None]

print(expanded_embedding.shape)  # è¾“å‡º: (2, 5, 512, 1)

æ–¹æ³• 2: ä½¿ç”¨ torch.unsqueeze
positional_embedding = positional_embedding.unsqueeze(-1)

postion_embedding =torch.randn(3,12,512)
# postion_embedding = postion_embedding[:,None,:]
# print(postion_embedding.shape)

postion_embedding = postion_embedding.unsqueeze(2)
print(postion_embedding.shape)
torch.Size([3, 12, 1, 512])
```

å‡è®¾ positional_embedding çš„å½¢çŠ¶ä¸º (batch_size, seq_len, embedding_dim)ï¼Œè°ƒç”¨ positional_embedding[:, None, :] åï¼Œç»“æœçš„å½¢çŠ¶å°†å˜ä¸º (batch_size, 1, seq_len, embedding_dim)ã€‚

## 2.torch.full ç”¨æ³•

```
target = torch.full(size=(3,4),fill_value=0.5,requires_grad=False)
print(target)
torch.full(size, fill_value, dtype=None, device=None, requires_grad=False)
```

- **size**: ç›®æ ‡å¼ é‡çš„å½¢çŠ¶ï¼ˆä¾‹å¦‚ï¼Œ`(2, 3)` è¡¨ç¤ºä¸€ä¸ª 2 è¡Œ 3 åˆ—çš„å¼ é‡ï¼‰ã€‚
- **fill_value**: å¡«å……å¼ é‡çš„å€¼ã€‚
- **dtype** (å¯é€‰): æŒ‡å®šå¼ é‡çš„æ•°æ®ç±»å‹ï¼ˆå¦‚ `torch.float`, `torch.int` ç­‰ï¼‰ã€‚
- **device** (å¯é€‰): æŒ‡å®šå¼ é‡çš„è®¾å¤‡ï¼ˆå¦‚ `'cpu'` æˆ– `'cuda'`ï¼‰ã€‚
- **requires_grad** (å¯é€‰): æ˜¯å¦éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ˆé»˜è®¤ä¸º `False`ï¼‰ã€‚

import torch

#         

## 3. ACT2FNç”¨æ³•

```python
from transformers.activations import ACT2FN
import torch
input = torch.randn(2,2,)
fn = ACT2FN['gelu']
output = fn(input)
print(output.shape)
```



```
from transformers import add_start_docstrings,add_end_docstrings
from transformers.utils import add_start_docstrings_to_model_forward,replace_return_docstrings

add_start_docstrings`ã€`add_start_docstrings_to_model_forward` å’Œ `replace_return_docstrings

add_start_docstringsç”¨æ³• 
class mymodel:
    @add_start_docstrings("æˆ‘å¯ä»¥å‡è£…å¾ˆå¥½")
    def __init__(self):
        """learn something"""
        pass
print(mymodel.__init__.__doc__)

class Pos:
    @add_start_docstrings("å¼€å§‹äº†")
    @add_end_docstrings("ç»“æŸäº†")
    def __init__(self):
        pass
    @add_start_docstrings("processå¼€å§‹äº†")
    @add_end_docstrings("processç»“æŸäº†")
    def process(self,input):
        return input

print(Pos.__init__.__doc__)

```



## 4. transformers.modeling_outputs

```python
transformers.modeling_outputs
from transformers.modeling_outputs import BaseModelOutputWithPast,CausalLMOutputWithPast,SequenceClassifierOutputWithPast

base_out= BaseModelOutputWithPast(last_hidden_state=None,
                                  hidden_states=None,
                                  past_key_values=None,
                                  attentions=None)
                                  
cassal_out = CausalLMOutputWithPast(logits=None,
                       past_key_values=None,
                       hidden_states=None,
                       attentions=None)
seq_out = SequenceClassifierOutputWithPast(
    logits=torch.rand(2,3),hidden_states=None,attentions=None,past_key_values=None
)

print(seq_out.logits)
```



## 5.torch.cumsum ç”¨æ³•

```
torch.cumsum
s1 =torch.tensor([1,2,3,4])
s2 =torch.cumsum(s1,dim=0)
print(s2)
s3 =torch.tensor([[1,2,3],
                 [2,3,4]])
s4 = torch.cumsum(s3,dim=1)
print(s4)
tensor([[0.1661, 0.7173, 0.9430],
        [0.4725, 0.0895, 0.2758]])
tensor([ 1,  3,  6, 10])
tensor([[1, 3, 6],
        [2, 5, 9]])
```



```python
seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)

attention_mask=torch.tensor(
    [[[1,2,3],
    [2,3,4]],
[[1,2,3],
    [2,3,4]],]
)
attention_mask= attention_mask.sum(dim=-1)
print(attention_mask)
tensor([[6, 9],
        [6, 9]])
```

## 6.torch.nonzero ç”¨æ³•

```
indices = atten_msk=torch.tensor(
    [[1,0,1,0],[2,0,1,3],[0,0,1,1]]
).flatten()
indices : tensor([1, 0, 1, 0, 2, 0, 1, 3, 0, 0, 1, 1])
print(torch.nonzero(indices))
tensor([[ 0],
        [ 2],
        [ 4],
        [ 6],
        [ 7],
        [10],
        [11]])
torch.Size([7, 1])

seq_batch =torch.tensor([[1,12,3],[2,3,10]])
print(seq_batch.max().item())  12
 
```

## 7. F.padçš„ç”¨æ³• 

```python
torch.nn.functional.pad(input, (1, 0)) çš„ä½¿ç”¨æ¶‰åŠå¯¹å¼ é‡è¿›è¡Œå¡«å……ã€‚è¿™é‡Œçš„å¡«å……å‚æ•° (1, 0) ä»£è¡¨äº†åœ¨ç‰¹å®šç»´åº¦ä¸Šçš„å¡«å……é‡ã€‚å…·ä½“æ¥è¯´ï¼Œç†è§£è¿™ä¸ªè°ƒç”¨å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥çœ‹ï¼š
å‚æ•°è§£é‡Š
input: è¦å¡«å……çš„å¼ é‡ã€‚
(1, 0): è¿™æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºå¯¹å¼ é‡çš„å¡«å……æ–¹å¼ã€‚
ç¬¬ä¸€ä¸ªå€¼ 1: åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆé€šå¸¸æ˜¯åˆ—ï¼‰å·¦ä¾§å¡«å…… 1 ä¸ªå•ä½ã€‚
ç¬¬äºŒä¸ªå€¼ 0: åœ¨æœ€åä¸€ä¸ªç»´åº¦å³ä¾§ä¸å¡«å……ï¼ˆ0 å•ä½ï¼‰ã€‚xxxxxxxxxxÂ torch.nn.functional.pad(input, (1, 0)) çš„ä½¿ç”¨æ¶‰åŠå¯¹å¼ é‡è¿›è¡Œå¡«å……ã€‚è¿™é‡Œçš„å¡«å……å‚æ•° (1, 0) ä»£è¡¨äº†åœ¨ç‰¹å®šç»´åº¦ä¸Šçš„å¡«å……é‡ã€‚å…·ä½“æ¥è¯´ï¼Œç†è§£è¿™ä¸ªè°ƒç”¨å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ¥çœ‹ï¼šå‚æ•°è§£é‡Šinput: è¦å¡«å……çš„å¼ é‡ã€‚(1, 0): è¿™æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œè¡¨ç¤ºå¯¹å¼ é‡çš„å¡«å……æ–¹å¼ã€‚ç¬¬ä¸€ä¸ªå€¼ 1: åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆé€šå¸¸æ˜¯åˆ—ï¼‰å·¦ä¾§å¡«å…… 1 ä¸ªå•ä½ã€‚ç¬¬äºŒä¸ªå€¼ 0: åœ¨æœ€åä¸€ä¸ªç»´åº¦å³ä¾§ä¸å¡«å……ï¼ˆ0 å•ä½ï¼‰ã€‚F.pad
import torch.nn.functional as F

# åˆ›å»ºä¸€ä¸ªç¤ºä¾‹å¼ é‡
tensor = torch.tensor([[1, 2], [3, 4]])
# ä½¿ç”¨ F.pad è¿›è¡Œå¡«å……
padded_tensor = F.pad(tensor, (1, 0))
print("Original tensor:\n", tensor)
print("Padded tensor:\n", padded_tensor)
 tensor([[0, 1, 2],
        [0, 3, 4]])
```

## 8 .torch.finfo(dtype)ç”¨æ³• 

```python
torch.tensor(torch.finfo(dtype).min
print(torch.finfo(torch.float32).min)  -3.4028234663852886e+38

print(torch.finfo(torch.int8).min)æŠ¥é”™ 
TypeError: torch.finfo() requires a floating point input type. Use torch.iinfo to handle 'torch.finfo'

print(torch.finfo(torch.float16).min)  -65504.0
```



## 9 . mask.masked_fill_ å’Œ mask.masked_fillçš„åŒºåˆ«

```python
1. æ–¹æ³•ç±»å‹
masked_fill: è¿™æ˜¯ä¸€ä¸ªéå°±åœ°æ“ä½œï¼Œè¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡ï¼ŒåŸå§‹å¼ é‡ä¿æŒä¸å˜ã€‚
masked_fill_: è¿™æ˜¯ä¸€ä¸ªå°±åœ°æ“ä½œï¼Œç›´æ¥ä¿®æ”¹åŸå§‹å¼ é‡ï¼Œè¿”å›å€¼ä¸º Noneã€‚

input=torch.tensor(
    [[1,2,3],[2,4,6]]
)
mask =torch.tensor([[True,False,True],[True,False,True]])
print(input.masked_fill(
    mask, 0
)) #è¿”å›ä¸€ä¸ªæ–°çš„å¼ é‡ï¼ŒåŸå¼ é‡ä¿æŒä¸å˜
masked_fill_ ä¿®æ”¹åŸå¼ é‡ï¼Œè¿”å›å€¼ä¸ºNone

```

## 10 .staticmethod å’Œ classmethodçš„åŒºåˆ«

```
staticmethod å’Œ classmethod éƒ½æ˜¯ Python ä¸­ç”¨äºå®šä¹‰ç±»çš„æ–¹æ³•ï¼Œä½†å®ƒä»¬ä¹‹é—´æœ‰ä¸€äº›é‡è¦çš„åŒºåˆ«ã€‚
1. staticmethod
å®šä¹‰: ä¸éœ€è¦è®¿é—®ç±»æˆ–å®ä¾‹çš„ä»»ä½•ä¿¡æ¯ã€‚
è°ƒç”¨: é€šè¿‡ç±»æˆ–å®ä¾‹è°ƒç”¨ã€‚
ç¬¬ä¸€ä¸ªå‚æ•°: ä¸éœ€è¦ self æˆ– cls å‚æ•°ã€‚
class BBB:
    def __init__(self):
        pass
    @staticmethod
    def print_data():
        return '11111'
print(BBB.print_data())  11111

classmethod
å®šä¹‰: éœ€è¦è®¿é—®ç±»çš„ä¿¡æ¯ã€‚
è°ƒç”¨: é€šè¿‡ç±»æˆ–å®ä¾‹è°ƒç”¨ã€‚
ç¬¬ä¸€ä¸ªå‚æ•°: å¿…é¡»æœ‰ä¸€ä¸ª cls å‚æ•°ï¼ŒæŒ‡å‘ç±»æœ¬èº«ã€‚
class CCCC:
    modeltype="BERT"
    model_length=100
    def __init__(self):
        pass
    @classmethod
    def print_data(cls):
        return "my model is {}".format(cls.model_length)
ccc=CCCC()
print(ccc.print_data())
staticmethod: ä¸éœ€è¦ç±»æˆ–å®ä¾‹ä¸Šä¸‹æ–‡ï¼Œé€‚åˆä½œä¸ºå·¥å…·å‡½æ•°ã€‚
classmethod: éœ€è¦ç±»ä¸Šä¸‹æ–‡ï¼Œé€‚åˆéœ€è¦è®¿é—®ç±»å˜é‡æˆ–æ–¹æ³•çš„åœºæ™¯ã€‚
```



## 11. è°ƒç”¨ _get_train_sampler



```python
import torch
from torch.utils.data import Dataset,DataLoader,Sampler,RandomSampler
from typing import Optional,Union,Tuple
class my_dataset(Dataset):
    def __init__(self,data):
        self.data =data
    def __len__(self):
        return len(self.data)
    def __getitem__(self, item):
        return self.data[item]
class MYmodel:
    def __init__(self,dataset:Optional[my_dataset]):
        self.dataset =dataset
    def _get_train_sample(self) -> Optional[Sampler]:
        if self.dataset is None or len(self.dataset)==0:
            return None
        return RandomSampler(self.dataset)
# åˆ›å»ºæ•°æ®é›†å’Œæ¨¡å‹å®ä¾‹
data = [i for i in range(10)]
train_dataset = MyDataset(data)
model = MyModel(train_dataset)

# è°ƒç”¨ _get_train_sampler æ–¹æ³•
sampler = model._get_train_sampler()
print(sampler)  # è¾“å‡ºéšæœºé‡‡æ ·å™¨å¯¹è±¡
è°ƒç”¨è¯´æ˜
åˆ›å»ºæ•°æ®é›†ï¼šé¦–å…ˆæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª MyDataset ç±»çš„å®ä¾‹ï¼ŒåŒ…å«ä¸€äº›æ•°æ®ã€‚
åˆ›å»ºæ¨¡å‹å®ä¾‹ï¼šç„¶åæˆ‘ä»¬åˆ›å»ºäº† MyModel ç±»çš„å®ä¾‹ï¼Œå¹¶å°†æ•°æ®é›†ä¼ å…¥ã€‚
è°ƒç”¨ç§æœ‰æ–¹æ³•ï¼šä½¿ç”¨ model._get_train_sampler() è°ƒç”¨ç§æœ‰æ–¹æ³•ï¼Œè¿”å›ä¸€ä¸ªéšæœºé‡‡æ ·å™¨ï¼ˆå¦‚æœæ•°æ®é›†å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼‰ã€‚
-> ä¸ç©ºæ ¼ ä¹Ÿæ˜¯åˆæ³•åˆè§„çš„ ä½†å¯èƒ½ä¸ç¬¦åˆä»£ç é£æ ¼è§„èŒƒã€‚

from einops import rearrange,repeat
input = torch.randn(4,3,24,24)
s = rearrange(input,'b c h w -> h w b c')
print(s.shape)
```



## 12 .LengthGroupedSampler

```
def get_length_grouped_indices(lengths, batch_size, world_size, generator=None, merge=True):
    # We need to use torch for the random part as a distributed sampler will set the random seed for torch.
    indices = torch.randperm(len(lengths), generator=generator)
    megabatch_size = world_size * batch_size
    megabatches = [indices[i : i + megabatch_size].tolist() for i in range(0, len(lengths), megabatch_size)]
    megabatches = [sorted(megabatch, key=lambda i: lengths[i], reverse=True) for megabatch in megabatches]
    megabatches = [split_to_even_chunks(megabatch, lengths, world_size) for megabatch in megabatches]

    return [i for megabatch in megabatches for batch in megabatch for i in batch]
```

```
åœ¨ LengthGroupedSampler ç±»ä¸­ï¼Œå„ä¸ªå‚æ•°çš„å«ä¹‰å¦‚ä¸‹ï¼š

å‚æ•°è§£é‡Š
batch_size: æ¯ä¸ªæ‰¹æ¬¡ä¸­çš„æ ·æœ¬æ•°é‡ã€‚è¿™æ˜¯è®­ç»ƒæ—¶æ‰€éœ€çš„æ ·æœ¬æ•°é‡ã€‚
world_size: åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„è¿›ç¨‹æ•°é‡ï¼Œè¡¨ç¤ºåœ¨å¤šä¸ªè®¾å¤‡ä¸Šå¹¶è¡Œè®­ç»ƒçš„æ€»è¿›ç¨‹æ•°ã€‚å®ƒé€šå¸¸ç­‰äºå¯ç”¨GPUçš„æ•°é‡ã€‚
dataset: å¯é€‰å‚æ•°ï¼Œè¡¨ç¤ºä½¿ç”¨çš„è®­ç»ƒæ•°æ®é›†ã€‚å¦‚æœæœªæä¾›ï¼Œåˆ™éœ€è¦æä¾› lengthsã€‚
lengths: å¯é€‰å‚æ•°ï¼Œè¡¨ç¤ºæ¯ä¸ªæ ·æœ¬çš„é•¿åº¦åˆ—è¡¨ã€‚è¿™æœ‰åŠ©äºç¡®å®šå¦‚ä½•åˆ†ç»„æ ·æœ¬ï¼Œä»¥ä¾¿åœ¨æ¯ä¸ªæ‰¹æ¬¡ä¸­å°½é‡ä½¿æ ·æœ¬é•¿åº¦ç›¸è¿‘ã€‚
model_input_name: å¯é€‰å‚æ•°ï¼ŒæŒ‡å®šæ•°æ®é›†ä¸­è¾“å…¥ç‰¹å¾çš„åç§°ï¼ˆä¾‹å¦‚ï¼Œinput_idsï¼‰ã€‚å¦‚æœæ•°æ®é›†æ˜¯å­—å…¸å½¢å¼çš„å¹¶ä¸”æ²¡æœ‰æä¾›æ­¤å‚æ•°ï¼Œå°†è‡ªåŠ¨æ¨æ–­ã€‚
generator: å¯é€‰å‚æ•°ï¼Œæ§åˆ¶éšæœºæ•°ç”Ÿæˆçš„ç§å­ï¼Œä»¥ä¾¿äºåœ¨å–æ ·æ—¶ä¿æŒä¸€è‡´æ€§ã€‚
world_size çš„ä½œç”¨
world_size ç”¨äºè®¡ç®—æ¯ä¸ªâ€œè¶…çº§æ‰¹æ¬¡â€ï¼ˆmegabatchï¼‰çš„å¤§å°ã€‚è¶…çº§æ‰¹æ¬¡æ˜¯å°†å¤šä¸ªå°æ‰¹æ¬¡ç»„åˆåœ¨ä¸€èµ·ï¼Œä»¥ä¾¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­æ›´æœ‰æ•ˆåœ°å¤„ç†æ•°æ®ã€‚å…·ä½“æ¥è¯´ï¼Œè¶…çº§æ‰¹æ¬¡çš„å¤§å°ä¸ºï¼š
megabatch_size = world_size * batch_size
```

## 13.triu_ ç”¨æ³• 



# except AttributeError å’Œ raise ValueError çš„åŒºåˆ«

except AttributeError å’Œ raise ValueError æ˜¯ä¸¤ç§ä¸åŒçš„å¼‚å¸¸å¤„ç†æœºåˆ¶ï¼Œåœ¨ Python ä¸­ç”¨äºå¤„ç†é”™è¯¯æƒ…å†µã€‚å®ƒä»¬çš„ä¸»è¦åŒºåˆ«å¦‚ä¸‹ï¼š

except AttributeError
ç”¨é€”ï¼šç”¨äºæ•è·ç‰¹å®šç±»å‹çš„å¼‚å¸¸ï¼ˆåœ¨è¿™ä¸ªä¾‹å­ä¸­æ˜¯ AttributeErrorï¼‰ã€‚
åœºæ™¯ï¼šå½“ä½ å°è¯•è®¿é—®ä¸€ä¸ªå¯¹è±¡ä¸å­˜åœ¨çš„å±æ€§æ—¶ï¼Œä¼šå¼•å‘ AttributeErrorã€‚
ç”¨æ³•ï¼šé€šå¸¸åœ¨ try å—å†…ä½¿ç”¨ï¼Œä»¥ä¾¿å¤„ç†å¯èƒ½å‘ç”Ÿçš„é”™è¯¯ã€‚
ç¤ºä¾‹

```
class Person:
    name = "Alice"

person = Person()
try:
    # å°è¯•è®¿é—®ä¸å­˜åœ¨çš„å±æ€§
    age = person.age
except AttributeError:
    print("å±æ€§ 'age' ä¸å­˜åœ¨")  # å¤„ç†å¼‚å¸¸
```

raise ValueError
ç”¨é€”ï¼šç”¨äºä¸»åŠ¨å¼•å‘ä¸€ä¸ªå¼‚å¸¸ï¼Œé€šå¸¸åœ¨ä»£ç ä¸­æ£€æµ‹åˆ°ä¸åˆé€‚çš„å€¼æ—¶ã€‚
åœºæ™¯ï¼šåœ¨å‡½æ•°æˆ–æ–¹æ³•ä¸­ï¼Œå½“å‚æ•°å€¼ä¸ç¬¦åˆé¢„æœŸæ—¶ï¼Œå¯ä»¥ä½¿ç”¨ raise ValueError æ¥é€šçŸ¥è°ƒç”¨è€…ã€‚
ç”¨æ³•ï¼šé€šå¸¸ç”¨äºéªŒè¯è¾“å…¥æˆ–çŠ¶æ€ï¼Œå¹¶åœ¨ä¸ç¬¦åˆæ¡ä»¶æ—¶å¼•å‘é”™è¯¯ã€‚
ç¤ºä¾‹

```
def set_age(age):
    if age < 0:
        raise ValueError("å¹´é¾„ä¸èƒ½ä¸ºè´Ÿæ•°")  # ä¸»åŠ¨å¼•å‘å¼‚å¸¸
    print(f"å¹´é¾„è®¾ç½®ä¸º: {age}")
try:
    set_age(-1)
except ValueError as e:
    print(e)  # è¾“å‡º: å¹´é¾„ä¸èƒ½ä¸ºè´Ÿæ•°
```







# llavaå¤šæ¨¡æ€

https://github.com/yuanzhoulvpi2017/zero_nlp

# segment-anything



æŠ å›¾ æ¨¡å‹

https://huggingface.co/schirrmacher/ormbg

Open Remove Background Model (ormbg)

This model is similar to [RMBG-1.4](https://huggingface.co/briaai/RMBG-1.4), but with open training data/process and commercially free to use.

## Inference

```
python utils/inference.py
```

## Training

The model was trained on a NVIDIA GeForce RTX 4090 (10.000 iterations) with the [Human Segmentation Dataset](https://huggingface.co/datasets/schirrmacher/humans) which was created with [LayerDiffuse](https://github.com/layerdiffusion/LayerDiffuse) and [IC-Light](https://github.com/lllyasviel/IC-Light).

## Want to train your own model?

Checkout *Highly Accurate Dichotomous Image Segmentation* code:

```
git clone https://github.com/xuebinqin/DIS.git
cd DIS
```

Follow the installation instructions on https://github.com/xuebinqin/DIS?tab=readme-ov-file#1-clone-this-repo. Download or create some data ([like this](https://huggingface.co/datasets/schirrmacher/humans)) and place it into the DIS project folder.

I am using the folder structure:

- training/im (images)
- training/gt (ground truth)
- validation/im (images)
- validation/gt (ground truth)

Apply this git patch for setting the right paths and remove normalization of images:

```
git apply dis-repo.patch
```

Start training:

```
cd IS-Net
python train_valid_inference_main.py
```

Export to ONNX (modify paths if needed):

```
python utils/pth_to_onnx.py
```



# å¤šæ¨¡æ€çš„ä¸€äº›åŒ…

torchscale æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ä¸”çµæ´»çš„åº“ï¼Œé€‚åˆç ”ç©¶äººå‘˜å’Œå¼€å‘è€…åœ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒå’Œå¤šæ¨¡æ€ä»»åŠ¡ä¸­ä½¿ç”¨

torchscale æ˜¯ä¸€ä¸ªåŸºäº PyTorch çš„åº“ï¼Œæ—¨åœ¨æ”¯æŒå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†ï¼Œç‰¹åˆ«æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚ä»¥ä¸‹æ˜¯å¯¹ torchscale çš„ä¸€äº›ä¸»è¦ç‰¹ç‚¹å’ŒåŠŸèƒ½çš„ä»‹ç»ï¼š

1. é«˜æ•ˆçš„æ¨¡å‹æ¶æ„
torchscale æä¾›äº†ä¸€äº›ä¼˜åŒ–çš„æ¨¡å‹æ¶æ„ï¼ŒåŒ…æ‹¬ Transformer å’Œå…¶ä»–å¤§è§„æ¨¡ç½‘ç»œã€‚è¿™äº›æ¶æ„ç»è¿‡ç²¾å¿ƒè®¾è®¡ï¼Œä»¥æé«˜è®­ç»ƒæ•ˆç‡å’Œæ¨ç†é€Ÿåº¦ã€‚

2. å¤šæ¨¡æ€æ”¯æŒ
è¯¥åº“ä¸“æ³¨äºå¤šæ¨¡æ€å­¦ä¹ ï¼Œèƒ½å¤Ÿå¤„ç†æ–‡æœ¬ã€å›¾åƒå’Œå…¶ä»–æ•°æ®ç±»å‹çš„è¾“å…¥ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨å¤šæ¨¡æ€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚

3. çµæ´»çš„ç»„ä»¶
torchscale å…è®¸ç”¨æˆ·çµæ´»åœ°ç»„åˆä¸åŒçš„ç»„ä»¶ï¼Œå¦‚åµŒå…¥ã€ç¼–ç å™¨å’Œè§£ç å™¨ç­‰ã€‚è¿™ç§æ¨¡å—åŒ–çš„è®¾è®¡ä½¿å¾—æ„å»ºå’Œå®éªŒæ–°çš„æ¨¡å‹å˜å¾—æ›´åŠ å®¹æ˜“ã€‚

## 1 .BEiT3

```
from torchscale.architecture import encoder
from torchscale.component.embedding import TextEmbedding,PositionalEmbedding,VisionEmbedding
from torchscale.component.multiway_network import MutliwayEmbedding
from torchscale.model.BEiT3 import BEiT3
from torchscale.architecture.config import EncoderConfig


Encoder: å®šä¹‰ç¼–ç å™¨çš„æ¨¡å—ã€‚
PositionalEmbedding, TextEmbedding, VisionEmbedding: ä¸åŒç±»å‹çš„åµŒå…¥ç±»ï¼Œç”¨äºå¤„ç†æ–‡æœ¬å’Œè§†è§‰æ•°æ®ã€‚
MutliwayEmbedding: å…è®¸å¤šæ¨¡æ€åµŒå…¥çš„ç»„åˆã€‚
args.multiway: ç¡®ä¿å¯ç”¨å¤šæ¨¡æ€å¤„ç†ã€‚
args.vocab_size: ç¡®ä¿è¯æ±‡è¡¨å¤§å°å¤§äº0ã€‚
args.share_encoder_input_output_embed: ç¡®ä¿è¾“å…¥è¾“å‡ºåµŒå…¥ä¸å…±äº«ã€‚
TextEmbedding: æ–‡æœ¬åµŒå…¥å±‚ï¼Œä½¿ç”¨ç»™å®šçš„è¯æ±‡è¡¨å¤§å°å’ŒåµŒå…¥ç»´åº¦ã€‚
VisionEmbedding: è§†è§‰åµŒå…¥å±‚ï¼Œå¤„ç†å›¾åƒè¾“å…¥ï¼Œæ”¯æŒæ©ç å’Œ CLS tokenã€‚

class BEiT3(nn.Module):
    def __init__(self, args, **kwargs):
        super().__init__()
        self.args = args
        assert args.multiway
        assert args.vocab_size > 0
        assert not args.share_encoder_input_output_embed
        self.text_embed = TextEmbedding(args.vocab_size, args.encoder_embed_dim)
        self.vision_embed = VisionEmbedding(
            args.img_size,
            args.patch_size,
            args.in_chans,
            args.encoder_embed_dim,
            contain_mask_token=True,
            prepend_cls_token=True,
        )
        # being consistent with Fairseq, which starts from 2 for position embedding
        embed_positions = MutliwayEmbedding(
            modules=[
                PositionalEmbedding(self.vision_embed.num_position_embeddings() + 2, args.encoder_embed_dim),
                PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim),
            ],
            dim=1,
        )
        self.encoder = Encoder(
            args,
            embed_tokens=None,
            embed_positions=embed_positions,
            output_projection=None,
            is_encoder_decoder=False,
        )
Encoder: åˆ›å»ºç¼–ç å™¨å¯¹è±¡ï¼Œä½¿ç”¨ä¹‹å‰å®šä¹‰çš„ä½ç½®åµŒå…¥
    def forward(
        self,
        textual_tokens=None,
        visual_tokens=None,
        text_padding_position=None,
        attn_mask=None,
        vision_masked_position=None,
        incremental_state=None,
        positions=None,
    ):
        assert textual_tokens is not None or visual_tokens is not None
		å¤„ç†è§†è§‰è¾“å…¥
        if textual_tokens is None:
            x = self.vision_embed(visual_tokens, vision_masked_position)
            encoder_padding_mask = None
            multiway_split_position = -1
        å¤„ç†æ–‡æœ¬è¾“å…¥
        elif visual_tokens is None:
            x = self.text_embed(textual_tokens)
            encoder_padding_mask = text_padding_position
            multiway_split_position = 0
        åŒæ—¶å¤„ç†æ–‡æœ¬å’Œè§†è§‰è¾“å…¥
        else:
            x1 = self.vision_embed(visual_tokens, vision_masked_position)
            multiway_split_position = x1.size(1)
            x2 = self.text_embed(textual_tokens)
            x = torch.cat([x1, x2], dim=1)

            if text_padding_position is not None:
                encoder_padding_mask = torch.cat(
                    [
                        torch.zeros(x1.shape[:-1]).to(x1.device).bool(),
                        text_padding_position,
                    ],
                    dim=1,
                )
            else:
                encoder_padding_mask = None

        encoder_out = self.encoder(
            src_tokens=None,
            encoder_padding_mask=encoder_padding_mask,
            attn_mask=attn_mask,
            token_embeddings=x,
            multiway_split_position=multiway_split_position,
            incremental_state=incremental_state,
            positions=positions,
        )
        encoder_out["multiway_split_position"] = multiway_split_position

        return encoder_out
```

1. **å®ä¾‹åŒ–æ¨¡å‹**:

```
args = {
    'multiway': True,
    'vocab_size': 30522,     # ç¤ºä¾‹è¯æ±‡è¡¨å¤§å°
    'encoder_embed_dim': 768, # åµŒå…¥ç»´åº¦
    'img_size': 224,         # å›¾åƒå¤§å°
    'patch_size': 16,        # å›¾åƒå—å¤§å°
    'in_chans': 3,           # è¾“å…¥é€šé“
    'max_source_positions': 512, # æœ€å¤§æºä½ç½®
    'share_encoder_input_output_embed': False,
}
textual_tokens = torch.randint(0, 30522, (2, 10))  # ç¤ºä¾‹æ–‡æœ¬è¾“å…¥
visual_tokens = torch.randn(2, 3, 224, 224)         # ç¤ºä¾‹è§†è§‰è¾“å…¥
text_padding_position = torch.zeros((2, 10), dtype=torch.bool) # æ–‡æœ¬å¡«å……ä½ç½®
vision_masked_position = None                          # è§†è§‰æ©ç ä½ç½®

# è°ƒç”¨å‰å‘ä¼ æ’­
output = beit3_model(
    textual_tokens=textual_tokens,
    visual_tokens=visual_tokens,
    text_padding_position=text_padding_position,
    vision_masked_position=vision_masked_position
)
```

- å½“ `multiway` è®¾ç½®ä¸º `True` æ—¶ï¼Œæ¨¡å‹èƒ½å¤Ÿæ¥å—æ–‡æœ¬å’Œè§†è§‰æ•°æ®çš„åŒæ—¶è¾“å…¥ï¼Œè¿™å¯¹äºè®¸å¤šä»»åŠ¡ï¼ˆå¦‚è§†è§‰é—®ç­”å’Œå›¾æ–‡åŒ¹é…ï¼‰è‡³å…³é‡è¦ã€‚

```
import math
import torch
import torch.nn as nn
from timm.models.layers import trunc_normal_ as __call_trunc_normal_
from timm.models.registry import register_model

from torchscale.model.BEiT3 import BEiT3
from torchscale.architecture.config import EncoderConfig


def trunc_normal_(tensor, mean=0., std=1.):
    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)


def _get_base_config(
        img_size=224, patch_size=16, drop_path_rate=0,
        checkpoint_activations=None, mlp_ratio=4, vocab_size=64010, **kwargs
):
    return EncoderConfig(
        img_size=img_size, patch_size=patch_size, vocab_size=vocab_size,
        multiway=True,layernorm_embedding=False, normalize_output=True,
        no_output_layer=True,
        drop_path_rate=drop_path_rate, encoder_embed_dim=768,
        encoder_attention_heads=12,
        encoder_ffn_embed_dim=int(768 * mlp_ratio), encoder_layers=12,
        checkpoint_activations=checkpoint_activations,
    )


def _get_large_config(
        img_size=224, patch_size=16, drop_path_rate=0,
        checkpoint_activations=None, mlp_ratio=4, vocab_size=64010, **kwargs
):
    return EncoderConfig(
        img_size=img_size, patch_size=patch_size, vocab_size=vocab_size, multiway=True,
        layernorm_embedding=False, normalize_output=True, no_output_layer=True,
        drop_path_rate=drop_path_rate, encoder_embed_dim=1024, encoder_attention_heads=16,
        encoder_ffn_embed_dim=int(1024 * mlp_ratio), encoder_layers=24,
        checkpoint_activations=checkpoint_activations,
    )


class BEiT3Wrapper(nn.Module):
    def __init__(self, args, **kwargs):
        super().__init__()
        self.args = args
        self.beit3 = BEiT3(args)
        self.apply(self._init_weights)
        self.mim_head = nn.Linear(1024, 8192)
        self.num_img_patches = self.beit3.vision_embed.num_position_embeddings()
        self.hidden_size = args.encoder_embed_dim

    def fix_init_weight(self):
        def rescale(param, layer_id):
            param.div_(math.sqrt(2.0 * layer_id))

        for layer_id, layer in enumerate(self.blocks):
            rescale(layer.attn.proj.weight.data, layer_id + 1)
            rescale(layer.mlp.fc2.weight.data, layer_id + 1)

    def get_num_layers(self):
        return self.beit3.encoder.num_layers

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'pos_embed', 'cls_token', 'beit3.encoder.embed_positions.A.weight', 'beit3.vision_embed.cls_token', 'logit_scale'}

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, pixel_values, query_embed=None):
        B = pixel_values.size(0)
        dtype = self.beit3.vision_embed.proj.weight.dtype
        pixel_values = pixel_values.to(dtype)
        token_embeddings = self.beit3.vision_embed(pixel_values)
        multiway_split_position = -1
        if query_embed is not None:
            query_embed = torch.stack([query_embed] * B)
            multiway_split_position = token_embeddings.size(1)
            token_embeddings = torch.cat([token_embeddings, query_embed], dim=1)

        outputs = self.beit3.encoder(
            src_tokens=None,
            token_embeddings=token_embeddings,
            multiway_split_position=multiway_split_position
        )
        vision_hidden_states = outputs["encoder_out"]
        if query_embed is not None:
            vision_hidden_states = vision_hidden_states[:, self.num_img_patches:]
        return vision_hidden_states
```



# instruct-pix2pix



# stable_signature æ°´å°æ¨¡å‹ 



è¿›å…¥V100-32GæœåŠ¡å™¨

conda activate ame

python app.py å¼€å¯æœåŠ¡ æ˜¾ç¤º å¦‚ä¸‹

```python
python app.py
 * Serving Flask app 'app'
 * Debug mode: on
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://192.168.1.42:8080
Press CTRL+C to quit
 * Restarting with stat
 * Debugger is active!
 * Debugger PIN: 913-831-270
```

è°ƒç”¨ æ¥å£ curl -X POST -F "file=@/root/cat.png" http://120.24.24.95:8080/image

curl -X POST -F "file=@/root/cat.png" http://120.24.24.95:8080/image

å“åº”æˆåŠŸ  {'message': 'image process successfully', 'filename': filename,'img':base64_str}





# ä»é›¶è®­ç»ƒ1Bä»¥ä¸‹çš„æ¨¡å‹ 

æœ€å¥½çš„å­¦ä¹ æ–¹å¼è«è¿‡äºè‡ªå·±ä»å¤´åšä¸€éã€‚å­¦ä¹ å¤§æ¨¡å‹çš„ç›¸å…³çŸ¥è¯†ä»¥æ¥ï¼Œä¸€ç›´éƒ½æƒ³ä»å¤´è‡ªå·±è®­ç»ƒä¸€ä¸ª 1B ä»¥ä¸‹çš„æ¨¡å‹ï¼Œæ„Ÿè§‰è¿™æ ·æ‰ç®—æ˜¯çœŸçš„å­¦è¿‡äº†ã€‚ä¸è¿‡ä»¥æ‰‹å¤´çš„èµ„æºï¼Œä¹Ÿåªèƒ½ç©ç©å„¿è¿·ä½ çš„å°æ¨¡å‹äº†ã€‚æœ€è¿‘åœ¨ç½‘ä¸Šæœäº†ä¸å°‘èµ„æ–™ï¼Œä¸»è¦æ˜¯ GitHub ä¸Šçš„ä»“åº“å’Œ Arxiv ä¸Šçš„ paperï¼Œé¡ºä¾¿è®°å½•åœ¨è¿™é‡Œã€‚

**https://github.com/karpathy/nanoGPT**

nanoGPT æ˜¯ karpathy å¤§ç¥å†™çš„ GPT-2 æœ€å°å®ç°ã€‚éº»é›€è™½å°ï¼Œäº”è„ä¿±å…¨ã€‚GPT-2 æ˜¯å¤§æ¨¡å‹çš„é¼»ç¥–ï¼Œå¾ˆå¤šè®ºæ–‡éƒ½ä»¥ nanoGPT ä¸ºåŸºç¡€é­”æ”¹æˆ–è€…ä½œä¸º baselineã€‚nanoGPT å…±æœ‰ 0.1B åˆ° 1.5B å››ä¸ªå¤§å°ä¸åŒçš„ç‰ˆæœ¬ã€‚

è®­ç»ƒ GPT-2 çš„æ–‡ç« æœ‰å¾ˆå¤šï¼Œè™½ç„¶æœ‰äº›æ¯”è¾ƒè€çš„ï¼Œä½†æ˜¯ä¹Ÿå€¼å¾—å‚è€ƒï¼Œåˆ—ä¸¾ä¸€äº›æˆ‘è§‰å¾—ä¸é”™çš„ï¼š

# ä»é›¶è®­ç»ƒclipå›¾æ–‡æ¨¡å‹

clip çš„ä»£ç è¯¦è§£   https://juejin.cn/post/7102007827161415710



# ä»é›¶è®­ç»ƒblip2 

https://blog.csdn.net/Faded1128/article/details/132266298 blip2è®²è§£ 

BLIP2 

ç†è®ºæ ¸å¿ƒ

1. å†»ç»“è§†è§‰ç¼–ç å™¨ï¼Œè®­ç»ƒQ-formerï¼Œå­¦ä¹ è§†è§‰-æ–‡æœ¬çš„è¡¨å¾
2. å†»ç»“LLMï¼Œè®­ç»ƒè§†è§‰åˆ°æ–‡æœ¬çš„ç”Ÿæˆ



1. Vision-and-Language Representation Learning
ä¸ºäº†è®­ç»ƒ Q-Former æ¥å®ç° queries èƒ½å¤ŸæŠ½å–å‡ºåŒ…å«å¯¹åº”æ–‡æœ¬ä¿¡æ¯çš„ visual representationï¼ŒåŒæ—¶ä¼˜åŒ–ä¸‰ä¸ªæŸå¤±å‡½æ•°ï¼š

1ã€Image-Text Contrastive Learning (ITC)ï¼šå­¦ä¹ å°† image ç‰¹å¾å’Œ text ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä½¿ä»–ä»¬çš„äº¤äº’ä¿¡æ¯æœ€å¤§åŒ–
Â· å®ç°æ–¹å¼ï¼šä½¿ç”¨å¯¹æ¯”å­¦ä¹ ï¼Œå­¦ä¹  image-text çš„ç›¸ä¼¼åº¦ï¼Œpositive pairs çš„ç›¸ä¼¼æ€§å¤§äº negative pairs çš„ç›¸ä¼¼æ€§

Â· t ï¼štext transformer çš„ [CLS] token çš„è¾“å‡º representationï¼ŒåŒ…å«å¤šä¸ªè¾“å‡ºç¼–ç ï¼ˆä¸€ä¸ª query å¯¹åº”ä¸€ä¸ªç¼–ç ï¼‰

Â· Zï¼šimage transformer çš„ è¾“å‡º query representation

è®¡ç®—æ¯ä¸ª query output å’Œ t çš„ ç›¸ä¼¼åº¦ï¼Œé€‰æ‹©ç›¸ä¼¼åº¦æœ€é«˜çš„ä½œä¸º image-text similarityï¼Œä¸ºäº†é¿å…ä¿¡æ¯æ³„éœ²ï¼Œä½œè€…è¿˜ä½¿ç”¨äº†å•æ¨¡æ€ self-attention maskï¼Œqueries å’Œ text ä¸èƒ½çœ‹åˆ°å¯¹æ–¹

ç”±äº image encoder æ˜¯è¢«å†»ç»“çš„ï¼Œæ‰€ä»¥ç›¸æ¯”äºç«¯åˆ°ç«¯è®­ç»ƒï¼Œå†»ç»“çš„æ–¹å¼èƒ½å¤Ÿåœ¨æ¯ä¸ª GPU ä¸Šæ”¾æ›´å¤šçš„æ ·æœ¬ï¼Œæ‰€ä»¥æœ¬æ–¹æ³•ä½¿ç”¨çš„ in-batch negatives è€Œé momentum queueï¼ˆBLIPï¼‰

2ã€Image-grounded Text Generation (ITG)ï¼šè®­ç»ƒ Q-Former æ¥ç”Ÿæˆ textï¼Œè¾“å…¥çš„å›¾åƒä½œä¸ºæ¡ä»¶
ç”±äº Q-Former ä¸å…è®¸ frozen image encoder å’Œ text image encoder çš„ç›´æ¥äº¤äº’ï¼Œä½†ç”Ÿæˆ text çš„åŸºç¡€ä¿¡æ¯æ˜¯æ¥æºäº query æŠ½å–åˆ°çš„å›¾åƒä¿¡æ¯ã€‚æ‰€ä»¥ï¼Œquery éœ€è¦èƒ½å¤ŸæŠ½å–å…³äº text å†…å®¹çš„æ‰€æœ‰çš„å›¾åƒä¿¡æ¯ã€‚ä½œè€…ä½¿ç”¨å¤šæ¨¡æ€ causal self-attention mask æ¥æ§åˆ¶ query-text çš„äº¤äº’ï¼Œç±»ä¼¼äº UniLMï¼Œquery å¯ä»¥å’Œå…¶ä»– query ä»¥åŠå‡ºç°åœ¨å®ƒå‰é¢çš„ text token è¿›è¡Œäº¤æ¢ï¼ŒåŒæ—¶ä¹Ÿæ˜¯è¦ [DEC] token æ›¿æ¢äº† [CLS] token ä½œä¸ºç¬¬ä¸€ä¸ª text toden æ¥æ ‡è®° decoding ä»»åŠ¡ã€‚

3ã€Image-Text Matching (ITM)ï¼šç”¨äºå­¦ä¹  image-text ä¹‹é—´æ›´ç»†ç²’åº¦çš„å¯¹é½
        è¯¥ä»»åŠ¡æ—¶ä¸€ä¸ªäºŒå€¼åˆ†ç±»ä»»åŠ¡ï¼Œç”¨äºé¢„æµ‹ä¸€ä¸ª image-text pairs æ˜¯ positiveï¼ˆmatchedï¼‰è¿˜æ˜¯ negativeï¼ˆunmatchedï¼‰ã€‚ä½œè€…ä½¿ç”¨ bi-directional self-attention maskï¼Œè¯¥ mask çš„å½¢å¼å…è®¸æ‰€æœ‰ query å’Œ text è¿›è¡Œäº¤äº’ï¼Œè¾“å‡º query embedding Z èƒ½å¤Ÿæ•æ‰å¤šæ¨¡æ€ä¿¡æ¯ã€‚å°†æ¯ä¸ªè¾“å‡º query embedding è¾“å…¥äºŒåˆ†ç±»çš„çº¿æ€§åˆ†ç±»å™¨ï¼Œæ¥è·å¾—ä¸€ä¸ª logitï¼Œå¹¶ä¸”å°† logit å¹³å‡åä½œä¸ºè¾“å‡ºçš„ matching scoreã€‚æ­¤å¤–ï¼Œä½œè€…ä¹Ÿä½¿ç”¨äº†è´Ÿéš¾ä¾‹æŒ–æ˜ï¼Œæ¥æå–å‡ºéš¾çš„ negative pairs

##### Image-Text Contrastive Learning (ITC)ï¼šå­¦ä¹ å°† image ç‰¹å¾å’Œ text ç‰¹å¾è¿›è¡Œå¯¹é½ï¼Œä½¿ä»–ä»¬çš„äº¤äº’ä¿¡æ¯æœ€å¤§åŒ–

##### Image-grounded Text Generation (ITG)ï¼šè®­ç»ƒ Q-Former æ¥ç”Ÿæˆ textï¼Œè¾“å…¥çš„å›¾åƒä½œä¸ºæ¡ä»¶

##### Image-Text Matching (ITM)ï¼šç”¨äºå­¦ä¹  image-text ä¹‹é—´æ›´ç»†ç²’åº¦çš„å¯¹é½

CLIPæ˜¯Contrastive Pre-trainingå¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒï¼ˆå›¾æ–‡å¯¹æ¯”å­¦ä¹ ï¼Œæœ¬è´¨æ˜¯`understandingç†è§£`ï¼Œ**åªæœ‰imageå’Œtextçš„Encoderçš„ç»“æ„**ï¼‰

BLIPæ˜¯å¤šä»»åŠ¡é¢„è®­ç»ƒï¼ˆå›¾æ–‡å¯¹æ¯”å­¦ä¹ +å›¾æ–‡åŒ¹é…+æ–‡æœ¬ç”Ÿæˆï¼šæœ¬è´¨æ˜¯understandingç†è§£+generationç”Ÿæˆï¼‰ã€‚ä¸ä»…æœ‰imageå’Œtextçš„Encoderï¼Œè¿˜æœ‰textçš„Decoderã€‚å…¶ä¸­textçš„ç¼–ç å™¨å’Œè§£ç å™¨éƒ½æ˜¯åŸºäºimageçš„ï¼ˆcross attentionæ³¨å…¥äº†ï¼‰ã€‚
BLIPï¼ˆBootstraping language Image Pre-trainingçš„åŸºæœ¬æ€æƒ³æ˜¯ åŒæ—¶å…¼é¡¾`å›¾æ–‡ç†è§£`å’Œ`å›¾æ–‡ç”Ÿæˆ`çš„å¤šæ¨¡æ€æ¨¡å‹ï¼ˆMultimodal mixture of Encoder-Decoderï¼‰ï¼ŒåŒæ—¶åœ¨ä¸‰ä¸ªè§†è§‰è¯­è¨€ç›®æ ‡ä¸Šè”åˆé¢„è®­ç»ƒï¼šå›¾åƒæ–‡æœ¬å¯¹æ¯”å­¦ä¹ ITCã€å›¾åƒæ–‡æœ¬åŒ¹é…ITMã€å›¾åƒæ¡ä»¶è¯­è¨€å»ºæ¨¡LMï¼›

# ä¸»æµVLMåŸç†æ·±å…¥åˆ¨æï¼ˆCLIPï¼ŒBLIPï¼ŒBLIP2ï¼ŒFlamingoï¼ŒLLaVAï¼ŒMiniCPTï¼ŒInstructBLIPï¼ŒmPLUG-owlï¼‰

https://blog.csdn.net/weixin_54338498/article/details/135258723

æ¨¡å‹ç»“æ„ï¼š

Image Encoderï¼ˆViTï¼‰ï¼šå›¾åƒæ‰“æˆpatcheså—åè¿›è¡Œç¼–ç ï¼Œå¢åŠ cls tokenæ¥è®°å½•imageçš„å…¨å±€çš„ç‰¹å¾ï¼ˆä½œç”¨ç±»ä¼¼ä½ç½®ç¼–ç ï¼Œä¿ç•™patchesçš„ç©ºé—´ç‰¹å¾ï¼‰ã€‚
Text Encoderï¼ˆBERTï¼‰ï¼šå¯¹å¥å­è¿›è¡Œç¼–ç ï¼Œå¢åŠ cls tokenè®°å½•textçš„å…¨å±€ç‰¹å¾ã€‚
Image-grounded Text Encoderï¼šåœ¨æ–‡æœ¬embeddingä¸­æ³¨å…¥äº†å›¾åƒç‰¹å¾ï¼Œé€šè¿‡åœ¨self-attentionå’ŒFFNä¸­é—´å¢åŠ ä¸€å±‚cross-attentionæ¥å¯¹é½text-encoderå’Œimg-encoderçš„ç‰¹å¾ã€‚
Image-grounded Text Decoderï¼šç”¨causal self-attentionå±‚ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰ä»£æ›¿äº†åŒå‘è‡ªæ³¨æ„åŠ›å±‚ï¼ˆå»ºç«‹å½“å‰è¾“å…¥tokençš„è¡¨è¾¾ï¼‰ã€å’Œå·¦è¾¹çš„encoderå…±äº«é™¤äº†self-attentionä¹‹å¤–çš„å±‚çš„å‚æ•°ã€‘ã€‚
è®­ç»ƒlossï¼šé¢„è®­ç»ƒé˜¶æ®µåŒæ—¶ä¼˜åŒ–3ä¸ªlossé¡¹ï¼Œæ¯ä¸ªå›¾æ–‡å¯¹åªè¿‡1æ¬¡vision-transormer(ç®—åŠ›æ¶ˆè€—è¾ƒå¤§)ï¼Œè¿‡3æ¬¡text-transormer

Image-Text Contrastive Loss (ITC)ç†è§£åŠŸèƒ½ï¼šï¼ˆè¾“å‡ºå›¾æ–‡çš„ç›¸ä¼¼åº¦ï¼‰ä¼˜åŒ–vision-transormer + text-transormerï¼Œè®©åŒ¹é…çš„å›¾æ–‡å¯¹æœ‰è¾ƒé«˜ç›¸ä¼¼åº¦çš„è¡¨è¾¾ï¼ˆç”¨äº†soft labelsï¼‰ï¼Œå¤šæ¨¡æ€ä¸­çš„ç»å…¸loss->ä½¿å…¶äº’ä¿¡æ¯æœ€å¤§åŒ–ï¼›
Image-Text Matching Loss (ITM)ç†è§£åŠŸèƒ½ ï¼šï¼ˆè¾“å‡ºå›¾æ–‡æ˜¯å¦åŒ¹é…çš„True/Falseï¼‰ä¼˜åŒ–Image-grounded text encoderï¼Œå­¦ä¹ å›¾æ–‡çš„ç»†ç²’åº¦åŒ¹é…çš„äºŒåˆ†ç±»ï¼Œé‡‡ç”¨äº†hard negative mining strategyï¼ˆå°†ITCä»»åŠ¡ä¸­å®¹æ˜“åˆ¤æ–­é”™çš„æ ·æœ¬å½“ä½œhard negative sampleï¼‰ï¼›
Language Modeling Loss (LM)ç”ŸæˆåŠŸèƒ½ï¼šï¼ˆç”Ÿæˆå›¾åƒçš„æ–‡æœ¬æè¿°ï¼‰ä¼˜åŒ–image-grounded text decoderï¼Œå­¦ä¹ å¦‚ä½•ä»ç»™å®šå›¾ç”Ÿæˆè¿è´¯çš„æ–‡æœ¬æè¿°ï¼Œé‡‡ç”¨äº¤å‰ç†µä»£ä»·å‡½æ•°ä»¥è‡ªå›å½’æ–¹å¼æœ€å¤§åŒ–å¯¹åº”æ–‡æœ¬æ¦‚ç‡ã€‚



## Q-Formeræ ¸å¿ƒ

BLIP2ä»£ç  

https://blog.csdn.net/RandyHan/article/details/134804888  blip2 ä»£ç  

[GitHub - kyegomez/zeta: Build high-performance AI models with modular building blocks](https://github.com/kyegomez/zeta)

```
pip3 install -U zetascale

import torch

from zeta.nn import FlashAttention

q = torch.randn(2, 4, 6, 8)
k = torch.randn(2, 4, 10, 8)
v = torch.randn(2, 4, 10, 8)

attention = FlashAttention(causal=False, dropout=0.1, flash=True)
output = attention(q, k, v)

print(output.shape)
```

# LLaVA Large Language and Vision Assistant

å¤šæ¨¡æ€å¤§æ¨¡å‹LLaVAæ¨¡å‹è®²è§£â€”â€”transformersæºç è§£è¯»  https://www.bilibili.com/video/BV1nw4m1S7nZ/

å›¾è§£+ debugå½¢å¼ï¼Œä»‹ç»llavaæ¨¡å‹åŸç†ï¼š 

1. å°±æ˜¯ä½¿ç”¨embeddingå°†æ–‡æœ¬è½¬æ¢æˆ text_embedï¼› 
2. ä½¿ç”¨vitã€mlpå°†å›¾åƒè½¬æ¢æˆimage_embedï¼› 
3. ç„¶ååœ¨æŒ‡å®šçš„ä½ç½®ï¼Œå°†text_embedå’Œimage_embedæ‹¼æ¥åœ¨ä¸€èµ·ï¼›
4. æœ€åå¾—åˆ°ä¸€ä¸ªå˜æˆhidden_states,ä¼ é€’ç»™æ–‡æœ¬æ¨¡å‹ï¼Œæœ€åè¾“å‡ºå¯¹åº”çš„å†…å®¹ã€‚





https://github.com/aurelio-labs/semantic-router

https://github.com/modelscope/modelscope-agent/blob/master/modelscope_agent/rag/README_zh.md

https://github.com/e2b-dev/awesome-ai-agents

https://zhuanlan.zhihu.com/p/707459943

https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/markdown/

https://blog.csdn.net/u012960155/article/details/132658756





curl -X POST -H "Content-Type: application/json" -d '{"data": """å›¾è§£+ debugå½¢å¼ï¼Œä»‹ç»llavaæ¨¡å‹åŸç†ï¼š 

1. å°±æ˜¯ä½¿ç”¨embeddingå°†æ–‡æœ¬è½¬æ¢æˆ text_embedï¼› 
2. ä½¿ç”¨vitã€mlpå°†å›¾åƒè½¬æ¢æˆimage_embedï¼› 
3. ç„¶ååœ¨æŒ‡å®šçš„ä½ç½®ï¼Œå°†text_embedå’Œimage_embedæ‹¼æ¥åœ¨ä¸€èµ·ï¼›
4. æœ€åå¾—åˆ°ä¸€ä¸ªå˜æˆhidden_states,ä¼ é€’ç»™æ–‡æœ¬æ¨¡å‹ï¼Œæœ€åè¾“å‡ºå¯¹åº”çš„å†…å®¹ã€‚

 """}' http://192.168.1.42:5010/generate_QA





curl -X POST -H "Content-Type: application/json" -d '{"data": "ä¼ é€’ç»™æ–‡æœ¬æ¨¡å‹ï¼Œæœ€åè¾“å‡ºå¯¹åº”çš„å†…å®¹ã€‚"}' http://192.168.1.42:5010/generate_QA

curl -X POST -H "Content-Type: application/json" -d '{"input": "ä¼ é€’ç»™æ–‡æœ¬æ¨¡å‹ï¼Œæœ€åè¾“å‡ºå¯¹åº”çš„å†…å®¹ã€‚"}' http://192.168.1.42:5010/qa_pair



## AutoencoderKL

AutoencoderKL ç”¨äºå›¾åƒçš„ç¼–ç å’Œè§£ç ï¼Œé€šå¸¸ç”¨äºç”Ÿæˆæ¨¡å‹ä¸­ã€‚

```python
from diffusers import AutoencoderKL
# åˆå§‹åŒ– AutoencoderKL
autoencoder = AutoencoderKL.from_pretrained("model_name")
# ç¼–ç å›¾åƒ
latent = autoencoder.encode(image)
# è§£ç æ½œåœ¨è¡¨ç¤º
reconstructed_image = autoencoder.decode(latent)
```

## DDPMScheduler

DDPMScheduler æ˜¯ç”¨äºæ‰©æ•£æ¨¡å‹çš„è°ƒåº¦å™¨ï¼Œç®¡ç†ç”Ÿæˆè¿‡ç¨‹ä¸­çš„æ—¶é—´æ­¥ã€‚

```python
from diffusers import DDPMScheduler

# åˆå§‹åŒ–è°ƒåº¦å™¨
scheduler = DDPMScheduler(num_train_timesteps=1000)

# åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä½¿ç”¨è°ƒåº¦å™¨
for t in range(num_timesteps):
    noise_pred = model(latent, t)
    latent = scheduler.step(noise_pred, t, latent)
```

## UNet2DConditionModel

UNet2DConditionModel æ˜¯ç”¨äºæ¡ä»¶ç”Ÿæˆçš„ U-Net æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®æ¡ä»¶è¾“å…¥ç”Ÿæˆå›¾åƒã€‚

```python
from diffusers import UNet2DConditionModel
# åˆå§‹åŒ– UNet æ¨¡å‹
unet = UNet2DConditionModel.from_pretrained("model_name")
# è¿›è¡Œæ¡ä»¶ç”Ÿæˆ
output = unet(latent, condition)
```



## LMSDiscreteScheduler

LMSDiscreteScheduler æ˜¯å¦ä¸€ç§è°ƒåº¦å™¨ï¼Œæä¾›ä¸åŒçš„æ—¶é—´æ­¥è°ƒåº¦ç­–ç•¥ï¼Œé€‚ç”¨äºæ‰©æ•£æ¨¡å‹ã€‚

```
from diffusers import LMSDiscreteScheduler

# åˆå§‹åŒ– LMS è°ƒåº¦å™¨
lms_scheduler = LMSDiscreteScheduler(num_train_timesteps=1000)

# ä½¿ç”¨ LMS è°ƒåº¦å™¨è¿›è¡Œç”Ÿæˆ
for t in range(num_timesteps):
    noise_pred = model(latent, t)
    latent = lms_scheduler.step(noise_pred, t, latent)
```





#  å‘é‡æ£€ç´¢æœåŠ¡DashVector 

  **å‘é‡æ£€ç´¢æœåŠ¡DashVector  API-KEY**          sk-4cSXo82PTMGey2Vh61D0yLwVz6YPo67AC9BE54A5711EF83429E3A4EE25FE6

```
import dashvector

client = dashvector.Client(
    api_key='sk-4cSXo82PTMGey2Vh61D0yLwVz6YPo67AC9BE54A5711EF83429E3A4EE25FE6',
    endpoint='vrs-cn-li93uctkg0001t.dashvector.cn-hangzhou.aliyuncs.com'
)

# åˆ¤æ–­clientæ˜¯å¦åˆ›å»ºæˆåŠŸ
if client:
    print('create client success!')
```





# æ¨¡å‹æœåŠ¡çµç§¯

```
æ¨¡å‹æœåŠ¡çµç§¯  API-KEY**   sk-14a1de8e32534bc58bf398780dce94ae
```



from modelscope.hub.snapshot_download import snapshot_download

local_dir_root = "/root/autodl-tmp/models_from_modelscope" 

snapshot_download('baichuan-inc/Baichuan2-13B-Chat', cache_dir=local_dir_root)

## Agentå¾®è°ƒæœ€ä½³å®è·µ

[swift/docs/source/LLM/Agentå¾®è°ƒæœ€ä½³å®è·µ.md at main Â· modelscope/swift Â· GitHub](https://github.com/modelscope/swift/blob/main/docs/source/LLM/Agentå¾®è°ƒæœ€ä½³å®è·µ.md#å¾®è°ƒ)

## å€ŸåŠ©çŸ¥è¯†å›¾è°±å’ŒLlama-Indexå®ç°åŸºäºå¤§æ¨¡å‹çš„RAG

ssh -p 30889 root@connect.yza1.seetacloud.com

s0I4D2W1mzFI

ssh -p 30889 root@connect.yza1.seetacloud.com

s0I4D2W1mzFI

ssh -p 30889 root@connect.yza1.seetacloud.com

oP7sHDvsSG9v



ssh -p 39741 root@connect.yza1.seetacloud.com





è¿™ä¸ªæ˜¯æˆ‘ ä½¿ç”¨ Leaflet æ˜¾ç¤ºåœ°å›¾çš„ä»£ç ï¼Œæ€ä¹ˆå¦å¤–åœ¨æ˜¾ç¤ºçš„å›¾ç‰‡ä¸Šåˆ›å»ºä¸åŒçš„ç²¾çµæˆ–è€…äººç‰©è§’è‰²

è¿™ä¸ªæ˜¯æˆ‘ ä½¿ç”¨ Leaflet æ˜¾ç¤ºåœ°å›¾çš„ä»£ç ï¼Œåœ¨æ˜¾ç¤ºçš„å›¾ç‰‡ä¸Šåˆ›å»ºäº†ç²¾çµï¼Œæ€ä¹ˆä½¿å¾—ç²¾çµè¿›è¡Œè¡Œèµ°å‘¢ ï¼Œä½ å¯ä»¥ä½¿ç”¨JavaScript å’Œ WebGL ç­‰æŠ€æœ¯æ¥å®ç°

è¯·ä½ ä¸ºä¸Šé¢çš„20ä¸ªè§’è‰²å®šä¹‰è§„åˆ™ï¼Œå†³å®šå®ƒä»¬åœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ååº”ã€‚ä»¥åŠä½¿ç”¨æœ‰é™çŠ¶æ€æœºç®¡ç†è§’è‰²çš„ä¸åŒçŠ¶æ€ï¼ˆå¦‚é—²èŠã€å·¥ä½œã€ç§»åŠ¨ç­‰ï¼‰ã€‚

ä»¥ä¸‹æ˜¯ä¸º20ä¸ªè§’è‰²å®šä¹‰çš„åŸºæœ¬è¡Œä¸ºè§„åˆ™å’Œæœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰æ¨¡å‹ï¼Œä»¥ç®¡ç†å®ƒä»¬åœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ååº”ã€‚

ä»¥ä¸‹æ˜¯20ç§å†œæ‘å°é•‡è§’è‰²åŠå…¶å‘½ä»¤ã€å¹´é¾„å’Œç©¿ç€æè¿°ï¼š

### è§’è‰²åˆ—è¡¨

1. **å†œå¤«**
   - **å¹´é¾„**: 45
   - **ç©¿ç€**: è“è‰²å·¥ä½œæœï¼Œè‰å¸½ï¼Œæ©¡èƒ¶é´
   - **å‘½ä»¤**: â€œå»ç”°é‡Œæµ‡æ°´ã€‚â€

2. **è€å¦‡äºº**
   - **å¹´é¾„**: 70
   - **ç©¿ç€**: èŠ±è£™å­ï¼Œå›´è£™ï¼Œå¸ƒé‹
   - **å‘½ä»¤**: â€œå¸®æˆ‘é‡‡æ‘˜ä¸€äº›è‹¹æœã€‚â€

3. **å°å¥³å­©**
   - **å¹´é¾„**: 8
   - **ç©¿ç€**: ç²‰è‰²è¿è¡£è£™ï¼Œç™½è‰²è¢œå­ï¼Œè¿åŠ¨é‹
   - **å‘½ä»¤**: â€œæˆ‘æƒ³å»ç©ç§‹åƒï¼â€

4. **è€å¸ˆ**
   - **å¹´é¾„**: 35
   - **ç©¿ç€**: è¡¬è¡«ã€é•¿è£¤ï¼Œå¹³åº•é‹
   - **å‘½ä»¤**: â€œè¯·æŠŠä½œä¸šäº¤ä¸Šæ¥ã€‚â€

5. **å•†è´©**
   - **å¹´é¾„**: 50
   - **ç©¿ç€**: å¤¹å…‹ï¼Œå›´è£™ï¼Œæ‹–é‹
   - **å‘½ä»¤**: â€œå¿«æ¥ä¹°æ–°é²œçš„è”¬èœï¼â€

6. **å¹´è½»æ¯äº²**
   - **å¹´é¾„**: 30
   - **ç©¿ç€**: Tæ¤ï¼Œç‰›ä»”è£¤ï¼Œè¿åŠ¨é‹
   - **å‘½ä»¤**: â€œå¸¦å­©å­å»å…¬å›­ç©å§ã€‚â€

7. **é€€ä¼‘è€äºº**
   - **å¹´é¾„**: 65
   - **ç©¿ç€**: æ£‰è´¨è¡¬è¡«ï¼Œä¼‘é—²è£¤ï¼Œè‰é‹
   - **å‘½ä»¤**: â€œå’Œæˆ‘ä¸€èµ·ä¸‹æ£‹ã€‚â€

8. **ç‰§ç¾Šäºº**
   - **å¹´é¾„**: 40
   - **ç©¿ç€**: è¿·å½©æœï¼Œç‰›ä»”é´ï¼Œå®½è¾¹å¸½
   - **å‘½ä»¤**: â€œå»æ”¾ç¾Šï¼â€

9. **ä¿®ç†å·¥**
   - **å¹´é¾„**: 38
   - **ç©¿ç€**: å·¥è£…ï¼Œæ‰‹å¥—ï¼Œå·¥ä½œé´
   - **å‘½ä»¤**: â€œå¸®æˆ‘æ¬ç‚¹å·¥å…·ã€‚â€

10. **å°ç”·å­©**
    - **å¹´é¾„**: 6
    - **ç©¿ç€**: Tæ¤ï¼ŒçŸ­è£¤ï¼Œå‡‰é‹
    - **å‘½ä»¤**: â€œæˆ‘æƒ³æŠ“è´è¶ï¼â€

11. **å¨å¸ˆ**
    - **å¹´é¾„**: 33
    - **ç©¿ç€**: å¨å¸ˆæœï¼Œå›´è£™ï¼Œç™½è‰²é‹
    - **å‘½ä»¤**: â€œå‡†å¤‡æ™šé¤ï¼â€

12. **å›­ä¸**
    - **å¹´é¾„**: 50
    - **ç©¿ç€**: å·¥ä½œæœï¼Œæ‰‹å¥—ï¼Œé´å­
    - **å‘½ä»¤**: â€œç»™èŠ±æµ‡æ°´ã€‚â€

13. **å¹´è½»å†œå¦‡**
    - **å¹´é¾„**: 28
    - **ç©¿ç€**: çŸ­è¢–è¡«ï¼Œé•¿è£™ï¼Œå¸†å¸ƒé‹
    - **å‘½ä»¤**: â€œå»å¸‚åœºä¹°èœã€‚â€

14. **å·¥åŒ **
    - **å¹´é¾„**: 45
    - **ç©¿ç€**: å·¥è£…ï¼Œå·¥å…·è…°å¸¦ï¼Œé´å­
    - **å‘½ä»¤**: â€œå¸®æˆ‘æ¬æœ¨æã€‚â€

15. **é‚®é€’å‘˜**
    - **å¹´é¾„**: 32
    - **ç©¿ç€**: åˆ¶æœï¼Œå¸½å­ï¼Œè¿åŠ¨é‹
    - **å‘½ä»¤**: â€œé€ä¿¡åˆ°é‚»å±…å®¶ã€‚â€

16. **å°å•†åº—è€æ¿**
    - **å¹´é¾„**: 52
    - **ç©¿ç€**: è¡¬è¡«ï¼Œé•¿è£¤ï¼Œå›´è£™
    - **å‘½ä»¤**: â€œæ¬¢è¿å…‰ä¸´ï¼Œçœ‹çœ‹æœ‰ä»€ä¹ˆéœ€è¦çš„ã€‚â€

17. **æ¸”å¤«**
    - **å¹´é¾„**: 47
    - **ç©¿ç€**: é˜²æ°´å¤–å¥—ï¼Œæ¸”é´ï¼Œå¸½å­
    - **å‘½ä»¤**: â€œå»é’“é±¼å§ï¼â€

18. **å°å­¦ç”Ÿ**
    - **å¹´é¾„**: 10
    - **ç©¿ç€**: æ ¡æœï¼Œè¿åŠ¨é‹
    - **å‘½ä»¤**: â€œæˆ‘æƒ³å»ä¸Šå­¦ï¼â€

19. **å…½åŒ»**
    - **å¹´é¾„**: 36
    - **ç©¿ç€**: ç™½å¤§è¤‚ï¼Œæ‰‹å¥—ï¼Œé´å­
    - **å‘½ä»¤**: â€œæ£€æŸ¥ä¸€ä¸‹åŠ¨ç‰©ã€‚â€

20. **å†œåœºä¸»**
    - **å¹´é¾„**: 55
    - **ç©¿ç€**: ç‰›ä»”è¡¬è¡«ï¼Œç‰›ä»”è£¤ï¼Œå·¥ä½œé´
    - **å‘½ä»¤**: â€œå¸®æˆ‘æ”¶å‰²åº„ç¨¼ã€‚â€

### ä½¿ç”¨å»ºè®®

è¿™äº›è§’è‰²å’Œå‘½ä»¤å¯ä»¥ç”¨ä½œæ¸¸æˆä¸­çš„NPCï¼Œä¸ç©å®¶äº’åŠ¨ï¼Œå¢åŠ æ¸¸æˆçš„ä¸°å¯Œæ€§å’Œè¶£å‘³æ€§ã€‚

### è§’è‰²è¡Œä¸ºè§„åˆ™

1. **å†œå¤«**
   - **çŠ¶æ€**: å·¥ä½œã€é—²èŠ
   - **è§„åˆ™**:
     - å·¥ä½œæ—¶ä¼šåœ¨ç”°é‡Œæµ‡æ°´æˆ–æ”¶å‰²ã€‚
     - é—²èŠæ—¶ä¼šè¯¢é—®å¤©æ°”æˆ–ä½œç‰©ç”Ÿé•¿æƒ…å†µã€‚

2. **è€å¦‡äºº**
   - **çŠ¶æ€**: ä¼‘æ¯ã€é—²èŠ
   - **è§„åˆ™**:
     - ä¼‘æ¯æ—¶ååœ¨é•¿æ¤…ä¸Šï¼Œå¶å°”å–‚é¸Ÿã€‚
     - é—²èŠæ—¶ä¼šè®²æ•…äº‹æˆ–åˆ†äº«å®¶åº­ç§˜æ–¹ã€‚

3. **å°å¥³å­©**
   - **çŠ¶æ€**: ç©è€ã€å¯»æ‰¾
   - **è§„åˆ™**:
     - ç©è€æ—¶ä¸å…¶ä»–å°æœ‹å‹ä¸€èµ·ç©æ¸¸æˆã€‚
     - å¯»æ‰¾æ—¶ä¼šå¯»æ‰¾è´è¶æˆ–ç©å…·ã€‚

4. **è€å¸ˆ**
   - **çŠ¶æ€**: æ•™å­¦ã€åŠå…¬
   - **è§„åˆ™**:
     - æ•™å­¦æ—¶ä¼šåœ¨æ•™å®¤é‡Œä¸Šè¯¾ã€‚
     - åŠå…¬æ—¶ä¼šæ‰¹æ”¹ä½œä¸šæˆ–å‡†å¤‡è¯¾ç¨‹ã€‚

5. **å•†è´©**
   - **çŠ¶æ€**: é”€å”®ã€é—²èŠ
   - **è§„åˆ™**:
     - é”€å”®æ—¶ä¸»åŠ¨å‘è·¯è¿‡çš„äººæ¨é”€å•†å“ã€‚
     - é—²èŠæ—¶ä¼šè®¨è®ºå¸‚åœºä»·æ ¼æˆ–é£Ÿè°±ã€‚

6. **å¹´è½»æ¯äº²**
   - **çŠ¶æ€**: ç…§é¡¾å­©å­ã€è´­ç‰©
   - **è§„åˆ™**:
     - ç…§é¡¾å­©å­æ—¶ä¼šå¸¦å­©å­å»å…¬å›­ã€‚
     - è´­ç‰©æ—¶ä¼šåœ¨å¸‚åœºä¸ŠæŒ‘é€‰é£Ÿæã€‚

7. **é€€ä¼‘è€äºº**
   - **çŠ¶æ€**: ä¼‘é—²ã€ç¤¾äº¤
   - **è§„åˆ™**:
     - ä¼‘é—²æ—¶ä¼šåœ¨å…¬å›­é‡Œæ•£æ­¥ã€‚
     - ç¤¾äº¤æ—¶ä¼šä¸é‚»å±…èŠå¤©ã€‚

8. **ç‰§ç¾Šäºº**
   - **çŠ¶æ€**: æ”¾ç¾Šã€ä¼‘æ¯
   - **è§„åˆ™**:
     - æ”¾ç¾Šæ—¶åœ¨è‰åœ°ä¸Šå¸¦é¢†ç¾Šç¾¤ã€‚
     - ä¼‘æ¯æ—¶ååœ¨æ ‘ä¸‹è§‚å¯Ÿç¾Šç¾¤ã€‚

9. **ä¿®ç†å·¥**
   - **çŠ¶æ€**: ä¿®ç†ã€é—²èŠ
   - **è§„åˆ™**:
     - ä¿®ç†æ—¶ä¼šä¿®ç†å·¥å…·æˆ–è®¾å¤‡ã€‚
     - é—²èŠæ—¶ä¼šè®¨è®ºå·¥å…·ä½¿ç”¨æŠ€å·§ã€‚

10. **å°ç”·å­©**
    - **çŠ¶æ€**: ç©è€ã€æ¢ç´¢
    - **è§„åˆ™**:
      - ç©è€æ—¶ä¼šä¸å°å¥³å­©ä¸€èµ·ç©ã€‚
      - æ¢ç´¢æ—¶ä¼šåœ¨å‘¨å›´å¯»æ‰¾æœ‰è¶£çš„ä¸œè¥¿ã€‚

11. **å¨å¸ˆ**
    - **çŠ¶æ€**: çƒ¹é¥ªã€æ‹›å¾…
    - **è§„åˆ™**:
      - çƒ¹é¥ªæ—¶åœ¨å¨æˆ¿å¿™ç¢Œã€‚
      - æ‹›å¾…æ—¶æ¬¢è¿é¡¾å®¢å¹¶æ¨èèœå“ã€‚

12. **å›­ä¸**
    - **çŠ¶æ€**: ç§æ¤ã€ç»´æŠ¤
    - **è§„åˆ™**:
      - ç§æ¤æ—¶åœ¨èŠ±å›­é‡Œç§èŠ±æˆ–ä¿®å‰ªæ¤ç‰©ã€‚
      - ç»´æŠ¤æ—¶æ£€æŸ¥æ¤ç‰©å¥åº·ã€‚

13. **å¹´è½»å†œå¦‡**
    - **çŠ¶æ€**: é‡‡è´­ã€ç…§é¡¾å®¶
    - **è§„åˆ™**:
      - é‡‡è´­æ—¶åœ¨å¸‚åœºä¸Šä¹°èœã€‚
      - ç…§é¡¾å®¶æ—¶åšå®¶åŠ¡ã€‚

14. **å·¥åŒ **
    - **çŠ¶æ€**: åˆ¶ä½œã€å±•ç¤º
    - **è§„åˆ™**:
      - åˆ¶ä½œæ—¶åœ¨å·¥ä½œåŠé‡Œåšæ‰‹å·¥è‰ºå“ã€‚
      - å±•ç¤ºæ—¶ä¸é¡¾å®¢å±•ç¤ºä½œå“ã€‚

15. **é‚®é€’å‘˜**
    - **çŠ¶æ€**: é€ä¿¡ã€ä¼‘æ¯
    - **è§„åˆ™**:
      - é€ä¿¡æ—¶æŒ‰æ—¶é€è¾¾é‚®ä»¶ã€‚
      - ä¼‘æ¯æ—¶åœ¨é‚®å±€ä¼‘æ¯ã€‚

16. **å°å•†åº—è€æ¿**
    - **çŠ¶æ€**: é”€å”®ã€ç®¡ç†
    - **è§„åˆ™**:
      - é”€å”®æ—¶ä¸é¡¾å®¢äº’åŠ¨ã€‚
      - ç®¡ç†æ—¶æ•´ç†è´§æ¶ã€‚

17. **æ¸”å¤«**
    - **çŠ¶æ€**: é’“é±¼ã€å½’æ¥
    - **è§„åˆ™**:
      - é’“é±¼æ—¶åœ¨æ²³è¾¹é™é™ç­‰å¾…ã€‚
      - å½’æ¥æ—¶åˆ†äº«æ•è·çš„é±¼ã€‚

18. **å°å­¦ç”Ÿ**
    - **çŠ¶æ€**: ä¸Šè¯¾ã€ç©è€
    - **è§„åˆ™**:
      - ä¸Šè¯¾æ—¶è®¤çœŸå¬è®²ã€‚
      - ç©è€æ—¶ä¸åŒå­¦ä¸€èµ·å¬‰æˆã€‚

19. **å…½åŒ»**
    - **çŠ¶æ€**: æ£€æŸ¥ã€æ²»ç–—
    - **è§„åˆ™**:
      - æ£€æŸ¥æ—¶ä»”ç»†è§‚å¯ŸåŠ¨ç‰©å¥åº·ã€‚
      - æ²»ç–—æ—¶ç»™åŠ¨ç‰©è¿›è¡ŒåŒ»ç–—å¤„ç†ã€‚

20. **å†œåœºä¸»**
    - **çŠ¶æ€**: ç®¡ç†ã€å·¡è§†
    - **è§„åˆ™**:
      - ç®¡ç†æ—¶å®‰æ’å†œåœºäº‹åŠ¡ã€‚
      - å·¡è§†æ—¶æ£€æŸ¥ä½œç‰©å’ŒåŠ¨ç‰©ã€‚

### æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰ç¤ºä¾‹

æ¯ä¸ªè§’è‰²å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æœ‰é™çŠ¶æ€æœºæ¨¡å‹ç®¡ç†çŠ¶æ€ï¼š

```plaintext
çŠ¶æ€æœºç¤ºä¾‹ï¼š
[Idle] <---> [Working] <---> [Chatting]

çŠ¶æ€è½¬æ¢è§„åˆ™ï¼š
- Idle â†’ Working: å½“è§’è‰²æ¥æ”¶åˆ°ä»»åŠ¡æ—¶ã€‚
- Working â†’ Idle: å½“ä»»åŠ¡å®Œæˆæ—¶ã€‚
- Idle â†’ Chatting: å½“ç”¨æˆ·é è¿‘å¹¶ä¸è§’è‰²äº’åŠ¨æ—¶ã€‚
- Chatting â†’ Idle: å½“å¯¹è¯ç»“æŸæˆ–ç”¨æˆ·ç¦»å¼€æ—¶ã€‚
- Working â†’ Chatting: å½“è§’è‰²è¢«ç”¨æˆ·æ‰“æ–­æ—¶ã€‚
```

### å®ç°å»ºè®®

- **çŠ¶æ€ç®¡ç†**: ä½¿ç”¨å¯¹è±¡æˆ–ç±»æ¥å®ç°æ¯ä¸ªè§’è‰²çš„çŠ¶æ€åŠå…¶è¡Œä¸ºã€‚
- **äº‹ä»¶é©±åŠ¨**: é€šè¿‡äº‹ä»¶è§¦å‘çŠ¶æ€è½¬æ¢ï¼Œä¾‹å¦‚ç”¨æˆ·äº¤äº’æˆ–æ—¶é—´å˜åŒ–ã€‚
- **å¯æ‰©å±•æ€§**: éšç€æ¸¸æˆçš„è¿›å±•ï¼Œå¯ä»¥æ·»åŠ æ›´å¤šçŠ¶æ€å’Œè¡Œä¸ºè§„åˆ™ï¼Œå¢å¼ºè§’è‰²çš„å¤æ‚æ€§å’Œäº’åŠ¨æ€§ã€‚

### JetBrains

ssh -p 39741 root@connect.yza1.seetacloud.com

rahulnyk/knowledge_graph

# é­”æ­ModelScopeç¤¾åŒº

### è§£é”å¼€æºæ¨¡å‹é«˜æ€§èƒ½æœåŠ¡ï¼šSGLang Runtime åº”ç”¨åœºæ™¯ä¸å®è·µ

https://mp.weixin.qq.com/s/kDOpaVSECJ8NyapSPKpQIA

swift inferå’Œswift deployå·²æ”¯æŒä½¿ç”¨lmdeployè¿›è¡Œæ¨ç†åŠ é€Ÿå’Œéƒ¨ç½². å…¶ä¸­åŒ…æ‹¬å¤šæ¨¡æ€æ¨¡å‹ï¼šqwen-vl, internvl, minicpm-v, xcomposer, glm4v, deepseek-vlç­‰ï¼ˆbatchæ¨¡å¼æ¯”åŸå§‹ptå¿«20å€ï¼‰

# ä¸€é”®æå–PDFå†…å®¹å’Œä¸€é”®ç”ŸæˆçŸ¥è¯†å›¾è°±

## MinerU

https://github.com/opendatalab/MinerU?tab=readme-ov-file#Magic-PDF

## rahulnyk]/knowledge_graph

https://github.com/rahulnyk/knowledge_graph/tree/main



# ollama-Image-tags ä½¿ç”¨æ•™ç¨‹ã€‚æ‡’äººæ‰“æ ‡ç¦éŸ³ï¼Œå…¨è‡ªåŠ¨AIæ‰“æ ‡å·¥å…·



å¯ä»¥[![img](https://i0.hdslb.com/bfs/reply/9f3ad0659e84

# è”ç½‘æœç´¢

è”ç½‘çš„è¯ä½ åŠ ä¸€ä¸ªtoolï¼Œæ¯”å¦‚Tavily search, è¿™éƒ½æ˜¯éå¸¸éå¸¸å®¹æ˜“å®ç°çš„äº†





https://weibo.com/ttarticle/p/show?id=2309405049908221575280

å®‰å…¨å†…å®¹è¿‡æ»¤æ¨¡å‹ 

safety content moderation models  google/shieldgemma

# MindSearchæŠ€æœ¯è¯¦è§£ï¼Œæœ¬åœ°æ­å»ºåª²ç¾Perplexityçš„AIæ€Â·ç´¢åº”ç”¨ï¼

https://mp.weixin.qq.com/s/AhPoPD8eOnKFP670xeVt_g



http://119.23.78.242:8000/

base_the_ville_n20

ringworld

base_the_ville_n25

base_the_ville_isabella_maria_klaus

```
CUDA_VISIBLE_DEVICES=0 swift deploy --model_type llama3-8b  --infer_backend pt --model_id_or_path /root/models/LLM-Research/Meta-Llama-3-8B-Instruct
```

```
curl http://localhost:8000/v1/chat/completions \
-H "Content-Type: application/json" \
-d '{
"model": "llama3-8b",
"messages": [{"role": "user", "content": "æ™šä¸Šç¡ä¸ç€è§‰æ€ä¹ˆåŠï¼Ÿ"}],
"max_tokens": 256,
"temperature": 0
}'
```

sk-4cSXo82PTMGey2Vh61D0yLwVz6YPo67AC9BE54A5711EF83429E3A4EE25FE6



https://mp.weixin.qq.com/s/f-rOmA9ZNADtO_OorhuKVQ

LLaVA

# å¼€æºTinyLLaVA Factory



from modelscope.msdatasets import MsDataset

ds =  MsDataset.load('swift/llava-data',cache_dir = r'D:\DDDDDDDDDDDfile\map_tile' )

ds =  MsDataset.load('swift/gpt4v-dataset',cache_dir = r'D:\caijian')

ds =  MsDataset.load('AI-ModelScope/LLaVA-Instruct-150K',cache_dir = r'D:\caijian')

from modelscope.msdatasets import MsDataset ds =  MsDataset.load('AI-ModelScope/LLaVA-Instruct-150K')

 ds =  MsDataset.load('AI-ModelScope/OpenOrca-Chinese',cache_dir = r'D:\caijian')

ds =  MsDataset.load('AI-ModelScope/LLaVA-Pretrain',cache_dir = r'D:\caijian')



ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼šåŸºäºCNNçš„ç¼–ç å™¨ã€MSAAæ¨¡å—å’ŒåŸºäºCSMambaçš„è§£ç å™¨



# BM25å”±ç½¢ï¼ŒBMXç™»åœºï¼

å¤§å¤šæ•°æ–‡æœ¬æœç´¢å¼•æ“èƒŒåçš„ç®—æ³•å¤šå¤šå°‘å°‘éƒ½è·ŸBM25æ²¾è¾¹ï¼ŒBM25çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºï¼Œå®ƒåœ¨åˆ†å¸ƒä¹‹å¤–çš„æ•°æ®ä¸­è¡¨ç°å¾—éå¸¸å¥½ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒå¯ä»¥å¾ˆå¥½çš„å¤„ç†ä»¥å‰ä»æœªè§è¿‡çš„æ•°æ®ã€‚ä½†æ˜¯ï¼å…³é”®å­—æœç´¢æ–¹æ³•æœ‰å…¶è‡ªèº«çš„å±€é™æ€§ï¼š

- BM25 ä¸è€ƒè™‘æŸ¥è¯¢ä¸ä»»ä½•ç»™å®šæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œè¿™å¯ä»¥æ›´å‡†ç¡®åœ°è¯„ä¼°è¯¥æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§ã€‚
- è¯æ±‡æœç´¢ç®—æ³•ç¼ºä¹è¯­ä¹‰ç†è§£ï¼Œå› æ­¤æ— æ³•å¤„ç†åŒä¹‰è¯å’ŒåŒéŸ³å¼‚ä¹‰è¯ç­‰è¯­è¨€ç»†å¾®å·®åˆ«ã€‚ä¸åŸºäºç‰¹å®šé¢†åŸŸæ–‡æœ¬åµŒå…¥çš„è¯­ä¹‰æœç´¢ç›¸æ¯”ï¼Œè¿™ç§é™åˆ¶æ˜¯è¯æ³•æœç´¢æ€§èƒ½ä¸ä½³çš„å…³é”®å› ç´ ã€‚

äºæ˜¯æœ¬æ–‡æå‡ºäº†BMXï¼Œè®¡ç®—ç®€å•ï¼Œæ•ˆæœä¼˜äºæ‰€æœ‰çš„BM25å˜ç§,å»ºç´¢å¼•ï¼Œæœç´¢éƒ½ä¸ä¼šæ˜æ˜¾æ…¢ï¼Œä½†æ˜¯æ•ˆæœæ˜æ˜¾å¥½ï¼

https://mp.weixin.qq.com/s/gLyfsk8ZGlTyUqyXZ_r54A

https://github.com/mixedbread-ai/baguetter

```
#Â pipÂ installÂ baguetter

fromÂ baguetter.indicesÂ importÂ BMXSparseIndex
Â 
#Â InitializeÂ BMğ’³Â index
bmxÂ =Â BMXSparseIndex()
Â 
#Â AddÂ bakeryÂ itemsÂ toÂ theÂ index
docsÂ =Â [
Â Â Â Â "FreshlyÂ crustyÂ bakedÂ sourdoughÂ breadÂ withÂ aÂ crispyÂ crust",
Â Â Â Â "FlakyÂ croissantsÂ madeÂ withÂ FrenchÂ butter",
Â Â Â Â "ChocolateÂ chipÂ cookiesÂ withÂ chunksÂ ofÂ darkÂ chocolate",
Â Â Â Â "CinnamonÂ rollsÂ withÂ creamÂ cheeseÂ frosting",
Â Â Â Â "ArtisanalÂ baguettesÂ withÂ aÂ softÂ interiorÂ andÂ crustyÂ exterior"
]
keysÂ =Â list(range(len(docs)))
Â 
bmx.add_many(keys=keys,Â values=docs)
Â 
#Â SearchÂ forÂ bread
queryÂ =Â "crustyÂ bread"
resultsÂ =Â bmx.search(query,Â top_k=2)
Â 
print(results)
#Â SearchResults(keys=[0,Â 4],Â scores=array([2.5519667Â ,Â 0.97304875],Â dtype=float32),Â normalized=False)
```



# å°ç™½ä¹Ÿå¯ä»¥æ¸…æ™°ç†è§£diffusionåŸç†: DDPM

https://mp.weixin.qq.com/s/sM9Ema97EyB3c_4sfl38vA
