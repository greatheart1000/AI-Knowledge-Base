#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LangGraph RAGç³»ç»Ÿæ¼”ç¤º
å±•ç¤ºæ£€ç´¢å¢å¼ºç”Ÿæˆã€å‘é‡æœç´¢ã€çŸ¥è¯†åº“ç®¡ç†
"""

import os
import sys
import json
import time
import uuid
import asyncio
from typing import TypedDict, List, Dict, Any, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime, timezone
import logging

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.store.memory import InMemoryStore
from langgraph.runtime import Runtime
from langchain_core.embeddings import Embeddings
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage


# ==================== åµŒå…¥æ¨¡å‹å®ç° ====================

class MockEmbeddings(Embeddings):
    """æ¨¡æ‹ŸåµŒå…¥æ¨¡å‹ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    
    def __init__(self, dimensions: int = 384):
        self.dimensions = dimensions
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ–‡æ¡£åµŒå…¥"""
        embeddings = []
        for text in texts:
            # ç®€å•çš„åŸºäºè¯é¢‘çš„å‘é‡åŒ–
            words = text.lower().split()
            vector = [0.0] * self.dimensions
            
            for i, word in enumerate(words[:self.dimensions]):
                # ä½¿ç”¨è¯çš„ä½ç½®å’Œé•¿åº¦ç”Ÿæˆå‘é‡
                vector[i] = (hash(word) % 1000) / 1000.0
            
            # å½’ä¸€åŒ–
            norm = sum(x * x for x in vector) ** 0.5
            if norm > 0:
                vector = [x / norm for x in vector]
            
            embeddings.append(vector)
        
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """æŸ¥è¯¢åµŒå…¥"""
        return self.embed_documents([text])[0]


# ==================== çŸ¥è¯†åº“ç®¡ç† ====================

@dataclass
class Document:
    """æ–‡æ¡£ç»“æ„"""
    id: str
    title: str
    content: str
    source: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "id": self.id,
            "title": self.title,
            "content": self.content,
            "source": self.source,
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat()
        }


class KnowledgeBase:
    """çŸ¥è¯†åº“ç®¡ç†å™¨"""
    
    def __init__(self, store: InMemoryStore, embeddings: Embeddings):
        self.store = store
        self.embeddings = embeddings
        self.documents: Dict[str, Document] = {}
    
    def add_document(self, document: Document) -> str:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        # å­˜å‚¨æ–‡æ¡£å…ƒæ•°æ®
        self.documents[document.id] = document
        self.store.put(
            ("documents",),
            document.id,
            document.to_dict()
        )
        
        # åˆ›å»ºåµŒå…¥å¹¶å­˜å‚¨
        embeddings = self.embeddings.embed_documents([document.content])
        self.store.put(
            ("embeddings",),
            document.id,
            {
                "document_id": document.id,
                "embedding": embeddings[0],
                "content": document.content,
                "title": document.title,
                "source": document.source
            }
        )
        
        return document.id
    
    def search_documents(self, query: str, limit: int = 5, min_score: float = 0.0) -> List[Dict[str, Any]]:
        """æœç´¢æ–‡æ¡£"""
        # ä½¿ç”¨å‘é‡æœç´¢
        results = self.store.search(
            ("embeddings",),
            query=query,
            limit=limit
        )
        
        # è¿‡æ»¤ä½åˆ†ç»“æœ
        filtered_results = []
        for result in results:
            if result.score and result.score >= min_score:
                filtered_results.append({
                    "document_id": result.value["document_id"],
                    "title": result.value["title"],
                    "content": result.value["content"],
                    "source": result.value["source"],
                    "score": result.score
                })
        
        return filtered_results
    
    def get_document(self, document_id: str) -> Optional[Document]:
        """è·å–æ–‡æ¡£"""
        return self.documents.get(document_id)


# ==================== RAGçŠ¶æ€å®šä¹‰ ====================

class RAGState(TypedDict):
    """RAGçŠ¶æ€"""
    query: str
    retrieved_documents: List[Dict[str, Any]]
    context: str
    response: str
    metadata: Dict[str, Any]


# ==================== RAGå›¾èŠ‚ç‚¹ ====================

# å…¨å±€çŸ¥è¯†åº“å®ä¾‹
_global_knowledge_base = None

def create_rag_graph():
    """åˆ›å»ºRAGå›¾"""
    
    def document_retrieval_node(state: RAGState, runtime: Runtime[Any]) -> RAGState:
        """æ–‡æ¡£æ£€ç´¢èŠ‚ç‚¹"""
        global _global_knowledge_base
        query = state["query"]
        
        # è·å–çŸ¥è¯†åº“
        if _global_knowledge_base is None:
            embeddings = MockEmbeddings()
            _global_knowledge_base = KnowledgeBase(runtime.store, embeddings)
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = _global_knowledge_base.search_documents(
            query=query,
            limit=5,
            min_score=0.3
        )
        
        state["retrieved_documents"] = retrieved_docs
        state["metadata"]["retrieval_count"] = len(retrieved_docs)
        
        return state
    
    def context_building_node(state: RAGState, runtime: Runtime[Any]) -> RAGState:
        """ä¸Šä¸‹æ–‡æ„å»ºèŠ‚ç‚¹"""
        retrieved_docs = state["retrieved_documents"]
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for i, doc in enumerate(retrieved_docs, 1):
            context_parts.append(
                f"æ–‡æ¡£{i} (æ¥æº: {doc['source']}, ç›¸å…³æ€§: {doc['score']:.3f}):\n"
                f"{doc['content'][:200]}..."
            )
        
        context = "\n\n".join(context_parts)
        state["context"] = context
        state["metadata"]["context_length"] = len(context)
        
        return state
    
    def response_generation_node(state: RAGState, runtime: Runtime[Any]) -> RAGState:
        """å“åº”ç”ŸæˆèŠ‚ç‚¹"""
        query = state["query"]
        context = state["context"]
        retrieved_docs = state["retrieved_documents"]
        
        # ç”ŸæˆåŸºäºä¸Šä¸‹æ–‡çš„å“åº”
        if retrieved_docs:
            # åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå“åº”
            response = f"åŸºäºçŸ¥è¯†åº“çš„å›ç­”:\n\n"
            response += f"é—®é¢˜: {query}\n\n"
            response += f"æ ¹æ®ç›¸å…³æ–‡æ¡£ï¼Œæˆ‘æ‰¾åˆ°ä»¥ä¸‹ä¿¡æ¯:\n\n"
            
            for i, doc in enumerate(retrieved_docs, 1):
                response += f"{i}. {doc['title']} (ç›¸å…³æ€§: {doc['score']:.3f})\n"
                response += f"   {doc['content'][:150]}...\n\n"
            
            response += f"åŸºäºè¿™äº›ä¿¡æ¯ï¼Œæˆ‘çš„å›ç­”æ˜¯: è¿™æ˜¯ä¸€ä¸ªå…³äº'{query}'çš„é—®é¢˜ï¼Œ"
            response += f"æˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°äº†{len(retrieved_docs)}ä¸ªç›¸å…³æ–‡æ¡£ã€‚"
        else:
            response = f"æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å…³äº'{query}'çš„ç›¸å…³ä¿¡æ¯ã€‚"
        
        state["response"] = response
        state["metadata"]["response_generated"] = True
        
        return state
    
    # åˆ›å»ºå›¾
    graph = StateGraph(state_schema=RAGState)
    graph.add_node("document_retrieval", document_retrieval_node)
    graph.add_node("context_building", context_building_node)
    graph.add_node("response_generation", response_generation_node)
    
    graph.set_entry_point("document_retrieval")
    graph.add_edge("document_retrieval", "context_building")
    graph.add_edge("context_building", "response_generation")
    graph.add_edge("response_generation", END)
    
    return graph


# ==================== çŸ¥è¯†åº“åˆå§‹åŒ– ====================

def initialize_knowledge_base(store: InMemoryStore, embeddings: Embeddings) -> KnowledgeBase:
    """åˆå§‹åŒ–çŸ¥è¯†åº“"""
    kb = KnowledgeBase(store, embeddings)
    
    # æ·»åŠ ç¤ºä¾‹æ–‡æ¡£
    documents = [
        Document(
            id="doc_001",
            title="LangGraphåŸºç¡€æ¦‚å¿µ",
            content="LangGraphæ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€å¤šæ™ºèƒ½ä½“åº”ç”¨çš„æ¡†æ¶ã€‚å®ƒåŸºäºLangChainæ„å»ºï¼Œæä¾›äº†å¼ºå¤§çš„å›¾æ‰§è¡Œå¼•æ“å’ŒçŠ¶æ€ç®¡ç†èƒ½åŠ›ã€‚LangGraphæ”¯æŒå¤æ‚çš„å¯¹è¯æµç¨‹ã€è®°å¿†æœºåˆ¶å’Œæ™ºèƒ½ä½“åä½œã€‚",
            source="å®˜æ–¹æ–‡æ¡£",
            metadata={"category": "framework", "difficulty": "beginner"}
        ),
        Document(
            id="doc_002",
            title="è®°å¿†æœºåˆ¶å®ç°",
            content="LangGraphçš„è®°å¿†æœºåˆ¶é€šè¿‡checkpointå’Œstoreå®ç°ã€‚checkpointä¿å­˜å›¾æ‰§è¡ŒçŠ¶æ€ï¼Œstoreæä¾›æŒä¹…åŒ–å­˜å‚¨ã€‚æ”¯æŒçŸ­æœŸè®°å¿†ã€é•¿æœŸè®°å¿†å’Œå‘é‡æœç´¢ã€‚è®°å¿†å¯ä»¥è·¨ä¼šè¯ä¿æŒï¼Œæ”¯æŒå¤æ‚çš„ä¸Šä¸‹æ–‡ç®¡ç†ã€‚",
            source="æŠ€æœ¯æ–‡æ¡£",
            metadata={"category": "memory", "difficulty": "intermediate"}
        ),
        Document(
            id="doc_003",
            title="RAGç³»ç»Ÿæ¶æ„",
            content="RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆã€‚ç³»ç»Ÿé¦–å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶ååŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚è¿™ç§æ–¹æ³•æé«˜äº†å›ç­”çš„å‡†ç¡®æ€§å’Œç›¸å…³æ€§ã€‚",
            source="AIè®ºæ–‡",
            metadata={"category": "rag", "difficulty": "advanced"}
        ),
        Document(
            id="doc_004",
            title="å‘é‡æœç´¢æŠ€æœ¯",
            content="å‘é‡æœç´¢ä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼Œç„¶åé€šè¿‡è®¡ç®—å‘é‡é—´çš„ç›¸ä¼¼åº¦æ¥æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚å¸¸ç”¨çš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•åŒ…æ‹¬ä½™å¼¦ç›¸ä¼¼åº¦å’Œæ¬§å‡ é‡Œå¾—è·ç¦»ã€‚",
            source="æœºå™¨å­¦ä¹ æŒ‡å—",
            metadata={"category": "vector_search", "difficulty": "intermediate"}
        ),
        Document(
            id="doc_005",
            title="å¤šæ™ºèƒ½ä½“åä½œ",
            content="å¤šæ™ºèƒ½ä½“ç³»ç»Ÿç”±å¤šä¸ªä¸“é—¨çš„æ™ºèƒ½ä½“ç»„æˆï¼Œæ¯ä¸ªæ™ºèƒ½ä½“è´Ÿè´£ç‰¹å®šçš„ä»»åŠ¡ã€‚æ™ºèƒ½ä½“ä¹‹é—´é€šè¿‡æ¶ˆæ¯ä¼ é€’å’ŒçŠ¶æ€å…±äº«è¿›è¡Œåä½œï¼Œå¯ä»¥å¤„ç†å¤æ‚çš„ä»»åŠ¡åˆ†è§£å’Œå¹¶è¡Œæ‰§è¡Œã€‚",
            source="ç³»ç»Ÿæ¶æ„æ–‡æ¡£",
            metadata={"category": "multi_agent", "difficulty": "advanced"}
        ),
        Document(
            id="doc_006",
            title="Pythonç¼–ç¨‹åŸºç¡€",
            content="Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚å®ƒå¹¿æ³›åº”ç”¨äºæ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ã€Webå¼€å‘ç­‰é¢†åŸŸã€‚Pythonæ”¯æŒé¢å‘å¯¹è±¡ç¼–ç¨‹å’Œå‡½æ•°å¼ç¼–ç¨‹ã€‚",
            source="ç¼–ç¨‹æ•™ç¨‹",
            metadata={"category": "programming", "difficulty": "beginner"}
        ),
        Document(
            id="doc_007",
            title="æœºå™¨å­¦ä¹ ç®—æ³•",
            content="æœºå™¨å­¦ä¹ ç®—æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚å¸¸ç”¨çš„ç®—æ³•æœ‰çº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€ç¥ç»ç½‘ç»œã€æ”¯æŒå‘é‡æœºç­‰ã€‚é€‰æ‹©åˆé€‚çš„ç®—æ³•å–å†³äºæ•°æ®ç‰¹å¾å’Œä»»åŠ¡éœ€æ±‚ã€‚",
            source="MLæ•™ç¨‹",
            metadata={"category": "ml", "difficulty": "intermediate"}
        ),
        Document(
            id="doc_008",
            title="æ·±åº¦å­¦ä¹ æ¡†æ¶",
            content="æ·±åº¦å­¦ä¹ æ¡†æ¶å¦‚TensorFlowã€PyTorchã€Kerasç­‰æä¾›äº†æ„å»ºå’Œè®­ç»ƒç¥ç»ç½‘ç»œçš„å·¥å…·ã€‚è¿™äº›æ¡†æ¶æ”¯æŒGPUåŠ é€Ÿã€è‡ªåŠ¨å¾®åˆ†å’Œæ¨¡å‹éƒ¨ç½²ï¼Œå¤§å¤§ç®€åŒ–äº†æ·±åº¦å­¦ä¹ åº”ç”¨çš„å¼€å‘ã€‚",
            source="æ·±åº¦å­¦ä¹ æŒ‡å—",
            metadata={"category": "deep_learning", "difficulty": "advanced"}
        )
    ]
    
    # æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
    for doc in documents:
        kb.add_document(doc)
    
    return kb


# ==================== æ¼”ç¤ºå‡½æ•° ====================

def demo_rag_system():
    """æ¼”ç¤ºRAGç³»ç»Ÿ"""
    print("ğŸ” RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå­˜å‚¨å’Œæ£€æŸ¥ç‚¹
    store = InMemoryStore(
        index={
            "dims": 384,
            "embed": MockEmbeddings(384),
            "fields": ["content", "title"]
        }
    )
    checkpointer = InMemorySaver()
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    embeddings = MockEmbeddings(384)
    knowledge_base = initialize_knowledge_base(store, embeddings)
    
    print(f"çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆï¼ŒåŒ…å« {len(knowledge_base.documents)} ä¸ªæ–‡æ¡£")
    
    # åˆ›å»ºå›¾
    graph = create_rag_graph()
    compiled_graph = graph.compile(store=store, checkpointer=checkpointer)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ",
        "å¦‚ä½•å®ç°è®°å¿†æœºåˆ¶ï¼Ÿ",
        "RAGç³»ç»Ÿå¦‚ä½•å·¥ä½œï¼Ÿ",
        "å‘é‡æœç´¢çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿå¦‚ä½•åä½œï¼Ÿ",
        "Pythonç¼–ç¨‹æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ",
        "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç®—æ³•ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ æ¡†æ¶æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    thread_id = "rag_thread_001"
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nç¬¬ {i} ä¸ªæŸ¥è¯¢:")
        print(f"é—®é¢˜: {query}")
        print("-" * 40)
        
        # åˆ›å»ºçŠ¶æ€
        state = RAGState(
            query=query,
            retrieved_documents=[],
            context="",
            response="",
            metadata={}
        )
        
        # è¿è¡Œå›¾
        result = compiled_graph.invoke(
            state,
            config={"configurable": {"thread_id": thread_id}}
        )
        
        print(f"æ£€ç´¢åˆ° {result['metadata']['retrieval_count']} ä¸ªç›¸å…³æ–‡æ¡£")
        print(f"ä¸Šä¸‹æ–‡é•¿åº¦: {result['metadata']['context_length']} å­—ç¬¦")
        print(f"å›ç­”: {result['response']}")
        
        # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„æ–‡æ¡£
        if result["retrieved_documents"]:
            print("\næ£€ç´¢åˆ°çš„æ–‡æ¡£:")
            for j, doc in enumerate(result["retrieved_documents"], 1):
                print(f"  {j}. {doc['title']} (ç›¸å…³æ€§: {doc['score']:.3f})")
                print(f"     æ¥æº: {doc['source']}")
                print(f"     å†…å®¹: {doc['content'][:100]}...")
        
        print("=" * 60)


def demo_vector_search():
    """æ¼”ç¤ºå‘é‡æœç´¢"""
    print("\nğŸ” å‘é‡æœç´¢æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå­˜å‚¨
    store = InMemoryStore(
        index={
            "dims": 384,
            "embed": MockEmbeddings(384),
            "fields": ["content"]
        }
    )
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    embeddings = MockEmbeddings(384)
    knowledge_base = initialize_knowledge_base(store, embeddings)
    
    # æµ‹è¯•ä¸åŒçš„æœç´¢æŸ¥è¯¢
    search_queries = [
        ("LangGraphæ¡†æ¶", "åº”è¯¥æ‰¾åˆ°æ¡†æ¶ç›¸å…³æ–‡æ¡£"),
        ("è®°å¿†å’Œå­˜å‚¨", "åº”è¯¥æ‰¾åˆ°è®°å¿†æœºåˆ¶æ–‡æ¡£"),
        ("æ£€ç´¢ç”Ÿæˆ", "åº”è¯¥æ‰¾åˆ°RAGç›¸å…³æ–‡æ¡£"),
        ("å‘é‡ç›¸ä¼¼åº¦", "åº”è¯¥æ‰¾åˆ°å‘é‡æœç´¢æ–‡æ¡£"),
        ("æ™ºèƒ½ä½“åä½œ", "åº”è¯¥æ‰¾åˆ°å¤šæ™ºèƒ½ä½“æ–‡æ¡£"),
        ("ç¼–ç¨‹è¯­è¨€", "åº”è¯¥æ‰¾åˆ°Pythonç›¸å…³æ–‡æ¡£"),
        ("å­¦ä¹ ç®—æ³•", "åº”è¯¥æ‰¾åˆ°æœºå™¨å­¦ä¹ æ–‡æ¡£"),
        ("ç¥ç»ç½‘ç»œ", "åº”è¯¥æ‰¾åˆ°æ·±åº¦å­¦ä¹ æ–‡æ¡£")
    ]
    
    for query, expected in search_queries:
        print(f"\næœç´¢æŸ¥è¯¢: {query}")
        print(f"é¢„æœŸç»“æœ: {expected}")
        
        results = knowledge_base.search_documents(query, limit=3, min_score=0.1)
        
        if results:
            print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")
            for i, result in enumerate(results, 1):
                print(f"  {i}. {result['title']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")
                print(f"     æ¥æº: {result['source']}")
        else:
            print("  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        print("-" * 40)


def demo_knowledge_base_management():
    """æ¼”ç¤ºçŸ¥è¯†åº“ç®¡ç†"""
    print("\nğŸ“š çŸ¥è¯†åº“ç®¡ç†æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºå­˜å‚¨
    store = InMemoryStore(
        index={
            "dims": 384,
            "embed": MockEmbeddings(384),
            "fields": ["content", "title"]
        }
    )
    
    # åˆå§‹åŒ–çŸ¥è¯†åº“
    embeddings = MockEmbeddings(384)
    knowledge_base = initialize_knowledge_base(store, embeddings)
    
    print(f"çŸ¥è¯†åº“åŒ…å« {len(knowledge_base.documents)} ä¸ªæ–‡æ¡£")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡æ–‡æ¡£
    categories = {}
    for doc in knowledge_base.documents.values():
        category = doc.metadata.get("category", "unknown")
        categories[category] = categories.get(category, 0) + 1
    
    print("\næŒ‰ç±»åˆ«ç»Ÿè®¡:")
    for category, count in categories.items():
        print(f"  {category}: {count} ä¸ªæ–‡æ¡£")
    
    # æŒ‰éš¾åº¦ç»Ÿè®¡æ–‡æ¡£
    difficulties = {}
    for doc in knowledge_base.documents.values():
        difficulty = doc.metadata.get("difficulty", "unknown")
        difficulties[difficulty] = difficulties.get(difficulty, 0) + 1
    
    print("\næŒ‰éš¾åº¦ç»Ÿè®¡:")
    for difficulty, count in difficulties.items():
        print(f"  {difficulty}: {count} ä¸ªæ–‡æ¡£")
    
    # æ¼”ç¤ºæ–‡æ¡£æ£€ç´¢
    print("\næ–‡æ¡£æ£€ç´¢æ¼”ç¤º:")
    doc_id = "doc_001"
    doc = knowledge_base.get_document(doc_id)
    if doc:
        print(f"æ–‡æ¡£ID: {doc.id}")
        print(f"æ ‡é¢˜: {doc.title}")
        print(f"æ¥æº: {doc.source}")
        print(f"å†…å®¹: {doc.content[:100]}...")
        print(f"å…ƒæ•°æ®: {doc.metadata}")
    
    # æ¼”ç¤ºæ‰¹é‡æœç´¢
    print("\næ‰¹é‡æœç´¢æ¼”ç¤º:")
    batch_queries = ["LangGraph", "è®°å¿†", "RAG", "å‘é‡"]
    for query in batch_queries:
        results = knowledge_base.search_documents(query, limit=2, min_score=0.1)
        print(f"æŸ¥è¯¢ '{query}': æ‰¾åˆ° {len(results)} ä¸ªæ–‡æ¡£")
        for result in results:
            print(f"  - {result['title']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ LangGraph RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    try:
        # è¿è¡Œæ¼”ç¤º
        demo_rag_system()
        demo_vector_search()
        demo_knowledge_base_management()
        
        print("\nğŸ‰ RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ RAGç³»ç»Ÿç‰¹æ€§æ€»ç»“:")
        print("1. çŸ¥è¯†åº“ç®¡ç†: æ”¯æŒæ–‡æ¡£çš„æ·»åŠ ã€å­˜å‚¨å’Œæ£€ç´¢")
        print("2. å‘é‡æœç´¢: ä½¿ç”¨åµŒå…¥æ¨¡å‹è¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢")
        print("3. æ£€ç´¢å¢å¼º: åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå‡†ç¡®å›ç­”")
        print("4. ä¸Šä¸‹æ–‡æ„å»º: æ™ºèƒ½ç»„åˆå¤šä¸ªç›¸å…³æ–‡æ¡£çš„ä¿¡æ¯")
        print("5. å“åº”ç”Ÿæˆ: åŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ç”Ÿæˆé«˜è´¨é‡å›ç­”")
        print("6. å¯æ‰©å±•æ€§: æ”¯æŒå¤§è§„æ¨¡çŸ¥è¯†åº“å’Œå¤æ‚æŸ¥è¯¢")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
